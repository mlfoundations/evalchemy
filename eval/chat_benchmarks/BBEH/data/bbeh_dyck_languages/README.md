# BBEH Dyck Language

This task is from the [BIG-Bench Mistake](https://arxiv.org/pdf/2311.08516).
This task involves finding the first mistake in an existing chain-of-thought
sequence, used to answer a Dyck Language question in the original BIG-Bench Hard
(BBH) dataset. In each example, the target answer is either the number where the
first mistake occurred, or that there are no mistakes in the CoT sequence. These
CoT sequences are generated by prompting PaLM 2 Unicorn on the original BBH
dataset at temperature = 0. The newline is used as a stop token so that each
intermediate step can be prepended with *Thought 1:* , *Thought 2: *, etc.
Further information on the prompting and generation process can be found in the
original work.
