{
  "gpt-4o-2024-05-13": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "57.5",
    "AE2.0": "51.3",
    "Arena Elo (hard-en) - 2024-07-16": 1280,
    "Arena Elo (hard-en) - latest": 1280,
    "haiku_reward.K=1500": 38.4765625,
    "llama_reward.K=1500": 51.66015625,
    "gpt4t_reward.K=1500": 0.390625,
    "haiku_reward.Creative Tasks.K=1500": 38.837920489296636,
    "llama_reward.Creative Tasks.K=1500": 44.952681388012614,
    "gpt4t_reward.Creative Tasks.K=1500": -8.206686930091186,
    "mixture_of_rewards.Creative Tasks.K=1500": 25.194638315739358,
    "haiku_reward.Planning & Reasoning.K=1500": 46.99828473413379,
    "llama_reward.Planning & Reasoning.K=1500": 64.32291666666666,
    "gpt4t_reward.Planning & Reasoning.K=1500": 4.10958904109589,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 38.47693014729878,
    "haiku_reward.Math & Data Analysis.K=1500": 53.23275862068966,
    "llama_reward.Math & Data Analysis.K=1500": 77.27272727272727,
    "gpt4t_reward.Math & Data Analysis.K=1500": 5.0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 45.16849529780564,
    "haiku_reward.Information/Advice seeking.K=1500": 40.08042895442359,
    "llama_reward.Information/Advice seeking.K=1500": 53.77358490566038,
    "gpt4t_reward.Information/Advice seeking.K=1500": 9.115281501340483,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 34.32309845380815,
    "haiku_reward.Coding & Debugging.K=1500": 51.15606936416185,
    "llama_reward.Coding & Debugging.K=1500": 74.85549132947978,
    "gpt4t_reward.Coding & Debugging.K=1500": -6.0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 40.00385356454721,
    "haiku_reward.task_macro.K=1500": 47.453680674531626,
    "llama_reward.task_macro.K=1500": 66.11626666129725,
    "gpt4t_reward.task_macro.K=1500": 1.1305482025957123,
    "mixture_of_rewards.K=1500": 30.17578125,
    "task_macro_reward.K=1500": 38.2334985128082,
    "WB_score.Creative Tasks": 59.12144702842377,
    "WB_score.Planning & Reasoning": 60.20958083832337,
    "WB_score.Math & Data Analysis": 57.29083665338646,
    "WB_score.Information/Advice seeking": 58.61386138613861,
    "WB_score.Coding & Debugging": 60.473933649289116,
    "WB_score": 58.80742913000978,
    "WB_score.task_macro": 59.298178803519555,
    "Length": 3723.516129032258,
    "Rank_ScoreMacro": 2,
    "Rank_TaskMacroReward.K": 1,
    "Rank_Avg": 1.5,
    "RewardScore_Avg": 48.76583865816387,
    "WB_Elo": 1239.4730859477231
  },
  "gpt-4-turbo-2024-04-09": {
    "Arena-Hard v0.1": "82.6",
    "AE2.0 LC": "55",
    "AE2.0": "46.1",
    "Arena Elo (hard-en) - 2024-07-16": 1247,
    "Arena Elo (hard-en) - latest": 1247,
    "haiku_reward.K=1500": 41.2109375,
    "llama_reward.K=1500": 54.78515625,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 43.53932584269663,
    "llama_reward.Creative Tasks.K=1500": 48.87323943661972,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 30.80418842643878,
    "haiku_reward.Planning & Reasoning.K=1500": 44.40894568690096,
    "llama_reward.Planning & Reasoning.K=1500": 60.789049919484704,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 35.06599853546189,
    "haiku_reward.Math & Data Analysis.K=1500": 45.22821576763486,
    "llama_reward.Math & Data Analysis.K=1500": 72.5,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 39.24273858921162,
    "haiku_reward.Information/Advice seeking.K=1500": 39.1025641025641,
    "llama_reward.Information/Advice seeking.K=1500": 50.256410256410255,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 29.786324786324787,
    "haiku_reward.Coding & Debugging.K=1500": 47.82608695652174,
    "llama_reward.Coding & Debugging.K=1500": 79.67032967032966,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 42.498805542283804,
    "haiku_reward.task_macro.K=1500": 44.55128264635466,
    "llama_reward.task_macro.K=1500": 65.30592671534433,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 31.998697916666668,
    "task_macro_reward.K=1500": 36.619069787232995,
    "WB_score.Creative Tasks": 58.65633074935401,
    "WB_score.Planning & Reasoning": 56.203288490284,
    "WB_score.Math & Data Analysis": 50.99601593625499,
    "WB_score.Information/Advice seeking": 57.178217821782184,
    "WB_score.Coding & Debugging": 55.071090047393355,
    "WB_score": 56.089931573802545,
    "WB_score.task_macro": 55.22122481039269,
    "Length": 3093.1700879765394,
    "Rank_ScoreMacro": 6,
    "Rank_TaskMacroReward.K": 3,
    "Rank_Avg": 4.5,
    "RewardScore_Avg": 45.92014729881284,
    "WB_Elo": 1222.6896973319426
  },
  "gpt-4-0125-preview": {
    "Arena-Hard v0.1": "78",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1237,
    "Arena Elo (hard-en) - latest": 1237,
    "haiku_reward.K=1500": 33.7890625,
    "llama_reward.K=1500": 50.341796875,
    "gpt4t_reward.K=1500": -4.248046875,
    "haiku_reward.Creative Tasks.K=1500": 40.87078651685393,
    "llama_reward.Creative Tasks.K=1500": 50.0,
    "gpt4t_reward.Creative Tasks.K=1500": 1.0582010582010581,
    "mixture_of_rewards.Creative Tasks.K=1500": 30.642995858351668,
    "haiku_reward.Planning & Reasoning.K=1500": 35.19108280254777,
    "llama_reward.Planning & Reasoning.K=1500": 55.92,
    "gpt4t_reward.Planning & Reasoning.K=1500": -4.953560371517028,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 28.719174143676913,
    "haiku_reward.Math & Data Analysis.K=1500": 30.65843621399177,
    "llama_reward.Math & Data Analysis.K=1500": 61.20331950207469,
    "gpt4t_reward.Math & Data Analysis.K=1500": -14.814814814814813,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 25.68231363375055,
    "haiku_reward.Information/Advice seeking.K=1500": 33.67609254498715,
    "llama_reward.Information/Advice seeking.K=1500": 46.1439588688946,
    "gpt4t_reward.Information/Advice seeking.K=1500": 1.0101010101010102,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 26.943384141327584,
    "haiku_reward.Coding & Debugging.K=1500": 33.51063829787234,
    "llama_reward.Coding & Debugging.K=1500": 69.31216931216932,
    "gpt4t_reward.Coding & Debugging.K=1500": -12.755102040816327,
    "mixture_of_rewards.Coding & Debugging.K=1500": 30.02256852307511,
    "haiku_reward.task_macro.K=1500": 34.15327373824931,
    "llama_reward.task_macro.K=1500": 58.38978953556996,
    "gpt4t_reward.task_macro.K=1500": -7.50820348285373,
    "mixture_of_rewards.K=1500": 26.627604166666668,
    "task_macro_reward.K=1500": 28.34495326365518,
    "WB_score.Creative Tasks": 57.571059431524546,
    "WB_score.Planning & Reasoning": 53.45291479820627,
    "WB_score.Math & Data Analysis": 45.79365079365079,
    "WB_score.Information/Advice seeking": 54.35643564356436,
    "WB_score.Coding & Debugging": 52.924528301886795,
    "WB_score": 53.28125,
    "WB_score.task_macro": 52.27753918256898,
    "Length": 3335.638671875,
    "Rank_ScoreMacro": 12,
    "Rank_TaskMacroReward.K": 6,
    "Rank_Avg": 9.0,
    "RewardScore_Avg": 40.31124622311208,
    "WB_Elo": 1208.302900834726
  },
  "claude-3-opus-20240229": {
    "Arena-Hard v0.1": "60.4",
    "AE2.0 LC": "40.5",
    "AE2.0": "29.1",
    "Arena Elo (hard-en) - 2024-07-16": 1230,
    "Arena Elo (hard-en) - latest": 1230,
    "haiku_reward.K=1500": 31.8359375,
    "llama_reward.K=1500": 42.48046875,
    "gpt4t_reward.K=1500": -19.62890625,
    "haiku_reward.Creative Tasks.K=1500": 31.896551724137932,
    "llama_reward.Creative Tasks.K=1500": 31.432748538011694,
    "gpt4t_reward.Creative Tasks.K=1500": -22.408963585434176,
    "mixture_of_rewards.Creative Tasks.K=1500": 13.640112225571817,
    "haiku_reward.Planning & Reasoning.K=1500": 32.769726247987116,
    "llama_reward.Planning & Reasoning.K=1500": 49.02597402597403,
    "gpt4t_reward.Planning & Reasoning.K=1500": -20.302547770700638,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 20.497717501086836,
    "haiku_reward.Math & Data Analysis.K=1500": 38.00813008130081,
    "llama_reward.Math & Data Analysis.K=1500": 66.73469387755102,
    "gpt4t_reward.Math & Data Analysis.K=1500": -18.442622950819672,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 28.766733669344053,
    "haiku_reward.Information/Advice seeking.K=1500": 29.457364341085274,
    "llama_reward.Information/Advice seeking.K=1500": 38.80208333333333,
    "gpt4t_reward.Information/Advice seeking.K=1500": -16.323907455012854,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 17.31184673980191,
    "haiku_reward.Coding & Debugging.K=1500": 43.956043956043956,
    "llama_reward.Coding & Debugging.K=1500": 67.03296703296702,
    "gpt4t_reward.Coding & Debugging.K=1500": -25.945945945945947,
    "mixture_of_rewards.Coding & Debugging.K=1500": 28.347688347688344,
    "haiku_reward.task_macro.K=1500": 36.20139830615213,
    "llama_reward.task_macro.K=1500": 54.02659094110172,
    "gpt4t_reward.task_macro.K=1500": -20.989611048548344,
    "mixture_of_rewards.K=1500": 18.229166666666668,
    "task_macro_reward.K=1500": 23.079459399568503,
    "WB_score.Creative Tasks": 53.0232558139535,
    "WB_score.Planning & Reasoning": 52.526158445440956,
    "WB_score.Math & Data Analysis": 46.74603174603174,
    "WB_score.Information/Advice seeking": 53.46534653465346,
    "WB_score.Coding & Debugging": 53.301886792452834,
    "WB_score": 52.109375,
    "WB_score.task_macro": 51.714047600287536,
    "Length": 2685.9794921875,
    "Rank_ScoreMacro": 13,
    "Rank_TaskMacroReward.K": 8,
    "Rank_Avg": 10.5,
    "RewardScore_Avg": 37.39675349992802,
    "WB_Elo": 1207.4751834070414
  },
  "Meta-Llama-3-70B-Instruct": {
    "Arena-Hard v0.1": "41.1",
    "AE2.0 LC": "34.4",
    "AE2.0": "33.2",
    "Arena Elo (hard-en) - 2024-07-16": 1212,
    "Arena Elo (hard-en) - latest": 1212,
    "haiku_reward.K=1500": 27.859237536656888,
    "llama_reward.K=1500": 45.992179863147605,
    "gpt4t_reward.K=1500": -18.181818181818183,
    "haiku_reward.Creative Tasks.K=1500": 31.594202898550726,
    "llama_reward.Creative Tasks.K=1500": 41.61764705882353,
    "gpt4t_reward.Creative Tasks.K=1500": -17.77777777777778,
    "mixture_of_rewards.Creative Tasks.K=1500": 18.478024059865493,
    "haiku_reward.Planning & Reasoning.K=1500": 30.63209076175041,
    "llama_reward.Planning & Reasoning.K=1500": 52.61011419249593,
    "gpt4t_reward.Planning & Reasoning.K=1500": -18.4,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 21.614068318082115,
    "haiku_reward.Math & Data Analysis.K=1500": 36.21399176954733,
    "llama_reward.Math & Data Analysis.K=1500": 67.07818930041152,
    "gpt4t_reward.Math & Data Analysis.K=1500": -19.834710743801654,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 27.819156775385732,
    "haiku_reward.Information/Advice seeking.K=1500": 28.221649484536083,
    "llama_reward.Information/Advice seeking.K=1500": 43.29896907216495,
    "gpt4t_reward.Information/Advice seeking.K=1500": -11.704834605597965,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 19.93859465036769,
    "haiku_reward.Coding & Debugging.K=1500": 26.536312849162012,
    "llama_reward.Coding & Debugging.K=1500": 67.77777777777779,
    "gpt4t_reward.Coding & Debugging.K=1500": -31.491712707182316,
    "mixture_of_rewards.Coding & Debugging.K=1500": 20.940792639919163,
    "haiku_reward.task_macro.K=1500": 30.45007049386637,
    "llama_reward.task_macro.K=1500": 57.02023254669245,
    "gpt4t_reward.task_macro.K=1500": -21.02460346755093,
    "mixture_of_rewards.K=1500": 18.556533072662102,
    "task_macro_reward.K=1500": 22.148566524335962,
    "WB_score.Creative Tasks": 54.30051813471502,
    "WB_score.Planning & Reasoning": 50.07473841554558,
    "WB_score.Math & Data Analysis": 42.063492063492056,
    "WB_score.Information/Advice seeking": 52.27722772277227,
    "WB_score.Coding & Debugging": 44.71698113207546,
    "WB_score": 49.579667644183765,
    "WB_score.task_macro": 47.770804496306326,
    "Length": 3046.6383186705766,
    "Rank_ScoreMacro": 18,
    "Rank_TaskMacroReward.K": 9,
    "Rank_Avg": 13.5,
    "RewardScore_Avg": 34.959685510321144,
    "WB_Elo": 1194.6486448820997
  },
  "Qwen1.5-72B-Chat-greedy": {
    "Arena-Hard v0.1": "36.1",
    "AE2.0 LC": "36.6",
    "AE2.0": "26.5",
    "Arena Elo (hard-en) - 2024-07-16": 1142,
    "Arena Elo (hard-en) - latest": 1142,
    "haiku_reward.K=1500": 12.841796875,
    "llama_reward.K=1500": 35.986328125,
    "gpt4t_reward.K=1500": -34.716796875,
    "haiku_reward.Creative Tasks.K=1500": 25.069637883008355,
    "llama_reward.Creative Tasks.K=1500": 32.262569832402235,
    "gpt4t_reward.Creative Tasks.K=1500": -27.308707124010557,
    "mixture_of_rewards.Creative Tasks.K=1500": 10.007833530466678,
    "haiku_reward.Planning & Reasoning.K=1500": 11.224489795918368,
    "llama_reward.Planning & Reasoning.K=1500": 41.07424960505529,
    "gpt4t_reward.Planning & Reasoning.K=1500": -36.9431643625192,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 5.118525012818154,
    "haiku_reward.Math & Data Analysis.K=1500": -1.0121457489878543,
    "llama_reward.Math & Data Analysis.K=1500": 49.59349593495935,
    "gpt4t_reward.Math & Data Analysis.K=1500": -46.138211382113816,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.8143796012858928,
    "haiku_reward.Information/Advice seeking.K=1500": 10.841836734693878,
    "llama_reward.Information/Advice seeking.K=1500": 30.179028132992325,
    "gpt4t_reward.Information/Advice seeking.K=1500": -30.303030303030305,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 3.572611521551965,
    "haiku_reward.Coding & Debugging.K=1500": 1.832460732984293,
    "llama_reward.Coding & Debugging.K=1500": 48.68421052631579,
    "gpt4t_reward.Coding & Debugging.K=1500": -52.03045685279187,
    "mixture_of_rewards.Coding & Debugging.K=1500": -0.5045951978305965,
    "haiku_reward.task_macro.K=1500": 7.573734506245275,
    "llama_reward.task_macro.K=1500": 42.22255307686707,
    "gpt4t_reward.task_macro.K=1500": -40.786708250638036,
    "mixture_of_rewards.K=1500": 4.703776041666667,
    "task_macro_reward.K=1500": 3.00319311082477,
    "WB_score.Creative Tasks": 50.362694300518136,
    "WB_score.Planning & Reasoning": 43.45345345345345,
    "WB_score.Math & Data Analysis": 29.800796812748995,
    "WB_score.Information/Advice seeking": 48.21782178217822,
    "WB_score.Coding & Debugging": 35.35545023696683,
    "WB_score": 43.46718903036239,
    "WB_score.task_macro": 39.927713665824655,
    "Length": 2392.364348677767,
    "Rank_ScoreMacro": 28,
    "Rank_TaskMacroReward.K": 21,
    "Rank_Avg": 24.5,
    "RewardScore_Avg": 21.46545338832471,
    "WB_Elo": 1150.1365337277027
  },
  "claude-3-sonnet-20240229": {
    "Arena-Hard v0.1": "46.8",
    "AE2.0 LC": "34.9",
    "AE2.0": "25.6",
    "Arena Elo (hard-en) - 2024-07-16": 1188,
    "Arena Elo (hard-en) - latest": 1188,
    "haiku_reward.K=1500": 18.132942326490713,
    "llama_reward.K=1500": 30.419921875,
    "gpt4t_reward.K=1500": -28.80859375,
    "haiku_reward.Creative Tasks.K=1500": 14.0625,
    "llama_reward.Creative Tasks.K=1500": 17.105263157894736,
    "gpt4t_reward.Creative Tasks.K=1500": -35.18005540166205,
    "mixture_of_rewards.Creative Tasks.K=1500": -1.3374307479224388,
    "haiku_reward.Planning & Reasoning.K=1500": 22.37479806138934,
    "llama_reward.Planning & Reasoning.K=1500": 38.03630363036304,
    "gpt4t_reward.Planning & Reasoning.K=1500": -28.190630048465266,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 10.740157214429038,
    "haiku_reward.Math & Data Analysis.K=1500": 25.102880658436217,
    "llama_reward.Math & Data Analysis.K=1500": 59.166666666666664,
    "gpt4t_reward.Math & Data Analysis.K=1500": -31.512605042016805,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 17.58564742769536,
    "haiku_reward.Information/Advice seeking.K=1500": 15.755208333333334,
    "llama_reward.Information/Advice seeking.K=1500": 24.345549738219894,
    "gpt4t_reward.Information/Advice seeking.K=1500": -24.352331606217618,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 5.249475488445204,
    "haiku_reward.Coding & Debugging.K=1500": 22.905027932960895,
    "llama_reward.Coding & Debugging.K=1500": 57.67045454545454,
    "gpt4t_reward.Coding & Debugging.K=1500": -34.63687150837989,
    "mixture_of_rewards.Coding & Debugging.K=1500": 15.312870323345182,
    "haiku_reward.task_macro.K=1500": 21.168488295026112,
    "llama_reward.task_macro.K=1500": 43.28671753032651,
    "gpt4t_reward.task_macro.K=1500": -30.716054398645788,
    "mixture_of_rewards.K=1500": 6.581423483830238,
    "task_macro_reward.K=1500": 11.246383808902278,
    "WB_score.Creative Tasks": 46.304909560723516,
    "WB_score.Planning & Reasoning": 47.425149700598794,
    "WB_score.Math & Data Analysis": 40.63745019920319,
    "WB_score.Information/Advice seeking": 47.128712871287135,
    "WB_score.Coding & Debugging": 46.09523809523809,
    "WB_score": 45.24461839530332,
    "WB_score.task_macro": 45.48145776375293,
    "Length": 2670.243639921722,
    "Rank_ScoreMacro": 24,
    "Rank_TaskMacroReward.K": 17,
    "Rank_Avg": 20.5,
    "RewardScore_Avg": 28.363920786327604,
    "WB_Elo": 1180.9039586012009
  },
  "mistral-large-2402": {
    "Arena-Hard v0.1": "37.7",
    "AE2.0 LC": "32.7",
    "AE2.0": "21.4",
    "Arena Elo (hard-en) - 2024-07-16": 1158,
    "Arena Elo (hard-en) - latest": 1158,
    "haiku_reward.K=1500": -2.880859375,
    "llama_reward.K=1500": 20.674486803519063,
    "gpt4t_reward.K=1500": -44.140625,
    "haiku_reward.Creative Tasks.K=1500": 10.028653295128938,
    "llama_reward.Creative Tasks.K=1500": 16.76300578034682,
    "gpt4t_reward.Creative Tasks.K=1500": -38.84297520661157,
    "mixture_of_rewards.Creative Tasks.K=1500": -4.017105377045271,
    "haiku_reward.Planning & Reasoning.K=1500": -6.94888178913738,
    "llama_reward.Planning & Reasoning.K=1500": 20.907617504051863,
    "gpt4t_reward.Planning & Reasoning.K=1500": -48.661417322834644,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -11.567560535973387,
    "haiku_reward.Math & Data Analysis.K=1500": -10.493827160493826,
    "llama_reward.Math & Data Analysis.K=1500": 38.589211618257266,
    "gpt4t_reward.Math & Data Analysis.K=1500": -55.809128630705395,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -9.237914724313985,
    "haiku_reward.Information/Advice seeking.K=1500": -8.868894601542417,
    "llama_reward.Information/Advice seeking.K=1500": 10.20671834625323,
    "gpt4t_reward.Information/Advice seeking.K=1500": -40.966921119592875,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -13.209699124960688,
    "haiku_reward.Coding & Debugging.K=1500": -13.563829787234042,
    "llama_reward.Coding & Debugging.K=1500": 38.35978835978836,
    "gpt4t_reward.Coding & Debugging.K=1500": -58.76288659793815,
    "mixture_of_rewards.Coding & Debugging.K=1500": -11.32230934179461,
    "haiku_reward.task_macro.K=1500": -7.952012828326409,
    "llama_reward.task_macro.K=1500": 27.096844336404395,
    "gpt4t_reward.task_macro.K=1500": -50.576039363088796,
    "mixture_of_rewards.K=1500": -8.782332523826978,
    "task_macro_reward.K=1500": -10.477069285003603,
    "WB_score.Creative Tasks": 49.66408268733851,
    "WB_score.Planning & Reasoning": 41.79910044977511,
    "WB_score.Math & Data Analysis": 30.879999999999992,
    "WB_score.Information/Advice seeking": 46.13861386138615,
    "WB_score.Coding & Debugging": 33.74407582938389,
    "WB_score": 42.28739002932551,
    "WB_score.task_macro": 38.89367833445423,
    "Length": 2514.9814090019568,
    "Rank_ScoreMacro": 31,
    "Rank_TaskMacroReward.K": 41,
    "Rank_Avg": 36.0,
    "RewardScore_Avg": 14.208304524725314,
    "WB_Elo": 1158.021169549111
  },
  "claude-3-haiku-20240307": {
    "Arena-Hard v0.1": "41.5",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1170,
    "Arena Elo (hard-en) - latest": 1170,
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 20.068359375,
    "gpt4t_reward.K=1500": -42.578125,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 5.341246290801187,
    "gpt4t_reward.Creative Tasks.K=1500": -45.78651685393258,
    "mixture_of_rewards.Creative Tasks.K=1500": -13.481756854377132,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 25.61174551386623,
    "gpt4t_reward.Planning & Reasoning.K=1500": -44.40894568690096,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -6.265733391011576,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 41.11570247933884,
    "gpt4t_reward.Math & Data Analysis.K=1500": -49.37759336099585,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -2.753963627219003,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 17.357512953367877,
    "gpt4t_reward.Information/Advice seeking.K=1500": -38.07692307692307,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -6.906470041185066,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 44.44444444444444,
    "gpt4t_reward.Coding & Debugging.K=1500": -49.184782608695656,
    "mixture_of_rewards.Coding & Debugging.K=1500": -1.580112721417071,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 30.394673218453406,
    "gpt4t_reward.task_macro.K=1500": -45.856990413715835,
    "mixture_of_rewards.K=1500": -7.503255208333333,
    "task_macro_reward.K=1500": -5.154105731754143,
    "WB_score.Creative Tasks": 42.94573643410853,
    "WB_score.Planning & Reasoning": 41.28550074738415,
    "WB_score.Math & Data Analysis": 31.428571428571423,
    "WB_score.Information/Advice seeking": 45.346534653465355,
    "WB_score.Coding & Debugging": 36.9811320754717,
    "WB_score": 40.25390625,
    "WB_score.task_macro": 38.893606666167265,
    "Length": 2601.029296875,
    "Rank_ScoreMacro": 32,
    "Rank_TaskMacroReward.K": 38,
    "Rank_Avg": 35.0,
    "RewardScore_Avg": 16.86975046720656,
    "WB_Elo": 1159.819595320082
  },
  "dbrx-instruct@together": {
    "Arena-Hard v0.1": "23.9",
    "AE2.0 LC": "25.4",
    "AE2.0": "18.4",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=1500": -15.13671875,
    "llama_reward.K=1500": 9.08203125,
    "gpt4t_reward.K=1500": -53.564453125,
    "haiku_reward.Creative Tasks.K=1500": -7.142857142857142,
    "llama_reward.Creative Tasks.K=1500": -2.5936599423631126,
    "gpt4t_reward.Creative Tasks.K=1500": -52.04918032786885,
    "mixture_of_rewards.Creative Tasks.K=1500": -20.5952324710297,
    "haiku_reward.Planning & Reasoning.K=1500": -17.647058823529413,
    "llama_reward.Planning & Reasoning.K=1500": 13.141025641025642,
    "gpt4t_reward.Planning & Reasoning.K=1500": -57.83699059561128,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -20.781007926038352,
    "haiku_reward.Math & Data Analysis.K=1500": -15.22633744855967,
    "llama_reward.Math & Data Analysis.K=1500": 33.81742738589212,
    "gpt4t_reward.Math & Data Analysis.K=1500": -63.27800829875518,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -14.895639453807577,
    "haiku_reward.Information/Advice seeking.K=1500": -21.31782945736434,
    "llama_reward.Information/Advice seeking.K=1500": -3.7371134020618557,
    "gpt4t_reward.Information/Advice seeking.K=1500": -51.65816326530612,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -25.571035374910775,
    "haiku_reward.Coding & Debugging.K=1500": -24.210526315789473,
    "llama_reward.Coding & Debugging.K=1500": 29.365079365079367,
    "gpt4t_reward.Coding & Debugging.K=1500": -64.69072164948454,
    "mixture_of_rewards.Coding & Debugging.K=1500": -19.845389533398215,
    "haiku_reward.task_macro.K=1500": -18.33855148150739,
    "llama_reward.task_macro.K=1500": 17.442188130693772,
    "gpt4t_reward.task_macro.K=1500": -59.20123394063979,
    "mixture_of_rewards.K=1500": -19.873046875,
    "task_macro_reward.K=1500": -20.03253243048447,
    "WB_score.Creative Tasks": 42.32558139534884,
    "WB_score.Planning & Reasoning": 36.227544910179645,
    "WB_score.Math & Data Analysis": 24.523809523809526,
    "WB_score.Information/Advice seeking": 41.089108910891085,
    "WB_score.Coding & Debugging": 26.445497630331758,
    "WB_score": 35.5425219941349,
    "WB_score.task_macro": 32.598891595850844,
    "Length": 2576.5190615835777,
    "Rank_ScoreMacro": 39,
    "Rank_TaskMacroReward.K": 47,
    "Rank_Avg": 43.0,
    "RewardScore_Avg": 6.2831795826831875,
    "WB_Elo": 1123.3353568518312
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "Arena-Hard v0.1": "23.4",
    "AE2.0 LC": "23.7",
    "AE2.0": "18.3",
    "Arena Elo (hard-en) - 2024-07-16": 1114,
    "Arena Elo (hard-en) - latest": 1114,
    "haiku_reward.K=1500": -12.255859375,
    "llama_reward.K=1500": 11.572265625,
    "gpt4t_reward.K=1500": -48.583984375,
    "haiku_reward.Creative Tasks.K=1500": -3.7142857142857144,
    "llama_reward.Creative Tasks.K=1500": 3.77906976744186,
    "gpt4t_reward.Creative Tasks.K=1500": -45.604395604395606,
    "mixture_of_rewards.Creative Tasks.K=1500": -15.17987051707982,
    "haiku_reward.Planning & Reasoning.K=1500": -18.327974276527332,
    "llama_reward.Planning & Reasoning.K=1500": 13.192182410423452,
    "gpt4t_reward.Planning & Reasoning.K=1500": -52.791068580542266,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -19.308953482215383,
    "haiku_reward.Math & Data Analysis.K=1500": -22.291666666666668,
    "llama_reward.Math & Data Analysis.K=1500": 28.870292887029287,
    "gpt4t_reward.Math & Data Analysis.K=1500": -60.08403361344538,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -17.835135797694253,
    "haiku_reward.Information/Advice seeking.K=1500": -11.24031007751938,
    "llama_reward.Information/Advice seeking.K=1500": 7.253886010362693,
    "gpt4t_reward.Information/Advice seeking.K=1500": -44.61538461538462,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -16.200602894180435,
    "haiku_reward.Coding & Debugging.K=1500": -25.806451612903224,
    "llama_reward.Coding & Debugging.K=1500": 24.863387978142075,
    "gpt4t_reward.Coding & Debugging.K=1500": -62.23404255319149,
    "mixture_of_rewards.Coding & Debugging.K=1500": -21.059035395984214,
    "haiku_reward.task_macro.K=1500": -18.473068303944686,
    "llama_reward.task_macro.K=1500": 17.63572742404824,
    "gpt4t_reward.task_macro.K=1500": -54.764033430389226,
    "mixture_of_rewards.K=1500": -16.422526041666668,
    "task_macro_reward.K=1500": -18.53379143676189,
    "WB_score.Creative Tasks": 42.753246753246756,
    "WB_score.Planning & Reasoning": 34.586466165413526,
    "WB_score.Math & Data Analysis": 22.142857142857135,
    "WB_score.Information/Advice seeking": 41.935483870967744,
    "WB_score.Coding & Debugging": 25.023696682464447,
    "WB_score": 35.0293542074364,
    "WB_score.task_macro": 31.47027304895869,
    "Length": 2653.5813725490198,
    "Rank_ScoreMacro": 41,
    "Rank_TaskMacroReward.K": 45,
    "Rank_Avg": 43.0,
    "RewardScore_Avg": 6.4682408060983985,
    "WB_Elo": 1122.7834564499847
  },
  "Starling-LM-7B-beta": {
    "Arena-Hard v0.1": "23",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1113,
    "Arena Elo (hard-en) - latest": 1113,
    "haiku_reward.K=1500": -4.1015625,
    "llama_reward.K=1500": 16.943359375,
    "gpt4t_reward.K=1500": -46.58203125,
    "haiku_reward.Creative Tasks.K=1500": 10.674157303370785,
    "llama_reward.Creative Tasks.K=1500": 15.3954802259887,
    "gpt4t_reward.Creative Tasks.K=1500": -37.0026525198939,
    "mixture_of_rewards.Creative Tasks.K=1500": -3.644338330178138,
    "haiku_reward.Planning & Reasoning.K=1500": -7.790143084260731,
    "llama_reward.Planning & Reasoning.K=1500": 18.341307814992025,
    "gpt4t_reward.Planning & Reasoning.K=1500": -50.77639751552795,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -13.408410928265551,
    "haiku_reward.Math & Data Analysis.K=1500": -20.781893004115226,
    "llama_reward.Math & Data Analysis.K=1500": 21.487603305785125,
    "gpt4t_reward.Math & Data Analysis.K=1500": -63.84297520661158,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -21.04575496831389,
    "haiku_reward.Information/Advice seeking.K=1500": -3.0848329048843186,
    "llama_reward.Information/Advice seeking.K=1500": 16.195372750642672,
    "gpt4t_reward.Information/Advice seeking.K=1500": -42.00507614213198,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -9.63151209879121,
    "haiku_reward.Coding & Debugging.K=1500": -20.212765957446805,
    "llama_reward.Coding & Debugging.K=1500": 27.513227513227513,
    "gpt4t_reward.Coding & Debugging.K=1500": -62.43523316062176,
    "mixture_of_rewards.Coding & Debugging.K=1500": -18.378257201613685,
    "haiku_reward.task_macro.K=1500": -11.107805333311052,
    "llama_reward.task_macro.K=1500": 20.768429924639136,
    "gpt4t_reward.task_macro.K=1500": -53.7605572459673,
    "mixture_of_rewards.K=1500": -11.246744791666666,
    "task_macro_reward.K=1500": -14.699977551546406,
    "WB_score.Creative Tasks": 43.79220779220779,
    "WB_score.Planning & Reasoning": 34.050822122571006,
    "WB_score.Math & Data Analysis": 16.984126984126977,
    "WB_score.Information/Advice seeking": 41.88118811881188,
    "WB_score.Coding & Debugging": 24.36018957345972,
    "WB_score": 34.17399804496579,
    "WB_score.task_macro": 30.16944980829014,
    "Length": 2797.807240704501,
    "Rank_ScoreMacro": 44,
    "Rank_TaskMacroReward.K": 43,
    "Rank_Avg": 43.5,
    "RewardScore_Avg": 7.734736128371867,
    "WB_Elo": 1119.3332864397855
  },
  "command-r": {
    "Arena-Hard v0.1": "17",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1106,
    "Arena Elo (hard-en) - latest": 1106,
    "haiku_reward.K=1500": -11.865234375,
    "llama_reward.K=1500": 10.15625,
    "gpt4t_reward.K=1500": -44.43359375,
    "haiku_reward.Creative Tasks.K=1500": 4.848484848484849,
    "llama_reward.Creative Tasks.K=1500": 7.561728395061729,
    "gpt4t_reward.Creative Tasks.K=1500": -39.910979228486646,
    "mixture_of_rewards.Creative Tasks.K=1500": -9.166921994980022,
    "haiku_reward.Planning & Reasoning.K=1500": -14.262295081967213,
    "llama_reward.Planning & Reasoning.K=1500": 13.471074380165291,
    "gpt4t_reward.Planning & Reasoning.K=1500": -50.814332247557005,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -17.20185098311964,
    "haiku_reward.Math & Data Analysis.K=1500": -34.232365145228215,
    "llama_reward.Math & Data Analysis.K=1500": 16.458333333333332,
    "gpt4t_reward.Math & Data Analysis.K=1500": -63.86554621848739,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -27.21319267679409,
    "haiku_reward.Information/Advice seeking.K=1500": -2.6178010471204187,
    "llama_reward.Information/Advice seeking.K=1500": 12.433862433862434,
    "gpt4t_reward.Information/Advice seeking.K=1500": -35.958005249343834,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -8.71398128753394,
    "haiku_reward.Coding & Debugging.K=1500": -36.81318681318682,
    "llama_reward.Coding & Debugging.K=1500": 15.027322404371585,
    "gpt4t_reward.Coding & Debugging.K=1500": -68.47826086956522,
    "mixture_of_rewards.Coding & Debugging.K=1500": -30.08804175946015,
    "haiku_reward.task_macro.K=1500": -20.550700184267082,
    "llama_reward.task_macro.K=1500": 13.723703228827816,
    "gpt4t_reward.task_macro.K=1500": -54.71690129813502,
    "mixture_of_rewards.K=1500": -15.380859375,
    "task_macro_reward.K=1500": -20.514632751191428,
    "WB_score.Creative Tasks": 47.44186046511628,
    "WB_score.Planning & Reasoning": 34.61883408071749,
    "WB_score.Math & Data Analysis": 16.031746031746028,
    "WB_score.Information/Advice seeking": 44.10891089108912,
    "WB_score.Coding & Debugging": 19.33962264150944,
    "WB_score": 35.05859375,
    "WB_score.task_macro": 29.533143228506248,
    "Length": 2919.423828125,
    "Rank_ScoreMacro": 47,
    "Rank_TaskMacroReward.K": 48,
    "Rank_Avg": 47.5,
    "RewardScore_Avg": 4.50925523865741,
    "WB_Elo": 1119.8497539885705
  },
  "command-r-plus": {
    "Arena-Hard v0.1": "33.1",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1155,
    "Arena Elo (hard-en) - latest": 1155,
    "haiku_reward.K=1500": 5.37109375,
    "llama_reward.K=1500": 26.3671875,
    "gpt4t_reward.K=1500": -33.203125,
    "haiku_reward.Creative Tasks.K=1500": 23.734177215189874,
    "llama_reward.Creative Tasks.K=1500": 29.73856209150327,
    "gpt4t_reward.Creative Tasks.K=1500": -24.371069182389938,
    "mixture_of_rewards.Creative Tasks.K=1500": 9.700556708101068,
    "haiku_reward.Planning & Reasoning.K=1500": 5.808080808080808,
    "llama_reward.Planning & Reasoning.K=1500": 32.28279386712095,
    "gpt4t_reward.Planning & Reasoning.K=1500": -36.930860033726816,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.38667154715831487,
    "haiku_reward.Math & Data Analysis.K=1500": -10.537190082644628,
    "llama_reward.Math & Data Analysis.K=1500": 37.39669421487603,
    "gpt4t_reward.Math & Data Analysis.K=1500": -52.083333333333336,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -8.407943067033978,
    "haiku_reward.Information/Advice seeking.K=1500": 11.716621253405995,
    "llama_reward.Information/Advice seeking.K=1500": 26.164383561643834,
    "gpt4t_reward.Information/Advice seeking.K=1500": -26.912568306010932,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 3.656145503012965,
    "haiku_reward.Coding & Debugging.K=1500": -14.606741573033707,
    "llama_reward.Coding & Debugging.K=1500": 32.10227272727273,
    "gpt4t_reward.Coding & Debugging.K=1500": -60.05586592178771,
    "mixture_of_rewards.Coding & Debugging.K=1500": -14.186778255849566,
    "haiku_reward.task_macro.K=1500": -0.1854655770918572,
    "llama_reward.task_macro.K=1500": 32.0780202473374,
    "gpt4t_reward.task_macro.K=1500": -43.30246349440625,
    "mixture_of_rewards.K=1500": -0.48828125,
    "task_macro_reward.K=1500": -3.8033029413869017,
    "WB_score.Creative Tasks": 52.55813953488372,
    "WB_score.Planning & Reasoning": 41.949025487256364,
    "WB_score.Math & Data Analysis": 23.492063492063497,
    "WB_score.Information/Advice seeking": 49.15841584158416,
    "WB_score.Coding & Debugging": 28.436018957345972,
    "WB_score": 41.565557729941304,
    "WB_score.task_macro": 36.76236856767293,
    "Length": 3293.812133072407,
    "Rank_ScoreMacro": 36,
    "Rank_TaskMacroReward.K": 37,
    "Rank_Avg": 36.5,
    "RewardScore_Avg": 16.479532813143013,
    "WB_Elo": 1151.2063636170662
  },
  "Meta-Llama-3-8B-Instruct": {
    "Arena-Hard v0.1": "20.6",
    "AE2.0 LC": "22.9",
    "AE2.0": "22.6",
    "Arena Elo (hard-en) - 2024-07-16": 1144,
    "Arena Elo (hard-en) - latest": 1144,
    "haiku_reward.K=1500": -9.27734375,
    "llama_reward.K=1500": 14.84375,
    "gpt4t_reward.K=1500": -46.09375,
    "haiku_reward.Creative Tasks.K=1500": 1.566951566951567,
    "llama_reward.Creative Tasks.K=1500": 8.60058309037901,
    "gpt4t_reward.Creative Tasks.K=1500": -43.85245901639344,
    "mixture_of_rewards.Creative Tasks.K=1500": -11.22830811968762,
    "haiku_reward.Planning & Reasoning.K=1500": -11.83574879227053,
    "llama_reward.Planning & Reasoning.K=1500": 20.0487012987013,
    "gpt4t_reward.Planning & Reasoning.K=1500": -48.1687898089172,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -13.318612434162143,
    "haiku_reward.Math & Data Analysis.K=1500": -24.173553719008265,
    "llama_reward.Math & Data Analysis.K=1500": 30.29045643153527,
    "gpt4t_reward.Math & Data Analysis.K=1500": -57.32217573221757,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -17.068424339896854,
    "haiku_reward.Information/Advice seeking.K=1500": -4.817708333333334,
    "llama_reward.Information/Advice seeking.K=1500": 12.43455497382199,
    "gpt4t_reward.Information/Advice seeking.K=1500": -39.87012987012987,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -10.751094409880404,
    "haiku_reward.Coding & Debugging.K=1500": -28.08988764044944,
    "llama_reward.Coding & Debugging.K=1500": 22.62569832402235,
    "gpt4t_reward.Coding & Debugging.K=1500": -62.77173913043478,
    "mixture_of_rewards.Coding & Debugging.K=1500": -22.745309482287293,
    "haiku_reward.task_macro.K=1500": -16.191664152933146,
    "llama_reward.task_macro.K=1500": 20.475718681157016,
    "gpt4t_reward.task_macro.K=1500": -52.17403448783193,
    "mixture_of_rewards.K=1500": -13.509114583333334,
    "task_macro_reward.K=1500": -15.963326653202687,
    "WB_score.Creative Tasks": 43.56589147286822,
    "WB_score.Planning & Reasoning": 34.401197604790426,
    "WB_score.Math & Data Analysis": 16.972111553784863,
    "WB_score.Information/Advice seeking": 39.30693069306932,
    "WB_score.Coding & Debugging": 21.9811320754717,
    "WB_score": 33.176930596285445,
    "WB_score.task_macro": 29.20277208638918,
    "Length": 2975.1876832844573,
    "Rank_ScoreMacro": 48,
    "Rank_TaskMacroReward.K": 44,
    "Rank_Avg": 46.0,
    "RewardScore_Avg": 6.619722716593247,
    "WB_Elo": 1134.2361846879544
  },
  "tulu-2-dpo-70b": {
    "Arena-Hard v0.1": "15",
    "AE2.0 LC": "21.2",
    "AE2.0": "16",
    "Arena Elo (hard-en) - 2024-07-16": 1101,
    "Arena Elo (hard-en) - latest": 1101,
    "haiku_reward.K=1500": -17.578125,
    "llama_reward.K=1500": 5.078125,
    "gpt4t_reward.K=1500": -53.61328125,
    "haiku_reward.Creative Tasks.K=1500": -3.7091988130563793,
    "llama_reward.Creative Tasks.K=1500": 4.464285714285714,
    "gpt4t_reward.Creative Tasks.K=1500": -52.259887005649716,
    "mixture_of_rewards.Creative Tasks.K=1500": -17.16826670147346,
    "haiku_reward.Planning & Reasoning.K=1500": -24.104234527687296,
    "llama_reward.Planning & Reasoning.K=1500": 6.372549019607843,
    "gpt4t_reward.Planning & Reasoning.K=1500": -59.294871794871796,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -25.67551910098375,
    "haiku_reward.Math & Data Analysis.K=1500": -35.833333333333336,
    "llama_reward.Math & Data Analysis.K=1500": 17.436974789915965,
    "gpt4t_reward.Math & Data Analysis.K=1500": -68.22033898305084,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -28.872232508822737,
    "haiku_reward.Information/Advice seeking.K=1500": -17.374005305039788,
    "llama_reward.Information/Advice seeking.K=1500": -5.98404255319149,
    "gpt4t_reward.Information/Advice seeking.K=1500": -50.5249343832021,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -24.62766074714446,
    "haiku_reward.Coding & Debugging.K=1500": -35.87570621468927,
    "llama_reward.Coding & Debugging.K=1500": 18.994413407821227,
    "gpt4t_reward.Coding & Debugging.K=1500": -68.5792349726776,
    "mixture_of_rewards.Coding & Debugging.K=1500": -28.486842593181876,
    "haiku_reward.task_macro.K=1500": -26.461760557181492,
    "llama_reward.task_macro.K=1500": 9.871534477147694,
    "gpt4t_reward.task_macro.K=1500": -61.49191931215032,
    "mixture_of_rewards.K=1500": -22.037760416666668,
    "task_macro_reward.K=1500": -26.027381797394707,
    "WB_score.Creative Tasks": 42.7012987012987,
    "WB_score.Planning & Reasoning": 32.30538922155688,
    "WB_score.Math & Data Analysis": 14.841269841269842,
    "WB_score.Information/Advice seeking": 40.69306930693068,
    "WB_score.Coding & Debugging": 20.663507109004744,
    "WB_score": 32.82502443792767,
    "WB_score.task_macro": 27.983756123225106,
    "Length": 2908.0714285714284,
    "Rank_ScoreMacro": 49,
    "Rank_TaskMacroReward.K": 53,
    "Rank_Avg": 51.0,
    "RewardScore_Avg": 0.9781871629151997,
    "WB_Elo": 1114.0029936423298
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1046,
    "Arena Elo (hard-en) - latest": 1046,
    "haiku_reward.K=1500": -16.959921798631473,
    "llama_reward.K=1500": 4.007820136852395,
    "gpt4t_reward.K=1500": -54.10557184750733,
    "haiku_reward.Creative Tasks.K=1500": -11.078717201166182,
    "llama_reward.Creative Tasks.K=1500": -8.797653958944283,
    "gpt4t_reward.Creative Tasks.K=1500": -57.2829131652661,
    "mixture_of_rewards.Creative Tasks.K=1500": -25.71976144179219,
    "haiku_reward.Planning & Reasoning.K=1500": -20.792079207920793,
    "llama_reward.Planning & Reasoning.K=1500": 6.76126878130217,
    "gpt4t_reward.Planning & Reasoning.K=1500": -57.88177339901478,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -23.970861275211135,
    "haiku_reward.Math & Data Analysis.K=1500": -21.338912133891213,
    "llama_reward.Math & Data Analysis.K=1500": 24.894514767932492,
    "gpt4t_reward.Math & Data Analysis.K=1500": -60.63829787234043,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -19.02756507943305,
    "haiku_reward.Information/Advice seeking.K=1500": -20.767195767195766,
    "llama_reward.Information/Advice seeking.K=1500": -3.590425531914894,
    "gpt4t_reward.Information/Advice seeking.K=1500": -54.629629629629626,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -26.329083642913428,
    "haiku_reward.Coding & Debugging.K=1500": -22.191011235955056,
    "llama_reward.Coding & Debugging.K=1500": 19.142857142857142,
    "gpt4t_reward.Coding & Debugging.K=1500": -65.92178770949721,
    "mixture_of_rewards.Coding & Debugging.K=1500": -22.98998060086504,
    "haiku_reward.task_macro.K=1500": -20.248953813624407,
    "llama_reward.task_macro.K=1500": 10.564847588362806,
    "gpt4t_reward.task_macro.K=1500": -60.00135327638646,
    "mixture_of_rewards.K=1500": -22.3525578364288,
    "task_macro_reward.K=1500": -23.228486500549355,
    "WB_score.Creative Tasks": 37.92207792207792,
    "WB_score.Planning & Reasoning": 34.24287856071963,
    "WB_score.Math & Data Analysis": 21.752988047808763,
    "WB_score.Information/Advice seeking": 39.75247524752476,
    "WB_score.Coding & Debugging": 26.037735849056602,
    "WB_score": 33.22233104799217,
    "WB_score.task_macro": 30.711400306676122,
    "Length": 2874.541625857003,
    "Rank_ScoreMacro": 42,
    "Rank_TaskMacroReward.K": 51,
    "Rank_Avg": 46.5,
    "RewardScore_Avg": 3.7414569030633835,
    "WB_Elo": 1080.5355552806532
  },
  "Mistral-7B-Instruct-v0.2": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "17.1",
    "AE2.0": "14.7",
    "Arena Elo (hard-en) - 2024-07-16": 1072,
    "Arena Elo (hard-en) - latest": 1072,
    "haiku_reward.K=1500": -19.873046875,
    "llama_reward.K=1500": 5.078125,
    "gpt4t_reward.K=1500": -52.783203125,
    "haiku_reward.Creative Tasks.K=1500": -3.314121037463977,
    "llama_reward.Creative Tasks.K=1500": 3.602305475504323,
    "gpt4t_reward.Creative Tasks.K=1500": -46.291208791208796,
    "mixture_of_rewards.Creative Tasks.K=1500": -15.334341451056149,
    "haiku_reward.Planning & Reasoning.K=1500": -28.074433656957932,
    "llama_reward.Planning & Reasoning.K=1500": 5.147058823529411,
    "gpt4t_reward.Planning & Reasoning.K=1500": -58.75796178343949,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -27.228445538956006,
    "haiku_reward.Math & Data Analysis.K=1500": -43.43220338983051,
    "llama_reward.Math & Data Analysis.K=1500": 7.6923076923076925,
    "gpt4t_reward.Math & Data Analysis.K=1500": -71.03004291845494,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -35.58997953865926,
    "haiku_reward.Information/Advice seeking.K=1500": -16.057441253263708,
    "llama_reward.Information/Advice seeking.K=1500": 1.7060367454068242,
    "gpt4t_reward.Information/Advice seeking.K=1500": -47.39583333333333,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -20.58241261373007,
    "haiku_reward.Coding & Debugging.K=1500": -38.73626373626374,
    "llama_reward.Coding & Debugging.K=1500": 17.857142857142858,
    "gpt4t_reward.Coding & Debugging.K=1500": -68.78306878306879,
    "mixture_of_rewards.Coding & Debugging.K=1500": -29.88739655406322,
    "haiku_reward.task_macro.K=1500": -29.609624545271025,
    "llama_reward.task_macro.K=1500": 8.321734755780065,
    "gpt4t_reward.task_macro.K=1500": -60.87343379414625,
    "mixture_of_rewards.K=1500": -22.526041666666668,
    "task_macro_reward.K=1500": -27.387107861212403,
    "WB_score.Creative Tasks": 42.072538860103634,
    "WB_score.Planning & Reasoning": 30.059880239520957,
    "WB_score.Math & Data Analysis": 10.079365079365079,
    "WB_score.Information/Advice seeking": 40.099255583126556,
    "WB_score.Coding & Debugging": 18.396226415094343,
    "WB_score": 30.694037145650057,
    "WB_score.task_macro": 25.633728318953878,
    "Length": 2832.3440860215055,
    "Rank_ScoreMacro": 51,
    "Rank_TaskMacroReward.K": 55,
    "Rank_Avg": 53.0,
    "RewardScore_Avg": -0.8766897711292625,
    "WB_Elo": 1095.8096850594482
  },
  "gpt-3.5-turbo-0125": {
    "Arena-Hard v0.1": "23.3",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1107,
    "Arena Elo (hard-en) - latest": 1107,
    "haiku_reward.K=1500": -26.3671875,
    "llama_reward.K=1500": 0.048828125,
    "gpt4t_reward.K=1500": -61.42578125,
    "haiku_reward.Creative Tasks.K=1500": -14.84593837535014,
    "llama_reward.Creative Tasks.K=1500": -5.492957746478874,
    "gpt4t_reward.Creative Tasks.K=1500": -58.6436170212766,
    "mixture_of_rewards.Creative Tasks.K=1500": -26.327504381035208,
    "haiku_reward.Planning & Reasoning.K=1500": -30.926216640502357,
    "llama_reward.Planning & Reasoning.K=1500": 3.6334913112164293,
    "gpt4t_reward.Planning & Reasoning.K=1500": -61.82380216383307,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -29.705509164373,
    "haiku_reward.Math & Data Analysis.K=1500": -35.4251012145749,
    "llama_reward.Math & Data Analysis.K=1500": 16.93877551020408,
    "gpt4t_reward.Math & Data Analysis.K=1500": -67.14285714285714,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -28.543060949075983,
    "haiku_reward.Information/Advice seeking.K=1500": -32.90816326530612,
    "llama_reward.Information/Advice seeking.K=1500": -10.714285714285714,
    "gpt4t_reward.Information/Advice seeking.K=1500": -61.33501259445844,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -34.985820524683426,
    "haiku_reward.Coding & Debugging.K=1500": -32.72251308900523,
    "llama_reward.Coding & Debugging.K=1500": 19.895287958115183,
    "gpt4t_reward.Coding & Debugging.K=1500": -70.2020202020202,
    "mixture_of_rewards.Coding & Debugging.K=1500": -27.676415110970083,
    "haiku_reward.task_macro.K=1500": -30.9663378974977,
    "llama_reward.task_macro.K=1500": 7.487906618508395,
    "gpt4t_reward.task_macro.K=1500": -64.73646380140123,
    "mixture_of_rewards.K=1500": -29.248046875,
    "task_macro_reward.K=1500": -29.404965026796845,
    "WB_score.Creative Tasks": 37.41602067183463,
    "WB_score.Planning & Reasoning": 33.3931240657698,
    "WB_score.Math & Data Analysis": 21.58730158730158,
    "WB_score.Information/Advice seeking": 36.485148514851474,
    "WB_score.Coding & Debugging": 26.54028436018958,
    "WB_score": 32.27761485826002,
    "WB_score.task_macro": 30.01598607195931,
    "Length": 1844.13880742913,
    "Rank_ScoreMacro": 45,
    "Rank_TaskMacroReward.K": 57,
    "Rank_Avg": 51.0,
    "RewardScore_Avg": 0.30551052258123335,
    "WB_Elo": 1124.5088476993683
  },
  "Qwen1.5-7B-Chat@together": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "14.7",
    "AE2.0": "11.8",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=1500": -20.107632093933464,
    "llama_reward.K=1500": 0.9286412512218964,
    "gpt4t_reward.K=1500": -54.49657869012707,
    "haiku_reward.Creative Tasks.K=1500": -2.106741573033708,
    "llama_reward.Creative Tasks.K=1500": 4.507042253521127,
    "gpt4t_reward.Creative Tasks.K=1500": -45.49071618037136,
    "mixture_of_rewards.Creative Tasks.K=1500": -14.363471833294646,
    "haiku_reward.Planning & Reasoning.K=1500": -23.80191693290735,
    "llama_reward.Planning & Reasoning.K=1500": 3.753993610223642,
    "gpt4t_reward.Planning & Reasoning.K=1500": -56.552262090483616,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -25.53339513772244,
    "haiku_reward.Math & Data Analysis.K=1500": -36.98347107438016,
    "llama_reward.Math & Data Analysis.K=1500": 6.6115702479338845,
    "gpt4t_reward.Math & Data Analysis.K=1500": -66.94214876033058,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -32.43801652892562,
    "haiku_reward.Information/Advice seeking.K=1500": -19.230769230769234,
    "llama_reward.Information/Advice seeking.K=1500": -2.6923076923076925,
    "gpt4t_reward.Information/Advice seeking.K=1500": -51.13636363636363,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -24.353146853146853,
    "haiku_reward.Coding & Debugging.K=1500": -35.32608695652174,
    "llama_reward.Coding & Debugging.K=1500": 5.376344086021505,
    "gpt4t_reward.Coding & Debugging.K=1500": -68.32460732984293,
    "mixture_of_rewards.Coding & Debugging.K=1500": -32.75811673344773,
    "haiku_reward.task_macro.K=1500": -26.604194451739016,
    "llama_reward.task_macro.K=1500": 3.843954258056866,
    "gpt4t_reward.task_macro.K=1500": -59.81807712698889,
    "mixture_of_rewards.K=1500": -24.55852317761288,
    "task_macro_reward.K=1500": -27.52610577355701,
    "WB_score.Creative Tasks": 38.29457364341085,
    "WB_score.Planning & Reasoning": 28.878923766816147,
    "WB_score.Math & Data Analysis": 11.904761904761898,
    "WB_score.Information/Advice seeking": 34.00990099009901,
    "WB_score.Coding & Debugging": 14.88151658767773,
    "WB_score": 27.370478983382203,
    "WB_score.task_macro": 23.42316313940188,
    "Length": 2519.4203323558163,
    "Rank_ScoreMacro": 54,
    "Rank_TaskMacroReward.K": 56,
    "Rank_Avg": 55.0,
    "RewardScore_Avg": -2.0514713170775654,
    "WB_Elo": 1080.6354917293063
  },
  "Llama-2-70b-chat-hf": {
    "Arena-Hard v0.1": "11.6",
    "AE2.0 LC": "14.7",
    "AE2.0": "13.9",
    "Arena Elo (hard-en) - 2024-07-16": 1071,
    "Arena Elo (hard-en) - latest": 1071,
    "haiku_reward.K=1500": -22.238514173998045,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": -53.61328125,
    "haiku_reward.Creative Tasks.K=1500": -10.237388724035608,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": -50.70422535211267,
    "mixture_of_rewards.Creative Tasks.K=1500": -20.313871358716096,
    "haiku_reward.Planning & Reasoning.K=1500": -27.696078431372552,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": -59.50080515297906,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -29.065627861450537,
    "haiku_reward.Math & Data Analysis.K=1500": -48.7551867219917,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": -72.70833333333333,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -40.48784001844168,
    "haiku_reward.Information/Advice seeking.K=1500": -12.727272727272727,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": -43.97435897435897,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -18.900543900543898,
    "haiku_reward.Coding & Debugging.K=1500": -52.77777777777778,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": -76.0989010989011,
    "mixture_of_rewards.Coding & Debugging.K=1500": -42.95889295889296,
    "haiku_reward.task_macro.K=1500": -34.528769661242514,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": -63.27175748526508,
    "mixture_of_rewards.K=1500": -25.28393180799935,
    "task_macro_reward.K=1500": -32.60017571550253,
    "WB_score.Creative Tasks": 40.0,
    "WB_score.Planning & Reasoning": 26.846846846846848,
    "WB_score.Math & Data Analysis": 4.176706827309236,
    "WB_score.Information/Advice seeking": 38.30845771144279,
    "WB_score.Coding & Debugging": 9.333333333333336,
    "WB_score": 26.9140625,
    "WB_score.task_macro": 20.659636912866645,
    "Length": 3138.3179587831205,
    "Rank_ScoreMacro": 58,
    "Rank_TaskMacroReward.K": 58,
    "Rank_Avg": 58.0,
    "RewardScore_Avg": -5.9702694013179425,
    "WB_Elo": 1088.1354760938955
  },
  "Llama-2-7b-chat-hf": {
    "Arena-Hard v0.1": "4.6",
    "AE2.0 LC": "5.4",
    "AE2.0": "5",
    "Arena Elo (hard-en) - 2024-07-16": 1012,
    "Arena Elo (hard-en) - latest": 1012,
    "haiku_reward.K=1500": -39.93157380254154,
    "llama_reward.K=1500": -24.072265625,
    "gpt4t_reward.K=1500": -66.2109375,
    "haiku_reward.Creative Tasks.K=1500": -26.308139534883722,
    "llama_reward.Creative Tasks.K=1500": -20.833333333333336,
    "gpt4t_reward.Creative Tasks.K=1500": -65.98360655737704,
    "mixture_of_rewards.Creative Tasks.K=1500": -37.70835980853136,
    "haiku_reward.Planning & Reasoning.K=1500": -48.41930116472546,
    "llama_reward.Planning & Reasoning.K=1500": -26.115702479338843,
    "gpt4t_reward.Planning & Reasoning.K=1500": -73.07692307692307,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -49.203975573662454,
    "haiku_reward.Math & Data Analysis.K=1500": -61.91489361702127,
    "llama_reward.Math & Data Analysis.K=1500": -32.35294117647059,
    "gpt4t_reward.Math & Data Analysis.K=1500": -82.47863247863248,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -58.915489090708114,
    "haiku_reward.Information/Advice seeking.K=1500": -35.826771653543304,
    "llama_reward.Information/Advice seeking.K=1500": -18.75,
    "gpt4t_reward.Information/Advice seeking.K=1500": -60.3359173126615,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -38.3042296554016,
    "haiku_reward.Coding & Debugging.K=1500": -71.5909090909091,
    "llama_reward.Coding & Debugging.K=1500": -43.575418994413404,
    "gpt4t_reward.Coding & Debugging.K=1500": -88.26815642458101,
    "mixture_of_rewards.Coding & Debugging.K=1500": -67.81149483663451,
    "haiku_reward.task_macro.K=1500": -53.042485251402894,
    "llama_reward.task_macro.K=1500": -30.30442309164791,
    "gpt4t_reward.task_macro.K=1500": -76.29552065656785,
    "mixture_of_rewards.K=1500": -43.40492564251385,
    "task_macro_reward.K=1500": -53.21414299987288,
    "WB_score.Creative Tasks": 29.76623376623376,
    "WB_score.Planning & Reasoning": 15.428571428571427,
    "WB_score.Math & Data Analysis": -7.177419354838701,
    "WB_score.Information/Advice seeking": 27.66169154228855,
    "WB_score.Coding & Debugging": -6.794258373205739,
    "WB_score": 15.225048923679054,
    "WB_score.task_macro": 8.262075264042464,
    "Length": 2985.1052114060963,
    "Rank_ScoreMacro": 59,
    "Rank_TaskMacroReward.K": 59,
    "Rank_Avg": 59.0,
    "RewardScore_Avg": -22.476033867915206,
    "WB_Elo": 1033.3690594246566
  },
  "gemma-7b-it": {
    "Arena-Hard v0.1": "7.5",
    "AE2.0 LC": "10.4",
    "AE2.0": "6.9",
    "Arena Elo (hard-en) - 2024-07-16": 1046,
    "Arena Elo (hard-en) - latest": 1046,
    "haiku_reward.K=1500": -51.318359375,
    "llama_reward.K=1500": -32.06256109481916,
    "gpt4t_reward.K=1500": -73.681640625,
    "haiku_reward.Creative Tasks.K=1500": -40.22346368715084,
    "llama_reward.Creative Tasks.K=1500": -35.79387186629526,
    "gpt4t_reward.Creative Tasks.K=1500": -69.4225721784777,
    "mixture_of_rewards.Creative Tasks.K=1500": -48.4799692439746,
    "haiku_reward.Planning & Reasoning.K=1500": -55.96546310832024,
    "llama_reward.Planning & Reasoning.K=1500": -32.148499210110586,
    "gpt4t_reward.Planning & Reasoning.K=1500": -76.46153846153845,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -54.85850025998976,
    "haiku_reward.Math & Data Analysis.K=1500": -63.96761133603239,
    "llama_reward.Math & Data Analysis.K=1500": -18.16326530612245,
    "gpt4t_reward.Math & Data Analysis.K=1500": -83.130081300813,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -55.08698598098928,
    "haiku_reward.Information/Advice seeking.K=1500": -54.20918367346938,
    "llama_reward.Information/Advice seeking.K=1500": -45.40816326530612,
    "gpt4t_reward.Information/Advice seeking.K=1500": -72.48743718592965,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -57.36826137490172,
    "haiku_reward.Coding & Debugging.K=1500": -62.36842105263158,
    "llama_reward.Coding & Debugging.K=1500": -22.5130890052356,
    "gpt4t_reward.Coding & Debugging.K=1500": -81.06060606060606,
    "mixture_of_rewards.Coding & Debugging.K=1500": -55.31403870615775,
    "haiku_reward.task_macro.K=1500": -57.40075497597887,
    "llama_reward.task_macro.K=1500": -29.145959780512083,
    "gpt4t_reward.task_macro.K=1500": -77.70728966993259,
    "mixture_of_rewards.K=1500": -52.354187031606386,
    "task_macro_reward.K=1500": -54.75133480880785,
    "WB_score.Creative Tasks": 21.19170984455959,
    "WB_score.Planning & Reasoning": 10.164424514200299,
    "WB_score.Math & Data Analysis": -3.6507936507936556,
    "WB_score.Information/Advice seeking": 12.72277227722773,
    "WB_score.Coding & Debugging": 1.8009478672985857,
    "WB_score": 10.17578125,
    "WB_score.task_macro": 6.61975914869064,
    "Length": 1726.3440860215053,
    "Rank_ScoreMacro": 60,
    "Rank_TaskMacroReward.K": 60,
    "Rank_Avg": 60.0,
    "RewardScore_Avg": -24.065787830058603,
    "WB_Elo": 1058.181990260453
  },
  "gemma-2b-it": {
    "Arena-Hard v0.1": "3",
    "AE2.0 LC": "5.4",
    "AE2.0": "3.4",
    "Arena Elo (hard-en) - 2024-07-16": 977,
    "Arena Elo (hard-en) - latest": 977,
    "haiku_reward.K=1500": -68.701171875,
    "llama_reward.K=1500": -55.37109375,
    "gpt4t_reward.K=1500": -84.228515625,
    "haiku_reward.Creative Tasks.K=1500": -60.893854748603346,
    "llama_reward.Creative Tasks.K=1500": -56.824512534818936,
    "gpt4t_reward.Creative Tasks.K=1500": -80.4461942257218,
    "mixture_of_rewards.Creative Tasks.K=1500": -66.05485383638135,
    "haiku_reward.Planning & Reasoning.K=1500": -74.88226059654632,
    "llama_reward.Planning & Reasoning.K=1500": -57.74091627172196,
    "gpt4t_reward.Planning & Reasoning.K=1500": -86.51771956856703,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -73.0469654789451,
    "haiku_reward.Math & Data Analysis.K=1500": -76.92307692307693,
    "llama_reward.Math & Data Analysis.K=1500": -46.54471544715447,
    "gpt4t_reward.Math & Data Analysis.K=1500": -90.2439024390244,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -71.23723160308526,
    "haiku_reward.Information/Advice seeking.K=1500": -75.0,
    "llama_reward.Information/Advice seeking.K=1500": -64.3765903307888,
    "gpt4t_reward.Information/Advice seeking.K=1500": -85.55276381909547,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -74.97645138329476,
    "haiku_reward.Coding & Debugging.K=1500": -79.73684210526316,
    "llama_reward.Coding & Debugging.K=1500": -58.15789473684211,
    "gpt4t_reward.Coding & Debugging.K=1500": -91.87817258883248,
    "mixture_of_rewards.Coding & Debugging.K=1500": -76.59096981031259,
    "haiku_reward.task_macro.K=1500": -75.13555424729273,
    "llama_reward.task_macro.K=1500": -56.44482699439172,
    "gpt4t_reward.task_macro.K=1500": -87.92135528693817,
    "mixture_of_rewards.K=1500": -69.43359375,
    "task_macro_reward.K=1500": -73.16724550954088,
    "WB_score.Creative Tasks": 7.220779220779221,
    "WB_score.Planning & Reasoning": -5.795795795795797,
    "WB_score.Math & Data Analysis": -18.64541832669323,
    "WB_score.Information/Advice seeking": -2.133995037220835,
    "WB_score.Coding & Debugging": -17.725118483412317,
    "WB_score": -5.249755142017634,
    "WB_score.task_macro": -9.691930072258819,
    "Length": 1590.0833333333333,
    "Rank_ScoreMacro": 61,
    "Rank_TaskMacroReward.K": 61,
    "Rank_Avg": 61.0,
    "RewardScore_Avg": -41.429587790899845,
    "WB_Elo": 996.5652530810216
  },
  "Llama-3-Instruct-8B-SimPO": {
    "Arena-Hard v0.1": "33.8",
    "AE2.0 LC": "44.7",
    "AE2.0": "40.5",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=1500": 17.236328125,
    "llama_reward.K=1500": 43.30400782013685,
    "gpt4t_reward.K=1500": -21.142578125,
    "haiku_reward.Creative Tasks.K=1500": 30.919220055710305,
    "llama_reward.Creative Tasks.K=1500": 45.65826330532213,
    "gpt4t_reward.Creative Tasks.K=1500": -13.324538258575197,
    "mixture_of_rewards.Creative Tasks.K=1500": 21.084315034152414,
    "haiku_reward.Planning & Reasoning.K=1500": 16.27172195892575,
    "llama_reward.Planning & Reasoning.K=1500": 44.61172741679873,
    "gpt4t_reward.Planning & Reasoning.K=1500": -22.993827160493826,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 12.629874071743552,
    "haiku_reward.Math & Data Analysis.K=1500": -0.8097165991902834,
    "llama_reward.Math & Data Analysis.K=1500": 47.357723577235774,
    "gpt4t_reward.Math & Data Analysis.K=1500": -41.46341463414634,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 1.6948641146330512,
    "haiku_reward.Information/Advice seeking.K=1500": 28.205128205128204,
    "llama_reward.Information/Advice seeking.K=1500": 43.73401534526854,
    "gpt4t_reward.Information/Advice seeking.K=1500": -6.423173803526448,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 21.8386565822901,
    "haiku_reward.Coding & Debugging.K=1500": -2.127659574468085,
    "llama_reward.Coding & Debugging.K=1500": 53.421052631578945,
    "gpt4t_reward.Coding & Debugging.K=1500": -41.3265306122449,
    "mixture_of_rewards.Coding & Debugging.K=1500": 3.3222874816219865,
    "haiku_reward.task_macro.K=1500": 11.259751065912232,
    "llama_reward.task_macro.K=1500": 47.479650905120074,
    "gpt4t_reward.task_macro.K=1500": -28.07231817515835,
    "mixture_of_rewards.K=1500": 13.132585940045617,
    "task_macro_reward.K=1500": 10.222361265291319,
    "WB_score.Creative Tasks": 50.64599483204134,
    "WB_score.Planning & Reasoning": 40.86696562032884,
    "WB_score.Math & Data Analysis": 23.984063745019917,
    "WB_score.Information/Advice seeking": 47.871287128712865,
    "WB_score.Coding & Debugging": 31.753554502369674,
    "WB_score": 41.17302052785924,
    "WB_score.task_macro": 37.049721402304925,
    "Length": 2541.9257086999023,
    "Rank_ScoreMacro": 35,
    "Rank_TaskMacroReward.K": 18,
    "Rank_Avg": 26.5,
    "RewardScore_Avg": 23.636041333798122,
    "WB_Elo": 1147.2753876490617
  },
  "SELM-Zephyr-7B-iter-3": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "24.00",
    "AE2.0": "-",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=1500": -3.173828125,
    "llama_reward.K=1500": 20.263671875,
    "gpt4t_reward.K=1500": -36.962890625,
    "haiku_reward.Creative Tasks.K=1500": 14.730878186968837,
    "llama_reward.Creative Tasks.K=1500": 24.641833810888254,
    "gpt4t_reward.Creative Tasks.K=1500": -29.166666666666668,
    "mixture_of_rewards.Creative Tasks.K=1500": 3.402015110396809,
    "haiku_reward.Planning & Reasoning.K=1500": -4.5600000000000005,
    "llama_reward.Planning & Reasoning.K=1500": 24.232633279483036,
    "gpt4t_reward.Planning & Reasoning.K=1500": -39.19558359621451,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -6.5076501055771585,
    "haiku_reward.Math & Data Analysis.K=1500": -24.173553719008265,
    "llama_reward.Math & Data Analysis.K=1500": 24.688796680497926,
    "gpt4t_reward.Math & Data Analysis.K=1500": -54.37499999999999,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -17.95325234617011,
    "haiku_reward.Information/Advice seeking.K=1500": 6.151832460732985,
    "llama_reward.Information/Advice seeking.K=1500": 25.654450261780106,
    "gpt4t_reward.Information/Advice seeking.K=1500": -24.870466321243523,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 2.3119388004231887,
    "haiku_reward.Coding & Debugging.K=1500": -38.39779005524862,
    "llama_reward.Coding & Debugging.K=1500": 9.217877094972067,
    "gpt4t_reward.Coding & Debugging.K=1500": -61.08108108108108,
    "mixture_of_rewards.Coding & Debugging.K=1500": -30.086998013785877,
    "haiku_reward.task_macro.K=1500": -13.871890073481028,
    "llama_reward.task_macro.K=1500": 20.64498721059916,
    "gpt4t_reward.task_macro.K=1500": -44.833055562291676,
    "mixture_of_rewards.K=1500": -6.624348958333333,
    "task_macro_reward.K=1500": -12.68665280839118,
    "WB_score.Creative Tasks": 44.70284237726098,
    "WB_score.Planning & Reasoning": 31.58682634730539,
    "WB_score.Math & Data Analysis": 12.669322709163353,
    "WB_score.Information/Advice seeking": 40.99009900990099,
    "WB_score.Coding & Debugging": 11.037735849056602,
    "WB_score": 31.5234375,
    "WB_score.task_macro": 25.061899136983598,
    "Length": 2823.7800586510266,
    "Rank_ScoreMacro": 52,
    "Rank_TaskMacroReward.K": 42,
    "Rank_Avg": 47.0,
    "RewardScore_Avg": 6.187623164296209,
    "WB_Elo": 1122.6875139363128
  },
  "Qwen2-72B-Instruct": {
    "Arena-Hard v0.1": "48.1",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1182,
    "Arena Elo (hard-en) - latest": 1182,
    "haiku_reward.K=1500": 11.865234375,
    "llama_reward.K=1500": 33.056640625,
    "gpt4t_reward.K=1500": -33.49609375,
    "haiku_reward.Creative Tasks.K=1500": 16.056338028169016,
    "llama_reward.Creative Tasks.K=1500": 23.579545454545457,
    "gpt4t_reward.Creative Tasks.K=1500": -32.93010752688172,
    "mixture_of_rewards.Creative Tasks.K=1500": 2.235258651944252,
    "haiku_reward.Planning & Reasoning.K=1500": 11.863057324840764,
    "llama_reward.Planning & Reasoning.K=1500": 38.24476650563607,
    "gpt4t_reward.Planning & Reasoning.K=1500": -34.53689167974882,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 5.190310716909337,
    "haiku_reward.Math & Data Analysis.K=1500": 17.28395061728395,
    "llama_reward.Math & Data Analysis.K=1500": 56.84647302904564,
    "gpt4t_reward.Math & Data Analysis.K=1500": -37.1900826446281,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 12.313447000567164,
    "haiku_reward.Information/Advice seeking.K=1500": 11.72680412371134,
    "llama_reward.Information/Advice seeking.K=1500": 29.404145077720205,
    "gpt4t_reward.Information/Advice seeking.K=1500": -26.854219948849106,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 4.758909750860814,
    "haiku_reward.Coding & Debugging.K=1500": 4.25531914893617,
    "llama_reward.Coding & Debugging.K=1500": 48.40425531914894,
    "gpt4t_reward.Coding & Debugging.K=1500": -48.4375,
    "mixture_of_rewards.Coding & Debugging.K=1500": 1.4073581560283703,
    "haiku_reward.task_macro.K=1500": 11.422145961341712,
    "llama_reward.task_macro.K=1500": 41.89486986222943,
    "gpt4t_reward.task_macro.K=1500": -37.3713244146088,
    "mixture_of_rewards.K=1500": 3.80859375,
    "task_macro_reward.K=1500": 5.3152304696541135,
    "WB_score.Creative Tasks": 49.92248062015504,
    "WB_score.Planning & Reasoning": 46.84603886397609,
    "WB_score.Math & Data Analysis": 40.95238095238095,
    "WB_score.Information/Advice seeking": 49.50495049504951,
    "WB_score.Coding & Debugging": 39.81132075471699,
    "WB_score": 46.40625,
    "WB_score.task_macro": 44.497691296234095,
    "Length": 2856.4482421875,
    "Rank_ScoreMacro": 25,
    "Rank_TaskMacroReward.K": 20,
    "Rank_Avg": 22.5,
    "RewardScore_Avg": 24.906460882944103,
    "WB_Elo": 1176.3672811642498
  },
  "Hermes-2-Theta-Llama-3-8B": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=1500": -15.591397849462366,
    "llama_reward.K=1500": 8.651026392961878,
    "gpt4t_reward.K=1500": -54.39882697947213,
    "haiku_reward.Creative Tasks.K=1500": -9.686609686609685,
    "llama_reward.Creative Tasks.K=1500": -1.5714285714285716,
    "gpt4t_reward.Creative Tasks.K=1500": -55.28455284552846,
    "mixture_of_rewards.Creative Tasks.K=1500": -22.18086370118891,
    "haiku_reward.Planning & Reasoning.K=1500": -21.12,
    "llama_reward.Planning & Reasoning.K=1500": 11.316211878009632,
    "gpt4t_reward.Planning & Reasoning.K=1500": -57.17665615141956,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -22.32681475780331,
    "haiku_reward.Math & Data Analysis.K=1500": -27.160493827160494,
    "llama_reward.Math & Data Analysis.K=1500": 29.303278688524593,
    "gpt4t_reward.Math & Data Analysis.K=1500": -63.01652892561983,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -20.29124802141858,
    "haiku_reward.Information/Advice seeking.K=1500": -13.178294573643413,
    "llama_reward.Information/Advice seeking.K=1500": 4.521963824289406,
    "gpt4t_reward.Information/Advice seeking.K=1500": -47.32142857142857,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -18.659253106927526,
    "haiku_reward.Coding & Debugging.K=1500": -29.444444444444446,
    "llama_reward.Coding & Debugging.K=1500": 19.166666666666668,
    "gpt4t_reward.Coding & Debugging.K=1500": -69.67213114754098,
    "mixture_of_rewards.Coding & Debugging.K=1500": -26.649969641772923,
    "haiku_reward.task_macro.K=1500": -22.12485792763231,
    "llama_reward.task_macro.K=1500": 14.739502200394258,
    "gpt4t_reward.task_macro.K=1500": -59.93912861066445,
    "mixture_of_rewards.K=1500": -20.44639947865754,
    "task_macro_reward.K=1500": -22.441494779300836,
    "WB_score.Creative Tasks": 39.79328165374676,
    "WB_score.Planning & Reasoning": 33.65269461077844,
    "WB_score.Math & Data Analysis": 18.725099601593627,
    "WB_score.Information/Advice seeking": 41.584158415841586,
    "WB_score.Coding & Debugging": 23.113207547169807,
    "WB_score": 32.9423264907136,
    "WB_score.task_macro": 29.635207776375477,
    "Length": 2742.169110459433,
    "Rank_ScoreMacro": 46,
    "Rank_TaskMacroReward.K": 49,
    "Rank_Avg": 47.5,
    "RewardScore_Avg": 3.5968564985373206,
    "WB_Elo": 1123.1201291077534
  },
  "yi-large": {
    "Arena-Hard v0.1": "63.7",
    "AE2.0 LC": "51.9",
    "AE2.0": "57.5",
    "Arena Elo (hard-en) - 2024-07-16": 1198,
    "Arena Elo (hard-en) - latest": 1198,
    "haiku_reward.K=1500": 24.31640625,
    "llama_reward.K=1500": 40.99804305283757,
    "gpt4t_reward.K=1500": -22.015655577299412,
    "haiku_reward.Creative Tasks.K=1500": 29.310344827586203,
    "llama_reward.Creative Tasks.K=1500": 34.285714285714285,
    "gpt4t_reward.Creative Tasks.K=1500": -18.37837837837838,
    "mixture_of_rewards.Creative Tasks.K=1500": 15.072560244974035,
    "haiku_reward.Planning & Reasoning.K=1500": 26.40449438202247,
    "llama_reward.Planning & Reasoning.K=1500": 47.80487804878049,
    "gpt4t_reward.Planning & Reasoning.K=1500": -23.30173775671406,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 16.96921155802963,
    "haiku_reward.Math & Data Analysis.K=1500": 30.991735537190085,
    "llama_reward.Math & Data Analysis.K=1500": 60.25104602510461,
    "gpt4t_reward.Math & Data Analysis.K=1500": -24.583333333333332,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 22.219816076320456,
    "haiku_reward.Information/Advice seeking.K=1500": 24.415584415584416,
    "llama_reward.Information/Advice seeking.K=1500": 37.109375,
    "gpt4t_reward.Information/Advice seeking.K=1500": -16.496163682864452,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 15.009598577573323,
    "haiku_reward.Coding & Debugging.K=1500": 20.165745856353592,
    "llama_reward.Coding & Debugging.K=1500": 59.94475138121547,
    "gpt4t_reward.Coding & Debugging.K=1500": -40.106951871657756,
    "mixture_of_rewards.Coding & Debugging.K=1500": 13.334515121970435,
    "haiku_reward.task_macro.K=1500": 25.720294012766647,
    "llama_reward.task_macro.K=1500": 50.50799393799088,
    "gpt4t_reward.task_macro.K=1500": -26.401159435818027,
    "mixture_of_rewards.K=1500": 14.432931241846054,
    "task_macro_reward.K=1500": 16.60904283831317,
    "WB_score.Creative Tasks": 51.80156657963445,
    "WB_score.Planning & Reasoning": 51.33834586466165,
    "WB_score.Math & Data Analysis": 44.46215139442231,
    "WB_score.Information/Advice seeking": 50.96774193548388,
    "WB_score.Coding & Debugging": 47.71428571428572,
    "WB_score": 48.93450635386118,
    "WB_score.task_macro": 48.92726960200772,
    "Length": 3095.335952848723,
    "Rank_ScoreMacro": 14,
    "Rank_TaskMacroReward.K": 13,
    "Rank_Avg": 13.5,
    "RewardScore_Avg": 32.76815622016044,
    "WB_Elo": 1189.4528661631805
  },
  "Yi-1.5-34B-Chat": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1160,
    "Arena Elo (hard-en) - latest": 1160,
    "haiku_reward.K=1500": 19.62890625,
    "llama_reward.K=1500": 38.6119257086999,
    "gpt4t_reward.K=1500": -18.359375,
    "haiku_reward.Creative Tasks.K=1500": 28.818443804034583,
    "llama_reward.Creative Tasks.K=1500": 36.887608069164266,
    "gpt4t_reward.Creative Tasks.K=1500": -12.121212121212121,
    "mixture_of_rewards.Creative Tasks.K=1500": 17.86161325066224,
    "haiku_reward.Planning & Reasoning.K=1500": 21.809369951534734,
    "llama_reward.Planning & Reasoning.K=1500": 46.016260162601625,
    "gpt4t_reward.Planning & Reasoning.K=1500": -17.8343949044586,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 16.663745069892588,
    "haiku_reward.Math & Data Analysis.K=1500": 21.638655462184875,
    "llama_reward.Math & Data Analysis.K=1500": 52.928870292887034,
    "gpt4t_reward.Math & Data Analysis.K=1500": -28.8135593220339,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 15.251322144346005,
    "haiku_reward.Information/Advice seeking.K=1500": 20.12987012987013,
    "llama_reward.Information/Advice seeking.K=1500": 39.0625,
    "gpt4t_reward.Information/Advice seeking.K=1500": -11.568123393316196,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 15.874748912184643,
    "haiku_reward.Coding & Debugging.K=1500": 7.18232044198895,
    "llama_reward.Coding & Debugging.K=1500": 46.408839779005525,
    "gpt4t_reward.Coding & Debugging.K=1500": -38.44086021505376,
    "mixture_of_rewards.Coding & Debugging.K=1500": 5.050100001980238,
    "haiku_reward.task_macro.K=1500": 18.396819569159867,
    "llama_reward.task_macro.K=1500": 45.515984053574336,
    "gpt4t_reward.task_macro.K=1500": -23.97775301621422,
    "mixture_of_rewards.K=1500": 13.2938189862333,
    "task_macro_reward.K=1500": 13.31168353550666,
    "WB_score.Creative Tasks": 53.523316062176164,
    "WB_score.Planning & Reasoning": 48.108108108108105,
    "WB_score.Math & Data Analysis": 39.43775100401606,
    "WB_score.Information/Advice seeking": 50.29702970297029,
    "WB_score.Coding & Debugging": 42.08530805687204,
    "WB_score": 47.350928641251215,
    "WB_score.task_macro": 45.613463477590955,
    "Length": 3523.557843137255,
    "Rank_ScoreMacro": 23,
    "Rank_TaskMacroReward.K": 14,
    "Rank_Avg": 18.5,
    "RewardScore_Avg": 29.462573506548807,
    "WB_Elo": 1162.7919494137102
  },
  "reka-flash-20240226": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1127,
    "Arena Elo (hard-en) - latest": 1127,
    "haiku_reward.K=1500": -4.296875,
    "llama_reward.K=1500": 22.021484375,
    "gpt4t_reward.K=1500": -42.236328125,
    "haiku_reward.Creative Tasks.K=1500": 2.801120448179272,
    "llama_reward.Creative Tasks.K=1500": 14.623955431754876,
    "gpt4t_reward.Creative Tasks.K=1500": -39.050131926121374,
    "mixture_of_rewards.Creative Tasks.K=1500": -7.208352015395742,
    "haiku_reward.Planning & Reasoning.K=1500": -6.692913385826772,
    "llama_reward.Planning & Reasoning.K=1500": 23.77567140600316,
    "gpt4t_reward.Planning & Reasoning.K=1500": -45.069337442218796,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -9.32885980734747,
    "haiku_reward.Math & Data Analysis.K=1500": -14.634146341463413,
    "llama_reward.Math & Data Analysis.K=1500": 39.83739837398374,
    "gpt4t_reward.Math & Data Analysis.K=1500": -53.46938775510204,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -9.422045240860571,
    "haiku_reward.Information/Advice seeking.K=1500": -3.1969309462915603,
    "llama_reward.Information/Advice seeking.K=1500": 12.372448979591837,
    "gpt4t_reward.Information/Advice seeking.K=1500": -36.649874055415616,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -9.158118674038446,
    "haiku_reward.Coding & Debugging.K=1500": -16.75392670157068,
    "llama_reward.Coding & Debugging.K=1500": 40.36458333333333,
    "gpt4t_reward.Coding & Debugging.K=1500": -53.55329949238579,
    "mixture_of_rewards.Coding & Debugging.K=1500": -9.980880953541048,
    "haiku_reward.task_macro.K=1500": -9.461017776071422,
    "llama_reward.task_macro.K=1500": 28.75874342078435,
    "gpt4t_reward.task_macro.K=1500": -47.107401142732655,
    "mixture_of_rewards.K=1500": -8.170572916666666,
    "task_macro_reward.K=1500": -9.269891832673244,
    "WB_score.Creative Tasks": 42.44155844155845,
    "WB_score.Planning & Reasoning": 35.01501501501501,
    "WB_score.Math & Data Analysis": 20.48,
    "WB_score.Information/Advice seeking": 41.53465346534654,
    "WB_score.Coding & Debugging": 22.085308056872037,
    "WB_score": 34.60410557184751,
    "WB_score.task_macro": 30.363615402031144,
    "Length": 2103.0098039215686,
    "Rank_ScoreMacro": 43,
    "Rank_TaskMacroReward.K": 40,
    "Rank_Avg": 41.5,
    "RewardScore_Avg": 10.54686178467895,
    "WB_Elo": 1130.7430402344874
  },
  "gemini-1.5-pro": {
    "Arena-Hard v0.1": "72.0",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1245,
    "Arena Elo (hard-en) - latest": 1245,
    "haiku_reward.K=1500": 36.767578125,
    "llama_reward.K=1500": 47.802734375,
    "gpt4t_reward.K=1500": -1.906158357771261,
    "haiku_reward.Creative Tasks.K=1500": 42.15384615384615,
    "llama_reward.Creative Tasks.K=1500": 46.08150470219436,
    "gpt4t_reward.Creative Tasks.K=1500": -8.208955223880597,
    "mixture_of_rewards.Creative Tasks.K=1500": 26.675465210719974,
    "haiku_reward.Planning & Reasoning.K=1500": 42.63698630136986,
    "llama_reward.Planning & Reasoning.K=1500": 58.63557858376511,
    "gpt4t_reward.Planning & Reasoning.K=1500": -0.5952380952380952,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 33.559108929965625,
    "haiku_reward.Math & Data Analysis.K=1500": 44.25531914893617,
    "llama_reward.Math & Data Analysis.K=1500": 68.24034334763948,
    "gpt4t_reward.Math & Data Analysis.K=1500": -3.67965367965368,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 36.27200293897399,
    "haiku_reward.Information/Advice seeking.K=1500": 38.37837837837838,
    "llama_reward.Information/Advice seeking.K=1500": 49.04632152588556,
    "gpt4t_reward.Information/Advice seeking.K=1500": 1.891891891891892,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 29.772197265385277,
    "haiku_reward.Coding & Debugging.K=1500": 55.095541401273884,
    "llama_reward.Coding & Debugging.K=1500": 75.3125,
    "gpt4t_reward.Coding & Debugging.K=1500": 9.375,
    "mixture_of_rewards.Coding & Debugging.K=1500": 46.594347133757964,
    "haiku_reward.task_macro.K=1500": 45.532986450094256,
    "llama_reward.task_macro.K=1500": 62.21072317323312,
    "gpt4t_reward.task_macro.K=1500": 0.9704189491110149,
    "mixture_of_rewards.K=1500": 27.55471804740958,
    "task_macro_reward.K=1500": 36.23804285747946,
    "WB_score.Creative Tasks": 55.124653739612185,
    "WB_score.Planning & Reasoning": 53.73271889400922,
    "WB_score.Math & Data Analysis": 48.59437751004016,
    "WB_score.Information/Advice seeking": 52.22506393861893,
    "WB_score.Coding & Debugging": 55.223880597014926,
    "WB_score": 47.3828125,
    "WB_score.task_macro": 52.95184246265066,
    "Length": 3247.9673135852913,
    "Rank_ScoreMacro": 11,
    "Rank_TaskMacroReward.K": 4,
    "Rank_Avg": 7.5,
    "RewardScore_Avg": 44.59494266006506,
    "WB_Elo": 1221.1308875018053
  },
  "gemini-1.5-flash": {
    "Arena-Hard v0.1": "49.6",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1212,
    "Arena Elo (hard-en) - latest": 1212,
    "haiku_reward.K=1500": 24.462890625,
    "llama_reward.K=1500": 39.55078125,
    "gpt4t_reward.K=1500": -12.158203125,
    "haiku_reward.Creative Tasks.K=1500": 28.57142857142857,
    "llama_reward.Creative Tasks.K=1500": 34.95145631067961,
    "gpt4t_reward.Creative Tasks.K=1500": -15.57632398753894,
    "mixture_of_rewards.Creative Tasks.K=1500": 15.982186964856412,
    "haiku_reward.Planning & Reasoning.K=1500": 28.942807625649912,
    "llama_reward.Planning & Reasoning.K=1500": 49.47735191637631,
    "gpt4t_reward.Planning & Reasoning.K=1500": -12.716262975778548,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 21.90129885541589,
    "haiku_reward.Math & Data Analysis.K=1500": 29.74137931034483,
    "llama_reward.Math & Data Analysis.K=1500": 61.30434782608696,
    "gpt4t_reward.Math & Data Analysis.K=1500": -12.06140350877193,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 26.32810787588662,
    "haiku_reward.Information/Advice seeking.K=1500": 23.26869806094183,
    "llama_reward.Information/Advice seeking.K=1500": 38.9196675900277,
    "gpt4t_reward.Information/Advice seeking.K=1500": -13.873626373626374,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 16.10491309244772,
    "haiku_reward.Coding & Debugging.K=1500": 41.66666666666667,
    "llama_reward.Coding & Debugging.K=1500": 69.62025316455697,
    "gpt4t_reward.Coding & Debugging.K=1500": -10.509554140127388,
    "mixture_of_rewards.Coding & Debugging.K=1500": 33.59245523036542,
    "haiku_reward.task_macro.K=1500": 31.524307369928685,
    "llama_reward.task_macro.K=1500": 54.07196484739245,
    "gpt4t_reward.task_macro.K=1500": -12.481485616346022,
    "mixture_of_rewards.K=1500": 17.28515625,
    "task_macro_reward.K=1500": 24.371595533658375,
    "WB_score.Creative Tasks": 51.65745856353592,
    "WB_score.Planning & Reasoning": 50.78582434514638,
    "WB_score.Math & Data Analysis": 45.322580645161295,
    "WB_score.Information/Advice seeking": 48.66666666666667,
    "WB_score.Coding & Debugging": 48.72549019607844,
    "WB_score": 44.14872798434443,
    "WB_score.task_macro": 48.85062170599164,
    "Length": 3654.3993871297243,
    "Rank_ScoreMacro": 15,
    "Rank_TaskMacroReward.K": 7,
    "Rank_Avg": 11.0,
    "RewardScore_Avg": 36.61110861982501,
    "WB_Elo": 1196.8042835473223
  },
  "reka-core-20240501": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1175,
    "Arena Elo (hard-en) - latest": 1175,
    "haiku_reward.K=1500": 25.732421875,
    "llama_reward.K=1500": 42.724609375,
    "gpt4t_reward.K=1500": -21.337890625,
    "haiku_reward.Creative Tasks.K=1500": 34.95575221238938,
    "llama_reward.Creative Tasks.K=1500": 40.680473372781066,
    "gpt4t_reward.Creative Tasks.K=1500": -17.6056338028169,
    "mixture_of_rewards.Creative Tasks.K=1500": 19.34353059411785,
    "haiku_reward.Planning & Reasoning.K=1500": 27.520661157024794,
    "llama_reward.Planning & Reasoning.K=1500": 48.336106489184694,
    "gpt4t_reward.Planning & Reasoning.K=1500": -24.634146341463413,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 17.074207101582026,
    "haiku_reward.Math & Data Analysis.K=1500": 27.729257641921397,
    "llama_reward.Math & Data Analysis.K=1500": 59.82532751091703,
    "gpt4t_reward.Math & Data Analysis.K=1500": -26.87224669603524,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 20.227446152267728,
    "haiku_reward.Information/Advice seeking.K=1500": 22.983870967741936,
    "llama_reward.Information/Advice seeking.K=1500": 40.97035040431267,
    "gpt4t_reward.Information/Advice seeking.K=1500": -19.518716577540108,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 14.811834931504833,
    "haiku_reward.Coding & Debugging.K=1500": 23.497267759562842,
    "llama_reward.Coding & Debugging.K=1500": 60.773480662983424,
    "gpt4t_reward.Coding & Debugging.K=1500": -37.16577540106952,
    "mixture_of_rewards.Coding & Debugging.K=1500": 15.701657673825581,
    "haiku_reward.task_macro.K=1500": 26.57209434098044,
    "llama_reward.task_macro.K=1500": 52.059012829632046,
    "gpt4t_reward.task_macro.K=1500": -26.855579622897885,
    "mixture_of_rewards.K=1500": 15.706380208333334,
    "task_macro_reward.K=1500": 17.258509182571533,
    "WB_score.Creative Tasks": 55.4874651810585,
    "WB_score.Planning & Reasoning": 48.00632911392405,
    "WB_score.Math & Data Analysis": 40.34188034188034,
    "WB_score.Information/Advice seeking": 52.254641909814325,
    "WB_score.Coding & Debugging": 40.60301507537689,
    "WB_score": 41.03515625,
    "WB_score.task_macro": 45.90279465292558,
    "Length": 2592.589397089397,
    "Rank_ScoreMacro": 21,
    "Rank_TaskMacroReward.K": 12,
    "Rank_Avg": 16.5,
    "RewardScore_Avg": 31.580651917748554,
    "WB_Elo": 1174.6634120985846
  },
  "yi-large-preview": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1229,
    "Arena Elo (hard-en) - latest": 1229,
    "haiku_reward.K=1500": 39.013671875,
    "llama_reward.K=1500": 52.05278592375366,
    "gpt4t_reward.K=1500": 1.270772238514174,
    "haiku_reward.Creative Tasks.K=1500": 44.42815249266862,
    "llama_reward.Creative Tasks.K=1500": 47.337278106508876,
    "gpt4t_reward.Creative Tasks.K=1500": 2.380952380952381,
    "mixture_of_rewards.Creative Tasks.K=1500": 31.382127660043295,
    "haiku_reward.Planning & Reasoning.K=1500": 44.09836065573771,
    "llama_reward.Planning & Reasoning.K=1500": 60.39933444259567,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0.6462035541195477,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 35.04796621748431,
    "haiku_reward.Math & Data Analysis.K=1500": 48.541666666666664,
    "llama_reward.Math & Data Analysis.K=1500": 71.39830508474576,
    "gpt4t_reward.Math & Data Analysis.K=1500": -1.4705882352941175,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 39.48979450537277,
    "haiku_reward.Information/Advice seeking.K=1500": 37.04188481675393,
    "llama_reward.Information/Advice seeking.K=1500": 54.089709762532976,
    "gpt4t_reward.Information/Advice seeking.K=1500": 9.220779220779221,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 33.45079126668871,
    "haiku_reward.Coding & Debugging.K=1500": 48.01136363636363,
    "llama_reward.Coding & Debugging.K=1500": 74.71264367816092,
    "gpt4t_reward.Coding & Debugging.K=1500": -8.707865168539326,
    "mixture_of_rewards.Coding & Debugging.K=1500": 38.00538071532841,
    "haiku_reward.task_macro.K=1500": 44.98406566119855,
    "llama_reward.task_macro.K=1500": 64.11035770754646,
    "gpt4t_reward.task_macro.K=1500": -0.7245483467912072,
    "mixture_of_rewards.K=1500": 30.779076679089275,
    "task_macro_reward.K=1500": 36.123291673984596,
    "WB_score.Creative Tasks": 57.64397905759162,
    "WB_score.Planning & Reasoning": 56.606606606606604,
    "WB_score.Math & Data Analysis": 51.92,
    "WB_score.Information/Advice seeking": 57.72277227722773,
    "WB_score.Coding & Debugging": 54.28571428571429,
    "WB_score": 54.83870967741936,
    "WB_score.task_macro": 55.294625232024785,
    "Length": 3512.678149606299,
    "Rank_ScoreMacro": 5,
    "Rank_TaskMacroReward.K": 5,
    "Rank_Avg": 5.0,
    "RewardScore_Avg": 45.70895845300469,
    "WB_Elo": 1216.4791603920064
  },
  "nemotron-4-340b-instruct": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1185,
    "Arena Elo (hard-en) - latest": 1185,
    "haiku_reward.K=1500": 28.076171875,
    "llama_reward.K=1500": 43.45703125,
    "gpt4t_reward.K=1500": -19.775390625,
    "haiku_reward.Creative Tasks.K=1500": 31.676136363636363,
    "llama_reward.Creative Tasks.K=1500": 38.319088319088316,
    "gpt4t_reward.Creative Tasks.K=1500": -16.93548387096774,
    "mixture_of_rewards.Creative Tasks.K=1500": 17.686580270585647,
    "haiku_reward.Planning & Reasoning.K=1500": 26.484751203852326,
    "llama_reward.Planning & Reasoning.K=1500": 47.99035369774919,
    "gpt4t_reward.Planning & Reasoning.K=1500": -23.5062893081761,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 16.989605197808476,
    "haiku_reward.Math & Data Analysis.K=1500": 29.831932773109244,
    "llama_reward.Math & Data Analysis.K=1500": 59.95850622406639,
    "gpt4t_reward.Math & Data Analysis.K=1500": -27.100840336134453,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 20.89653288701373,
    "haiku_reward.Information/Advice seeking.K=1500": 26.288659793814436,
    "llama_reward.Information/Advice seeking.K=1500": 39.97429305912596,
    "gpt4t_reward.Information/Advice seeking.K=1500": -15.101522842639595,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 17.0538100034336,
    "haiku_reward.Coding & Debugging.K=1500": 36.96808510638298,
    "llama_reward.Coding & Debugging.K=1500": 65.05376344086021,
    "gpt4t_reward.Coding & Debugging.K=1500": -22.5130890052356,
    "mixture_of_rewards.Coding & Debugging.K=1500": 26.502919847335864,
    "haiku_reward.task_macro.K=1500": 30.463692881701554,
    "llama_reward.task_macro.K=1500": 52.71661402130879,
    "gpt4t_reward.task_macro.K=1500": -21.982936272707924,
    "mixture_of_rewards.K=1500": 17.252604166666668,
    "task_macro_reward.K=1500": 20.39912354343414,
    "WB_score.Creative Tasks": 53.3160621761658,
    "WB_score.Planning & Reasoning": 49.12912912912914,
    "WB_score.Math & Data Analysis": 40.80321285140562,
    "WB_score.Information/Advice seeking": 53.00248138957816,
    "WB_score.Coding & Debugging": 46.25592417061611,
    "WB_score": 48.84765625,
    "WB_score.task_macro": 47.67250981186394,
    "Length": 2754.0098039215686,
    "Rank_ScoreMacro": 19,
    "Rank_TaskMacroReward.K": 10,
    "Rank_Avg": 14.5,
    "RewardScore_Avg": 34.03581667764904,
    "WB_Elo": 1184.3773976027633
  },
  "claude-3-5-sonnet-20240620": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1271,
    "Arena Elo (hard-en) - latest": 1271,
    "haiku_reward.K=1500": 39.990234375,
    "llama_reward.K=1500": 50.9765625,
    "gpt4t_reward.K=1500": -2.783203125,
    "haiku_reward.Creative Tasks.K=1500": 38.772455089820355,
    "llama_reward.Creative Tasks.K=1500": 39.09090909090909,
    "gpt4t_reward.Creative Tasks.K=1500": -12.134502923976607,
    "mixture_of_rewards.Creative Tasks.K=1500": 21.909620418917612,
    "haiku_reward.Planning & Reasoning.K=1500": 45.38714991762768,
    "llama_reward.Planning & Reasoning.K=1500": 60.76158940397352,
    "gpt4t_reward.Planning & Reasoning.K=1500": -1.3957307060755337,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 34.917669538508555,
    "haiku_reward.Math & Data Analysis.K=1500": 50.208333333333336,
    "llama_reward.Math & Data Analysis.K=1500": 77.61506276150628,
    "gpt4t_reward.Math & Data Analysis.K=1500": -1.050420168067227,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 42.25765864225746,
    "haiku_reward.Information/Advice seeking.K=1500": 41.12271540469974,
    "llama_reward.Information/Advice seeking.K=1500": 52.74151436031331,
    "gpt4t_reward.Information/Advice seeking.K=1500": 3.6458333333333335,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 32.50335436611546,
    "haiku_reward.Coding & Debugging.K=1500": 55.05952380952381,
    "llama_reward.Coding & Debugging.K=1500": 79.94186046511628,
    "gpt4t_reward.Coding & Debugging.K=1500": 7.352941176470589,
    "mixture_of_rewards.Coding & Debugging.K=1500": 47.45144181703689,
    "haiku_reward.task_macro.K=1500": 47.577882019096364,
    "llama_reward.task_macro.K=1500": 65.80967692917012,
    "gpt4t_reward.task_macro.K=1500": 0.6448876753554292,
    "mixture_of_rewards.K=1500": 29.39453125,
    "task_macro_reward.K=1500": 38.01081554120731,
    "WB_score.Creative Tasks": 55.60723514211887,
    "WB_score.Planning & Reasoning": 55.635276532137524,
    "WB_score.Math & Data Analysis": 50.15873015873016,
    "WB_score.Information/Advice seeking": 55.54455445544555,
    "WB_score.Coding & Debugging": 56.509433962264154,
    "WB_score": 54.53125,
    "WB_score.task_macro": 54.69508456618439,
    "Length": 2911.845703125,
    "Rank_ScoreMacro": 7,
    "Rank_TaskMacroReward.K": 2,
    "Rank_Avg": 4.5,
    "RewardScore_Avg": 46.35295005369585,
    "WB_Elo": 1226.3132607594066
  },
  "deepseek-coder-v2": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=1500": 18.9453125,
    "llama_reward.K=1500": 37.158203125,
    "gpt4t_reward.K=1500": -25.708699902248288,
    "haiku_reward.Creative Tasks.K=1500": 26.988636363636363,
    "llama_reward.Creative Tasks.K=1500": 33.14285714285714,
    "gpt4t_reward.Creative Tasks.K=1500": -19.35483870967742,
    "mixture_of_rewards.Creative Tasks.K=1500": 13.592218265605359,
    "haiku_reward.Planning & Reasoning.K=1500": 19.42215088282504,
    "llama_reward.Planning & Reasoning.K=1500": 41.92245557350566,
    "gpt4t_reward.Planning & Reasoning.K=1500": -27.567140600315952,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 11.259155285338247,
    "haiku_reward.Math & Data Analysis.K=1500": 19.34156378600823,
    "llama_reward.Math & Data Analysis.K=1500": 55.3941908713693,
    "gpt4t_reward.Math & Data Analysis.K=1500": -34.583333333333336,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 13.384140441348064,
    "haiku_reward.Information/Advice seeking.K=1500": 14.637305699481864,
    "llama_reward.Information/Advice seeking.K=1500": 29.533678756476682,
    "gpt4t_reward.Information/Advice seeking.K=1500": -22.762148337595907,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 7.13627870612088,
    "haiku_reward.Coding & Debugging.K=1500": 15.675675675675677,
    "llama_reward.Coding & Debugging.K=1500": 59.13978494623656,
    "gpt4t_reward.Coding & Debugging.K=1500": -36.64921465968586,
    "mixture_of_rewards.Coding & Debugging.K=1500": 12.722081987408792,
    "haiku_reward.task_macro.K=1500": 18.460240934855516,
    "llama_reward.task_macro.K=1500": 46.40919561259644,
    "gpt4t_reward.task_macro.K=1500": -29.81111747713079,
    "mixture_of_rewards.K=1500": 10.131605240917237,
    "task_macro_reward.K=1500": 11.68610635677372,
    "WB_score.Creative Tasks": 54.49350649350649,
    "WB_score.Planning & Reasoning": 49.24698795180723,
    "WB_score.Math & Data Analysis": 41.59362549800797,
    "WB_score.Information/Advice seeking": 51.54228855721392,
    "WB_score.Coding & Debugging": 44.85714285714286,
    "WB_score": 48.895405669599214,
    "WB_score.task_macro": 47.39521235239142,
    "Length": 2795.3091265947005,
    "Rank_ScoreMacro": 20,
    "Rank_TaskMacroReward.K": 16,
    "Rank_Avg": 18.0,
    "RewardScore_Avg": 29.54065935458257,
    "WB_Elo": 1183.530275223054
  },
  "gemma-2-9b-it": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1155,
    "Arena Elo (hard-en) - latest": 1155,
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 51.007751937984494,
    "WB_score.Planning & Reasoning": 46.65667166416792,
    "WB_score.Math & Data Analysis": 36.42857142857142,
    "WB_score.Information/Advice seeking": 48.960396039603964,
    "WB_score.Coding & Debugging": 36.66666666666666,
    "WB_score": 45.36203522504893,
    "WB_score.task_macro": 42.696193124381026,
    "Length": 2802.8923679060667,
    "Rank_ScoreMacro": 27,
    "Rank_TaskMacroReward.K": 25,
    "Rank_Avg": 26.0,
    "RewardScore_Avg": 21.348096562190513,
    "WB_Elo": 1159.347071190945
  },
  "deepseek-v2-chat-0628": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1233,
    "Arena Elo (hard-en) - latest": 1233,
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 56.43410852713178,
    "WB_score.Planning & Reasoning": 54.82810164424514,
    "WB_score.Math & Data Analysis": 51.42857142857142,
    "WB_score.Information/Advice seeking": 52.72277227722773,
    "WB_score.Coding & Debugging": 55.0,
    "WB_score": 53.80859375,
    "WB_score.task_macro": 53.994280411655694,
    "Length": 3252.376953125,
    "Rank_ScoreMacro": 8,
    "Rank_TaskMacroReward.K": 26,
    "Rank_Avg": 17.0,
    "RewardScore_Avg": 26.997140205827847,
    "WB_Elo": 1209.9997081626336
  },
  "deepseek-v2-coder-0628": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1204,
    "Arena Elo (hard-en) - latest": 1204,
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 40.775193798449614,
    "WB_score.Planning & Reasoning": 47.17488789237669,
    "WB_score.Math & Data Analysis": 46.42857142857142,
    "WB_score.Information/Advice seeking": 40.04950495049505,
    "WB_score.Coding & Debugging": 48.86792452830189,
    "WB_score": 43.4375,
    "WB_score.task_macro": 45.66459211926647,
    "Length": 2580.181640625,
    "Rank_ScoreMacro": 22,
    "Rank_TaskMacroReward.K": 27,
    "Rank_Avg": 24.5,
    "RewardScore_Avg": 22.832296059633236,
    "WB_Elo": 1194.3052888237632
  },
  "Athene-70B": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 60.36175710594314,
    "WB_score.Planning & Reasoning": 60.95952023988005,
    "WB_score.Math & Data Analysis": 57.13147410358566,
    "WB_score.Information/Advice seeking": 60.79207920792079,
    "WB_score.Coding & Debugging": 58.95734597156398,
    "WB_score": 59.41291585127202,
    "WB_score.task_macro": 59.53736733195851,
    "Length": 3175.1438356164385,
    "Rank_ScoreMacro": 1,
    "Rank_TaskMacroReward.K": 28,
    "Rank_Avg": 14.5,
    "RewardScore_Avg": 29.768683665979253,
    "WB_Elo": 1204.5925588524296
  },
  "gpt-4o-mini-2024-07-18": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 60.051679586563296,
    "WB_score.Planning & Reasoning": 58.23617339312406,
    "WB_score.Math & Data Analysis": 54.04761904761905,
    "WB_score.Information/Advice seeking": 57.42574257425743,
    "WB_score.Coding & Debugging": 57.16981132075471,
    "WB_score": 57.265625,
    "WB_score.task_macro": 57.13689403451416,
    "Length": 3648.126953125,
    "Rank_ScoreMacro": 3,
    "Rank_TaskMacroReward.K": 29,
    "Rank_Avg": 16.0,
    "RewardScore_Avg": 28.56844701725708,
    "WB_Elo": 1191.6787570765769
  },
  "Mistral-Large-2": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 58.860103626943,
    "WB_score.Planning & Reasoning": 57.21556886227545,
    "WB_score.Math & Data Analysis": 52.66932270916335,
    "WB_score.Information/Advice seeking": 57.37623762376238,
    "WB_score.Coding & Debugging": 53.83886255924171,
    "WB_score": 55.80078125,
    "WB_score.task_macro": 55.56833516154802,
    "Length": 3503.6262230919765,
    "Rank_ScoreMacro": 4,
    "Rank_TaskMacroReward.K": 30,
    "Rank_Avg": 17.0,
    "RewardScore_Avg": 27.78416758077401,
    "WB_Elo": 1197.7105795590762
  },
  "gemma-2-9b-it-DPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 59.067357512953365,
    "WB_score.Planning & Reasoning": 55.47226386806596,
    "WB_score.Math & Data Analysis": 47.12,
    "WB_score.Information/Advice seeking": 58.21782178217822,
    "WB_score.Coding & Debugging": 50.52132701421801,
    "WB_score": 54.2578125,
    "WB_score.task_macro": 53.22295446230848,
    "Length": 3982.628795298727,
    "Rank_ScoreMacro": 10,
    "Rank_TaskMacroReward.K": 31,
    "Rank_Avg": 20.5,
    "RewardScore_Avg": 26.61147723115424,
    "WB_Elo": 1181.0736048785952
  },
  "gemma-2-9b-it-SimPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 57.97927461139896,
    "WB_score.Planning & Reasoning": 55.645645645645644,
    "WB_score.Math & Data Analysis": 48.59437751004016,
    "WB_score.Information/Advice seeking": 56.485148514851474,
    "WB_score.Coding & Debugging": 50.857142857142854,
    "WB_score": 54.07624633431085,
    "WB_score.task_macro": 53.27923406955029,
    "Length": 4277.667647058824,
    "Rank_ScoreMacro": 9,
    "Rank_TaskMacroReward.K": 32,
    "Rank_Avg": 20.5,
    "RewardScore_Avg": 26.639617034775146,
    "WB_Elo": 1180.4986055478087
  },
  "deepseekv2-chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 20.41015625,
    "llama_reward.K=1500": 38.671875,
    "gpt4t_reward.K=1500": -22.75390625,
    "haiku_reward.Creative Tasks.K=1500": 28.062678062678064,
    "llama_reward.Creative Tasks.K=1500": 34.339080459770116,
    "gpt4t_reward.Creative Tasks.K=1500": -17.7027027027027,
    "mixture_of_rewards.Creative Tasks.K=1500": 14.899685273248492,
    "haiku_reward.Planning & Reasoning.K=1500": 22.150882825040128,
    "llama_reward.Planning & Reasoning.K=1500": 44.605475040257645,
    "gpt4t_reward.Planning & Reasoning.K=1500": -24.68553459119497,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 14.023607758034268,
    "haiku_reward.Math & Data Analysis.K=1500": 21.810699588477366,
    "llama_reward.Math & Data Analysis.K=1500": 55.625,
    "gpt4t_reward.Math & Data Analysis.K=1500": -28.63070539419087,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 16.268331398095498,
    "haiku_reward.Information/Advice seeking.K=1500": 17.829457364341085,
    "llama_reward.Information/Advice seeking.K=1500": 31.3953488372093,
    "gpt4t_reward.Information/Advice seeking.K=1500": -20.918367346938776,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 9.43547961820387,
    "haiku_reward.Coding & Debugging.K=1500": 12.903225806451612,
    "llama_reward.Coding & Debugging.K=1500": 54.54545454545454,
    "gpt4t_reward.Coding & Debugging.K=1500": -36.38743455497382,
    "mixture_of_rewards.Coding & Debugging.K=1500": 10.353748598977441,
    "haiku_reward.task_macro.K=1500": 19.58563524893408,
    "llama_reward.task_macro.K=1500": 46.37467659788047,
    "gpt4t_reward.task_macro.K=1500": -27.265693408096276,
    "mixture_of_rewards.K=1500": 12.109375,
    "task_macro_reward.K=1500": 12.898206146239424,
    "WB_score.Creative Tasks": 53.59173126614987,
    "WB_score.Planning & Reasoning": 50.62874251497006,
    "WB_score.Math & Data Analysis": 44.523809523809526,
    "WB_score.Information/Advice seeking": 51.811414392059554,
    "WB_score.Coding & Debugging": 44.43396226415095,
    "WB_score": 50.04887585532748,
    "WB_score.task_macro": 48.21191935259587,
    "Length": 2896.965786901271,
    "Rank_ScoreMacro": 17,
    "Rank_TaskMacroReward.K": 15,
    "Rank_Avg": 16.0,
    "RewardScore_Avg": 30.555062749417647,
    "WB_Elo": 1184.0251150954687
  },
  "gemma-2-27b-it@together": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 53.626943005181346,
    "WB_score.Planning & Reasoning": 50.55472263868065,
    "WB_score.Math & Data Analysis": 43.919999999999995,
    "WB_score.Information/Advice seeking": 50.49504950495049,
    "WB_score.Coding & Debugging": 47.01421800947868,
    "WB_score": 49.39453125,
    "WB_score.task_macro": 48.54019672452688,
    "Length": 2924.5455435847207,
    "Rank_ScoreMacro": 16,
    "Rank_TaskMacroReward.K": 33,
    "Rank_Avg": 24.5,
    "RewardScore_Avg": 24.27009836226344,
    "WB_Elo": 1182.173597976001
  },
  "Mistral-Nemo-Instruct-2407": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 54.573643410852725,
    "WB_score.Planning & Reasoning": 47.41405082212257,
    "WB_score.Math & Data Analysis": 35.63492063492063,
    "WB_score.Information/Advice seeking": 51.93069306930694,
    "WB_score.Coding & Debugging": 39.71563981042655,
    "WB_score": 46.86217008797654,
    "WB_score.task_macro": 44.37513167010813,
    "Length": 3318.2130987292276,
    "Rank_ScoreMacro": 26,
    "Rank_TaskMacroReward.K": 34,
    "Rank_Avg": 30.0,
    "RewardScore_Avg": 22.187565835054066,
    "WB_Elo": 1166.8049601578437
  },
  "Llama-3-8B-Magpie-Align-v0.1": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 25.146484375,
    "llama_reward.K=1500": 45.849609375,
    "gpt4t_reward.K=1500": -15.869140625,
    "haiku_reward.Creative Tasks.K=1500": 40.17094017094017,
    "llama_reward.Creative Tasks.K=1500": 46.13180515759313,
    "gpt4t_reward.Creative Tasks.K=1500": -5.9620596205962055,
    "mixture_of_rewards.Creative Tasks.K=1500": 26.78022856931236,
    "haiku_reward.Planning & Reasoning.K=1500": 22.416534181240063,
    "llama_reward.Planning & Reasoning.K=1500": 47.52,
    "gpt4t_reward.Planning & Reasoning.K=1500": -18.613707165109034,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 17.10760900537701,
    "haiku_reward.Math & Data Analysis.K=1500": 15.447154471544716,
    "llama_reward.Math & Data Analysis.K=1500": 52.244897959183675,
    "gpt4t_reward.Math & Data Analysis.K=1500": -35.10204081632653,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 10.863337204800622,
    "haiku_reward.Information/Advice seeking.K=1500": 33.67609254498715,
    "llama_reward.Information/Advice seeking.K=1500": 50.51413881748073,
    "gpt4t_reward.Information/Advice seeking.K=1500": -3.435114503816794,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 26.918372286217025,
    "haiku_reward.Coding & Debugging.K=1500": 11.141304347826086,
    "llama_reward.Coding & Debugging.K=1500": 58.96739130434783,
    "gpt4t_reward.Coding & Debugging.K=1500": -30.628272251308903,
    "mixture_of_rewards.Coding & Debugging.K=1500": 13.16014113362167,
    "haiku_reward.task_macro.K=1500": 21.628840447807967,
    "llama_reward.task_macro.K=1500": 51.853819743479065,
    "gpt4t_reward.task_macro.K=1500": -21.518290689475712,
    "mixture_of_rewards.K=1500": 18.375651041666668,
    "task_macro_reward.K=1500": 17.321456500603777,
    "WB_score.Creative Tasks": 49.19896640826874,
    "WB_score.Planning & Reasoning": 42.7245508982036,
    "WB_score.Math & Data Analysis": 29.76000000000001,
    "WB_score.Information/Advice seeking": 48.910891089108915,
    "WB_score.Coding & Debugging": 33.74407582938389,
    "WB_score": 42.44618395303327,
    "WB_score.task_macro": 39.290196827463255,
    "Length": 3107.77397260274,
    "Rank_ScoreMacro": 29,
    "Rank_TaskMacroReward.K": 11,
    "Rank_Avg": 20.0,
    "RewardScore_Avg": 28.305826664033518,
    "WB_Elo": 1151.402258570742
  },
  "Llama-3-Instruct-8B-SimPO-v0.2": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 51.83462532299741,
    "WB_score.Planning & Reasoning": 40.71856287425149,
    "WB_score.Math & Data Analysis": 24.38247011952191,
    "WB_score.Information/Advice seeking": 47.871287128712865,
    "WB_score.Coding & Debugging": 31.50943396226415,
    "WB_score": 41.50537634408602,
    "WB_score.task_macro": 37.1554198259368,
    "Length": 2533.764418377322,
    "Rank_ScoreMacro": 34,
    "Rank_TaskMacroReward.K": 35,
    "Rank_Avg": 34.5,
    "RewardScore_Avg": 18.5777099129684,
    "WB_Elo": 1148.0879185900626
  },
  "glm-4-9b-chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 5.180840664711632,
    "llama_reward.K=1500": 27.174975562072333,
    "gpt4t_reward.K=1500": -30.528375733855185,
    "haiku_reward.Creative Tasks.K=1500": 16.516516516516518,
    "llama_reward.Creative Tasks.K=1500": 20.820668693009118,
    "gpt4t_reward.Creative Tasks.K=1500": -26.676384839650147,
    "mixture_of_rewards.Creative Tasks.K=1500": 3.5536001232918295,
    "haiku_reward.Planning & Reasoning.K=1500": 5.387205387205387,
    "llama_reward.Planning & Reasoning.K=1500": 35.714285714285715,
    "gpt4t_reward.Planning & Reasoning.K=1500": -34.583333333333336,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 2.172719256052588,
    "haiku_reward.Math & Data Analysis.K=1500": -4.8034934497816595,
    "llama_reward.Math & Data Analysis.K=1500": 37.93859649122807,
    "gpt4t_reward.Math & Data Analysis.K=1500": -48.6784140969163,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -5.181103685156631,
    "haiku_reward.Information/Advice seeking.K=1500": 5.614973262032086,
    "llama_reward.Information/Advice seeking.K=1500": 27.016129032258064,
    "gpt4t_reward.Information/Advice seeking.K=1500": -22.872340425531913,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 3.252920622919413,
    "haiku_reward.Coding & Debugging.K=1500": 0.8571428571428572,
    "llama_reward.Coding & Debugging.K=1500": 50.0,
    "gpt4t_reward.Coding & Debugging.K=1500": -50.84269662921348,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.004815409309792074,
    "haiku_reward.task_macro.K=1500": 3.2571431706180984,
    "llama_reward.task_macro.K=1500": 37.00081784311228,
    "gpt4t_reward.task_macro.K=1500": -39.14808406085765,
    "mixture_of_rewards.K=1500": 0.6091468309762599,
    "task_macro_reward.K=1500": 0.36995898429091056,
    "WB_score.Creative Tasks": 47.751937984496124,
    "WB_score.Planning & Reasoning": 42.48502994011975,
    "WB_score.Math & Data Analysis": 29.800796812748995,
    "WB_score.Information/Advice seeking": 46.28712871287128,
    "WB_score.Coding & Debugging": 35.37735849056604,
    "WB_score": 41.17302052785924,
    "WB_score.task_macro": 39.09896797431742,
    "Length": 3692.043010752688,
    "Rank_ScoreMacro": 30,
    "Rank_TaskMacroReward.K": 23,
    "Rank_Avg": 26.5,
    "RewardScore_Avg": 19.734463479304164,
    "WB_Elo": 1146.685826739111
  },
  "SELM-Llama-3-8B-Instruct-iter-3": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 11.279296875,
    "llama_reward.K=1500": 33.7890625,
    "gpt4t_reward.K=1500": -33.28445747800586,
    "haiku_reward.Creative Tasks.K=1500": 27.73109243697479,
    "llama_reward.Creative Tasks.K=1500": 36.93820224719101,
    "gpt4t_reward.Creative Tasks.K=1500": -23.076923076923077,
    "mixture_of_rewards.Creative Tasks.K=1500": 13.864123869080908,
    "haiku_reward.Planning & Reasoning.K=1500": 9.807073954983924,
    "llama_reward.Planning & Reasoning.K=1500": 36.655948553054664,
    "gpt4t_reward.Planning & Reasoning.K=1500": -36.018957345971565,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 3.4813550540223424,
    "haiku_reward.Math & Data Analysis.K=1500": -4.30327868852459,
    "llama_reward.Math & Data Analysis.K=1500": 38.88888888888889,
    "gpt4t_reward.Math & Data Analysis.K=1500": -50.20661157024794,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -5.2070004566278785,
    "haiku_reward.Information/Advice seeking.K=1500": 15.850515463917525,
    "llama_reward.Information/Advice seeking.K=1500": 34.79381443298969,
    "gpt4t_reward.Information/Advice seeking.K=1500": -23.97959183673469,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 8.888246020057508,
    "haiku_reward.Coding & Debugging.K=1500": -7.670454545454546,
    "llama_reward.Coding & Debugging.K=1500": 42.737430167597765,
    "gpt4t_reward.Coding & Debugging.K=1500": -61.66666666666667,
    "mixture_of_rewards.Coding & Debugging.K=1500": -8.866563681507818,
    "haiku_reward.task_macro.K=1500": 5.078090187328983,
    "llama_reward.task_macro.K=1500": 38.46212089221941,
    "gpt4t_reward.task_macro.K=1500": -42.491957263591225,
    "mixture_of_rewards.K=1500": 3.927967298998046,
    "task_macro_reward.K=1500": 0.3494179386523892,
    "WB_score.Creative Tasks": 51.05943152454781,
    "WB_score.Planning & Reasoning": 39.78978978978979,
    "WB_score.Math & Data Analysis": 23.505976095617527,
    "WB_score.Information/Advice seeking": 46.05459057071961,
    "WB_score.Coding & Debugging": 27.333333333333325,
    "WB_score": 39.96078431372549,
    "WB_score.task_macro": 35.25906077680738,
    "Length": 2913.1470588235293,
    "Rank_ScoreMacro": 37,
    "Rank_TaskMacroReward.K": 24,
    "Rank_Avg": 30.5,
    "RewardScore_Avg": 17.804239357729884,
    "WB_Elo": 1141.2979653779491
  },
  "Yi-1.5-9B-Chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 4.647749510763209,
    "llama_reward.K=1500": 26.099706744868033,
    "gpt4t_reward.K=1500": -30.37109375,
    "haiku_reward.Creative Tasks.K=1500": 9.322033898305085,
    "llama_reward.Creative Tasks.K=1500": 19.65811965811966,
    "gpt4t_reward.Creative Tasks.K=1500": -31.182795698924732,
    "mixture_of_rewards.Creative Tasks.K=1500": -0.7342140474999953,
    "haiku_reward.Planning & Reasoning.K=1500": 8.756137479541735,
    "llama_reward.Planning & Reasoning.K=1500": 35.65573770491803,
    "gpt4t_reward.Planning & Reasoning.K=1500": -28.696343402225754,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 5.238510594078004,
    "haiku_reward.Math & Data Analysis.K=1500": 8.547008547008547,
    "llama_reward.Math & Data Analysis.K=1500": 46.38297872340426,
    "gpt4t_reward.Math & Data Analysis.K=1500": -34.11016949152542,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 6.939939259629128,
    "haiku_reward.Information/Advice seeking.K=1500": 2.8350515463917527,
    "llama_reward.Information/Advice seeking.K=1500": 23.316062176165804,
    "gpt4t_reward.Information/Advice seeking.K=1500": -25.06361323155216,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.36250016366846555,
    "haiku_reward.Coding & Debugging.K=1500": -5.865921787709497,
    "llama_reward.Coding & Debugging.K=1500": 43.05555555555556,
    "gpt4t_reward.Coding & Debugging.K=1500": -49.73262032085562,
    "mixture_of_rewards.Coding & Debugging.K=1500": -4.180995517669852,
    "haiku_reward.task_macro.K=1500": 3.9888597412095153,
    "llama_reward.task_macro.K=1500": 36.22909510740641,
    "gpt4t_reward.task_macro.K=1500": -35.06010145652708,
    "mixture_of_rewards.K=1500": 0.12545416854374736,
    "task_macro_reward.K=1500": 1.7192844640296154,
    "WB_score.Creative Tasks": 45.5958549222798,
    "WB_score.Planning & Reasoning": 42.37237237237237,
    "WB_score.Math & Data Analysis": 32.20883534136546,
    "WB_score.Information/Advice seeking": 42.62376237623762,
    "WB_score.Coding & Debugging": 34.97630331753555,
    "WB_score": 39.8435972629521,
    "WB_score.task_macro": 38.66535351517231,
    "Length": 3468.23431372549,
    "Rank_ScoreMacro": 33,
    "Rank_TaskMacroReward.K": 22,
    "Rank_Avg": 27.5,
    "RewardScore_Avg": 20.192318989600963,
    "WB_Elo": 1140.0225599844637
  },
  "Llama-3-Instruct-8B-SimPO-ExPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 19.140625,
    "llama_reward.K=1500": 41.015625,
    "gpt4t_reward.K=1500": -22.998046875,
    "haiku_reward.Creative Tasks.K=1500": 34.540389972144844,
    "llama_reward.Creative Tasks.K=1500": 41.17647058823529,
    "gpt4t_reward.Creative Tasks.K=1500": -12.928759894459102,
    "mixture_of_rewards.Creative Tasks.K=1500": 20.92936688864034,
    "haiku_reward.Planning & Reasoning.K=1500": 16.90251572327044,
    "llama_reward.Planning & Reasoning.K=1500": 43.59177215189873,
    "gpt4t_reward.Planning & Reasoning.K=1500": -25.811437403400312,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 11.560950157256286,
    "haiku_reward.Math & Data Analysis.K=1500": -4.048582995951417,
    "llama_reward.Math & Data Analysis.K=1500": 40.447154471544714,
    "gpt4t_reward.Math & Data Analysis.K=1500": -43.08943089430895,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -2.2302864729052154,
    "haiku_reward.Information/Advice seeking.K=1500": 29.028132992327365,
    "llama_reward.Information/Advice seeking.K=1500": 45.39641943734015,
    "gpt4t_reward.Information/Advice seeking.K=1500": -8.564231738035264,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 21.953440230544086,
    "haiku_reward.Coding & Debugging.K=1500": 5.026455026455026,
    "llama_reward.Coding & Debugging.K=1500": 50.0,
    "gpt4t_reward.Coding & Debugging.K=1500": -46.391752577319586,
    "mixture_of_rewards.Coding & Debugging.K=1500": 2.878234149711813,
    "haiku_reward.task_macro.K=1500": 13.137668457162812,
    "llama_reward.task_macro.K=1500": 44.646825164955985,
    "gpt4t_reward.task_macro.K=1500": -30.785467814939967,
    "mixture_of_rewards.K=1500": 12.386067708333334,
    "task_macro_reward.K=1500": 8.99967526905961,
    "WB_score.Creative Tasks": 49.14728682170542,
    "WB_score.Planning & Reasoning": 39.46107784431138,
    "WB_score.Math & Data Analysis": 21.195219123505975,
    "WB_score.Information/Advice seeking": 47.32673267326733,
    "WB_score.Coding & Debugging": 28.584905660377355,
    "WB_score": 39.687194525904204,
    "WB_score.task_macro": 35.01502977266739,
    "Length": 2480.6490713587486,
    "Rank_ScoreMacro": 38,
    "Rank_TaskMacroReward.K": 19,
    "Rank_Avg": 28.5,
    "RewardScore_Avg": 22.0073525208635,
    "WB_Elo": 1144.9796209560836
  },
  "Starling-LM-7B-beta-ExPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 1.5625,
    "llama_reward.K=1500": 23.92578125,
    "gpt4t_reward.K=1500": -42.626953125,
    "haiku_reward.Creative Tasks.K=1500": 17.787114845938376,
    "llama_reward.Creative Tasks.K=1500": 22.6123595505618,
    "gpt4t_reward.Creative Tasks.K=1500": -30.87071240105541,
    "mixture_of_rewards.Creative Tasks.K=1500": 3.176253998481588,
    "haiku_reward.Planning & Reasoning.K=1500": -0.6339144215530903,
    "llama_reward.Planning & Reasoning.K=1500": 26.273885350318473,
    "gpt4t_reward.Planning & Reasoning.K=1500": -45.20123839009288,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -6.520422487109166,
    "haiku_reward.Math & Data Analysis.K=1500": -16.73469387755102,
    "llama_reward.Math & Data Analysis.K=1500": 30.942622950819672,
    "gpt4t_reward.Math & Data Analysis.K=1500": -60.040983606557376,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -15.277684844429574,
    "haiku_reward.Information/Advice seeking.K=1500": 3.974358974358974,
    "llama_reward.Information/Advice seeking.K=1500": 21.794871794871796,
    "gpt4t_reward.Information/Advice seeking.K=1500": -35.984848484848484,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -3.405205905205905,
    "haiku_reward.Coding & Debugging.K=1500": -14.397905759162304,
    "llama_reward.Coding & Debugging.K=1500": 36.12565445026178,
    "gpt4t_reward.Coding & Debugging.K=1500": -62.121212121212125,
    "mixture_of_rewards.Coding & Debugging.K=1500": -13.464487810037552,
    "haiku_reward.task_macro.K=1500": -4.97897735783302,
    "llama_reward.task_macro.K=1500": 28.756711857469412,
    "gpt4t_reward.task_macro.K=1500": -49.81432406523214,
    "mixture_of_rewards.K=1500": -5.712890625,
    "task_macro_reward.K=1500": -8.678863188531917,
    "WB_score.Creative Tasks": 44.30051813471502,
    "WB_score.Planning & Reasoning": 36.31736526946108,
    "WB_score.Math & Data Analysis": 18.571428571428577,
    "WB_score.Information/Advice seeking": 42.871287128712865,
    "WB_score.Coding & Debugging": 25.308056872037916,
    "WB_score": 35.01466275659824,
    "WB_score.task_macro": 31.559353823619887,
    "Length": 2835.826810176125,
    "Rank_ScoreMacro": 40,
    "Rank_TaskMacroReward.K": 39,
    "Rank_Avg": 39.5,
    "RewardScore_Avg": 11.440245317543985,
    "WB_Elo": 1125.6065314200982
  },
  "gemma-2-2b-it": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": 0,
    "llama_reward.K=1500": 0,
    "gpt4t_reward.K=1500": 0,
    "haiku_reward.Creative Tasks.K=1500": 0,
    "llama_reward.Creative Tasks.K=1500": 0,
    "gpt4t_reward.Creative Tasks.K=1500": 0,
    "mixture_of_rewards.Creative Tasks.K=1500": 0.0,
    "haiku_reward.Planning & Reasoning.K=1500": 0,
    "llama_reward.Planning & Reasoning.K=1500": 0,
    "gpt4t_reward.Planning & Reasoning.K=1500": 0,
    "mixture_of_rewards.Planning & Reasoning.K=1500": 0.0,
    "haiku_reward.Math & Data Analysis.K=1500": 0,
    "llama_reward.Math & Data Analysis.K=1500": 0,
    "gpt4t_reward.Math & Data Analysis.K=1500": 0,
    "mixture_of_rewards.Math & Data Analysis.K=1500": 0.0,
    "haiku_reward.Information/Advice seeking.K=1500": 0,
    "llama_reward.Information/Advice seeking.K=1500": 0,
    "gpt4t_reward.Information/Advice seeking.K=1500": 0,
    "mixture_of_rewards.Information/Advice seeking.K=1500": 0.0,
    "haiku_reward.Coding & Debugging.K=1500": 0,
    "llama_reward.Coding & Debugging.K=1500": 0,
    "gpt4t_reward.Coding & Debugging.K=1500": 0,
    "mixture_of_rewards.Coding & Debugging.K=1500": 0.0,
    "haiku_reward.task_macro.K=1500": 0,
    "llama_reward.task_macro.K=1500": 0,
    "gpt4t_reward.task_macro.K=1500": 0,
    "mixture_of_rewards.K=1500": 0.0,
    "task_macro_reward.K=1500": 0.0,
    "WB_score.Creative Tasks": 43.61757105943152,
    "WB_score.Planning & Reasoning": 33.811659192825104,
    "WB_score.Math & Data Analysis": 15.79365079365079,
    "WB_score.Information/Advice seeking": 39.90099009900991,
    "WB_score.Coding & Debugging": 17.904761904761912,
    "WB_score": 32.72015655577299,
    "WB_score.task_macro": 27.826043214654263,
    "Length": 3589.3894324853227,
    "Rank_ScoreMacro": 50,
    "Rank_TaskMacroReward.K": 36,
    "Rank_Avg": 43.0,
    "RewardScore_Avg": 13.913021607327131,
    "WB_Elo": 1114.9711016640829
  },
  "neo_7b_instruct_v0.1": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": -10.25390625,
    "llama_reward.K=1500": 13.18359375,
    "gpt4t_reward.K=1500": -43.24191968658178,
    "haiku_reward.Creative Tasks.K=1500": 7.636887608069164,
    "llama_reward.Creative Tasks.K=1500": 17.151162790697676,
    "gpt4t_reward.Creative Tasks.K=1500": -34.75274725274725,
    "mixture_of_rewards.Creative Tasks.K=1500": -3.32156561799347,
    "haiku_reward.Planning & Reasoning.K=1500": -10.38961038961039,
    "llama_reward.Planning & Reasoning.K=1500": 19.33115823817292,
    "gpt4t_reward.Planning & Reasoning.K=1500": -44.01913875598086,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -11.692530302472775,
    "haiku_reward.Math & Data Analysis.K=1500": -26.778242677824267,
    "llama_reward.Math & Data Analysis.K=1500": 18.410041841004183,
    "gpt4t_reward.Math & Data Analysis.K=1500": -58.26271186440678,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -22.21030423374229,
    "haiku_reward.Information/Advice seeking.K=1500": -7.198952879581152,
    "llama_reward.Information/Advice seeking.K=1500": 11.067708333333332,
    "gpt4t_reward.Information/Advice seeking.K=1500": -36.246786632390744,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -10.792677059546188,
    "haiku_reward.Coding & Debugging.K=1500": -37.784090909090914,
    "llama_reward.Coding & Debugging.K=1500": 6.2857142857142865,
    "gpt4t_reward.Coding & Debugging.K=1500": -70.87912087912088,
    "mixture_of_rewards.Coding & Debugging.K=1500": -34.1258325008325,
    "haiku_reward.task_macro.K=1500": -18.647608454074145,
    "llama_reward.task_macro.K=1500": 14.16999897709727,
    "gpt4t_reward.task_macro.K=1500": -51.88357894925277,
    "mixture_of_rewards.K=1500": -13.437410728860593,
    "task_macro_reward.K=1500": -18.787062808743215,
    "WB_score.Creative Tasks": 39.48186528497409,
    "WB_score.Planning & Reasoning": 31.44992526158445,
    "WB_score.Math & Data Analysis": 15.0,
    "WB_score.Information/Advice seeking": 36.33663366336634,
    "WB_score.Coding & Debugging": 14.02843601895734,
    "WB_score": 29.19921875,
    "WB_score.task_macro": 25.019233576987165,
    "Length": 3735.800586510264,
    "Rank_ScoreMacro": 53,
    "Rank_TaskMacroReward.K": 46,
    "Rank_Avg": 49.5,
    "RewardScore_Avg": 3.116085384121975,
    "WB_Elo": 1104.2844374018302
  },
  "neo_7b_instruct_v0.1-ExPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": -12.6953125,
    "llama_reward.K=1500": 9.090909090909092,
    "gpt4t_reward.K=1500": -42.08211143695015,
    "haiku_reward.Creative Tasks.K=1500": 6.744868035190615,
    "llama_reward.Creative Tasks.K=1500": 17.44868035190616,
    "gpt4t_reward.Creative Tasks.K=1500": -31.60112359550562,
    "mixture_of_rewards.Creative Tasks.K=1500": -2.469191736136281,
    "haiku_reward.Planning & Reasoning.K=1500": -16.39072847682119,
    "llama_reward.Planning & Reasoning.K=1500": 13.02170283806344,
    "gpt4t_reward.Planning & Reasoning.K=1500": -45.36585365853659,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -16.24495976576478,
    "haiku_reward.Math & Data Analysis.K=1500": -32.00836820083682,
    "llama_reward.Math & Data Analysis.K=1500": 9.414225941422593,
    "gpt4t_reward.Math & Data Analysis.K=1500": -62.13389121338913,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -28.242677824267787,
    "haiku_reward.Information/Advice seeking.K=1500": -7.887700534759358,
    "llama_reward.Information/Advice seeking.K=1500": 11.76470588235294,
    "gpt4t_reward.Information/Advice seeking.K=1500": -35.978835978835974,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -10.700610210414132,
    "haiku_reward.Coding & Debugging.K=1500": -49.112426035502956,
    "llama_reward.Coding & Debugging.K=1500": -2.631578947368421,
    "gpt4t_reward.Coding & Debugging.K=1500": -72.72727272727273,
    "mixture_of_rewards.Coding & Debugging.K=1500": -41.49042590338137,
    "haiku_reward.task_macro.K=1500": -24.511716363098152,
    "llama_reward.task_macro.K=1500": 8.410473766844033,
    "gpt4t_reward.task_macro.K=1500": -53.16526062684854,
    "mixture_of_rewards.K=1500": -15.228838282013685,
    "task_macro_reward.K=1500": -23.088834407700887,
    "WB_score.Creative Tasks": 38.549222797927456,
    "WB_score.Planning & Reasoning": 28.669656203288483,
    "WB_score.Math & Data Analysis": 12.589641434262955,
    "WB_score.Information/Advice seeking": 34.85148514851485,
    "WB_score.Coding & Debugging": 12.76190476190477,
    "WB_score": 27.624633431085037,
    "WB_score.task_macro": 23.114172189706185,
    "Length": 4107.917808219178,
    "Rank_ScoreMacro": 56,
    "Rank_TaskMacroReward.K": 50,
    "Rank_Avg": 53.0,
    "RewardScore_Avg": 0.012668891002649119,
    "WB_Elo": 1104.5166529743904
  },
  "Yi-1.5-6B-Chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": -17.546432062561095,
    "llama_reward.K=1500": 3.759765625,
    "gpt4t_reward.K=1500": -48.92578125,
    "haiku_reward.Creative Tasks.K=1500": -14.6875,
    "llama_reward.Creative Tasks.K=1500": -6.5625,
    "gpt4t_reward.Creative Tasks.K=1500": -51.041666666666664,
    "mixture_of_rewards.Creative Tasks.K=1500": -24.097222222222218,
    "haiku_reward.Planning & Reasoning.K=1500": -20.153061224489797,
    "llama_reward.Planning & Reasoning.K=1500": 10.0,
    "gpt4t_reward.Planning & Reasoning.K=1500": -53.76254180602007,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -21.305201010169956,
    "haiku_reward.Math & Data Analysis.K=1500": -20.085470085470085,
    "llama_reward.Math & Data Analysis.K=1500": 24.261603375527425,
    "gpt4t_reward.Math & Data Analysis.K=1500": -58.54700854700855,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -18.123625085650403,
    "haiku_reward.Information/Advice seeking.K=1500": -16.34078212290503,
    "llama_reward.Information/Advice seeking.K=1500": 1.2605042016806722,
    "gpt4t_reward.Information/Advice seeking.K=1500": -46.54696132596685,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -20.54241308239707,
    "haiku_reward.Coding & Debugging.K=1500": -40.78212290502793,
    "llama_reward.Coding & Debugging.K=1500": 6.353591160220995,
    "gpt4t_reward.Coding & Debugging.K=1500": -73.78378378378379,
    "mixture_of_rewards.Coding & Debugging.K=1500": -36.07077184286357,
    "haiku_reward.task_macro.K=1500": -24.390270913588637,
    "llama_reward.task_macro.K=1500": 8.919520100434564,
    "gpt4t_reward.task_macro.K=1500": -58.61283576043618,
    "mixture_of_rewards.K=1500": -20.90414922918703,
    "task_macro_reward.K=1500": -24.694528857863418,
    "WB_score.Creative Tasks": 31.088082901554408,
    "WB_score.Planning & Reasoning": 27.2972972972973,
    "WB_score.Math & Data Analysis": 16.799999999999997,
    "WB_score.Information/Advice seeking": 31.414392059553347,
    "WB_score.Coding & Debugging": 16.587677725118475,
    "WB_score": 25.278592375366564,
    "WB_score.task_macro": 23.318116689149882,
    "Length": 3899.4686274509804,
    "Rank_ScoreMacro": 55,
    "Rank_TaskMacroReward.K": 52,
    "Rank_Avg": 53.5,
    "RewardScore_Avg": -0.6882060843567679,
    "WB_Elo": 1099.4947697983446
  },
  "reka-edge": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=1500": -17.236328125,
    "llama_reward.K=1500": 4.296875,
    "gpt4t_reward.K=1500": -52.978515625,
    "haiku_reward.Creative Tasks.K=1500": -3.7572254335260116,
    "llama_reward.Creative Tasks.K=1500": 2.161383285302594,
    "gpt4t_reward.Creative Tasks.K=1500": -47.41847826086957,
    "mixture_of_rewards.Creative Tasks.K=1500": -16.338106803030996,
    "haiku_reward.Planning & Reasoning.K=1500": -24.4281045751634,
    "llama_reward.Planning & Reasoning.K=1500": 3.9215686274509802,
    "gpt4t_reward.Planning & Reasoning.K=1500": -59.519999999999996,
    "mixture_of_rewards.Planning & Reasoning.K=1500": -26.675511982570807,
    "haiku_reward.Math & Data Analysis.K=1500": -39.58333333333333,
    "llama_reward.Math & Data Analysis.K=1500": 12.552301255230125,
    "gpt4t_reward.Math & Data Analysis.K=1500": -69.9579831932773,
    "mixture_of_rewards.Math & Data Analysis.K=1500": -32.32967175712684,
    "haiku_reward.Information/Advice seeking.K=1500": -14.745308310991955,
    "llama_reward.Information/Advice seeking.K=1500": -1.6042780748663104,
    "gpt4t_reward.Information/Advice seeking.K=1500": -48.01061007957559,
    "mixture_of_rewards.Information/Advice seeking.K=1500": -21.453398821811287,
    "haiku_reward.Coding & Debugging.K=1500": -34.12698412698413,
    "llama_reward.Coding & Debugging.K=1500": 15.263157894736842,
    "gpt4t_reward.Coding & Debugging.K=1500": -68.62244897959184,
    "mixture_of_rewards.Coding & Debugging.K=1500": -29.162091737279706,
    "haiku_reward.task_macro.K=1500": -26.466271191794675,
    "llama_reward.task_macro.K=1500": 7.665408946203467,
    "gpt4t_reward.task_macro.K=1500": -61.02173497008074,
    "mixture_of_rewards.K=1500": -21.97265625,
    "task_macro_reward.K=1500": -26.60753240522398,
    "WB_score.Creative Tasks": 36.180371352785144,
    "WB_score.Planning & Reasoning": 25.007727975270484,
    "WB_score.Math & Data Analysis": 8.89795918367346,
    "WB_score.Information/Advice seeking": 34.3896103896104,
    "WB_score.Coding & Debugging": 13.526570048309186,
    "WB_score": 23.186705767350926,
    "WB_score.task_macro": 21.252257932999665,
    "Length": 2417.351106639839,
    "Rank_ScoreMacro": 57,
    "Rank_TaskMacroReward.K": 54,
    "Rank_Avg": 55.5,
    "RewardScore_Avg": -2.677637236112158,
    "WB_Elo": 1098.4979869242243
  }
}