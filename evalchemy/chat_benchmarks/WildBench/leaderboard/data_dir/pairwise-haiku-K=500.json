{
  "claude-3-5-sonnet-20240620": {
    "model": "claude-3-5-sonnet-20240620",
    "win_much": 314,
    "win": 267,
    "tie": 213,
    "lose": 93,
    "lose_much": 37,
    "total": 1024,
    "avg_len": 2405.253246753247,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 105,
        "win": 114,
        "tie": 114,
        "lose": 38,
        "lose_much": 12
      },
      "Planning & Reasoning": {
        "win_much": 193,
        "win": 182,
        "tie": 157,
        "lose": 56,
        "lose_much": 19
      },
      "Coding & Debugging": {
        "win_much": 77,
        "win": 44,
        "tie": 25,
        "lose": 16,
        "lose_much": 6
      },
      "Math & Data Analysis": {
        "win_much": 97,
        "win": 67,
        "tie": 45,
        "lose": 22,
        "lose_much": 9
      },
      "Creative Tasks": {
        "win_much": 101,
        "win": 98,
        "tie": 82,
        "lose": 37,
        "lose_much": 16
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.34203655352480417,
      "Planning & Reasoning": 0.3904448105436573,
      "Coding & Debugging": 0.5059523809523809,
      "Math & Data Analysis": 0.46041666666666664,
      "Creative Tasks": 0.3458083832335329
    },
    "reward": 0.35546875,
    "task_macro_reward": 0.4232304763783335,
    "K": 500
  },
  "gpt-4-turbo-2024-04-09": {
    "model": "gpt-4-turbo-2024-04-09",
    "win_much": 336,
    "win": 158,
    "tie": 321,
    "lose": 112,
    "lose_much": 32,
    "total": 1024,
    "avg_len": 2956.7309697601668,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 103,
        "win": 69,
        "tie": 162,
        "lose": 42,
        "lose_much": 14
      },
      "Planning & Reasoning": {
        "win_much": 205,
        "win": 108,
        "tie": 229,
        "lose": 67,
        "lose_much": 17
      },
      "Coding & Debugging": {
        "win_much": 80,
        "win": 20,
        "tie": 48,
        "lose": 29,
        "lose_much": 7
      },
      "Math & Data Analysis": {
        "win_much": 98,
        "win": 27,
        "tie": 71,
        "lose": 35,
        "lose_much": 10
      },
      "Creative Tasks": {
        "win_much": 116,
        "win": 79,
        "tie": 115,
        "lose": 42,
        "lose_much": 4
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.26282051282051283,
      "Planning & Reasoning": 0.3330670926517572,
      "Coding & Debugging": 0.37228260869565216,
      "Math & Data Analysis": 0.34854771784232363,
      "Creative Tasks": 0.36657303370786515
    },
    "reward": 0.3193359375,
    "task_macro_reward": 0.3390814202096637,
    "K": 500
  },
  "gpt-4o-2024-05-13": {
    "model": "gpt-4o-2024-05-13",
    "win_much": 342,
    "win": 108,
    "tie": 319,
    "lose": 100,
    "lose_much": 34,
    "total": 1024,
    "avg_len": 3211.889258028793,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 78,
        "win": 23,
        "tie": 47,
        "lose": 19,
        "lose_much": 6
      },
      "Creative Tasks": {
        "win_much": 105,
        "win": 52,
        "tie": 124,
        "lose": 35,
        "lose_much": 11
      },
      "Information/Advice seeking": {
        "win_much": 117,
        "win": 39,
        "tie": 160,
        "lose": 49,
        "lose_much": 8
      },
      "Planning & Reasoning": {
        "win_much": 225,
        "win": 64,
        "tie": 226,
        "lose": 50,
        "lose_much": 18
      },
      "Math & Data Analysis": {
        "win_much": 117,
        "win": 20,
        "tie": 63,
        "lose": 21,
        "lose_much": 11
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.4277456647398844,
      "Creative Tasks": 0.31345565749235477,
      "Information/Advice seeking": 0.27882037533512066,
      "Planning & Reasoning": 0.3670668953687822,
      "Math & Data Analysis": 0.4547413793103448
    },
    "reward": 0.3046875,
    "task_macro_reward": 0.38191582940919916,
    "K": 500
  },
  "gemini-1.5-pro": {
    "model": "gemini-1.5-pro",
    "win_much": 297,
    "win": 183,
    "tie": 272,
    "lose": 91,
    "lose_much": 40,
    "total": 1024,
    "avg_len": 2843.5617214043036,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 103,
        "win": 82,
        "tie": 129,
        "lose": 40,
        "lose_much": 16
      },
      "Coding & Debugging": {
        "win_much": 76,
        "win": 21,
        "tie": 44,
        "lose": 10,
        "lose_much": 6
      },
      "Planning & Reasoning": {
        "win_much": 184,
        "win": 129,
        "tie": 192,
        "lose": 54,
        "lose_much": 25
      },
      "Math & Data Analysis": {
        "win_much": 90,
        "win": 37,
        "tie": 68,
        "lose": 28,
        "lose_much": 12
      },
      "Creative Tasks": {
        "win_much": 102,
        "win": 88,
        "tie": 89,
        "lose": 33,
        "lose_much": 13
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.2918918918918919,
      "Coding & Debugging": 0.48089171974522293,
      "Planning & Reasoning": 0.336472602739726,
      "Math & Data Analysis": 0.35106382978723405,
      "Creative Tasks": 0.35846153846153844
    },
    "reward": 0.2958984375,
    "task_macro_reward": 0.37282503600907546,
    "K": 500
  },
  "yi-large-preview": {
    "model": "yi-large-preview",
    "win_much": 326,
    "win": 111,
    "tie": 357,
    "lose": 98,
    "lose_much": 36,
    "total": 1024,
    "avg_len": 3244.700431034483,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 101,
        "win": 48,
        "tie": 177,
        "lose": 43,
        "lose_much": 13
      },
      "Planning & Reasoning": {
        "win_much": 209,
        "win": 74,
        "tie": 249,
        "lose": 60,
        "lose_much": 18
      },
      "Coding & Debugging": {
        "win_much": 75,
        "win": 20,
        "tie": 51,
        "lose": 22,
        "lose_much": 8
      },
      "Math & Data Analysis": {
        "win_much": 100,
        "win": 18,
        "tie": 94,
        "lose": 21,
        "lose_much": 7
      },
      "Creative Tasks": {
        "win_much": 121,
        "win": 51,
        "tie": 121,
        "lose": 34,
        "lose_much": 14
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.2369109947643979,
      "Planning & Reasoning": 0.32459016393442625,
      "Coding & Debugging": 0.375,
      "Math & Data Analysis": 0.38125,
      "Creative Tasks": 0.3387096774193548
    },
    "reward": 0.28955078125,
    "task_macro_reward": 0.3374264820423177,
    "K": 500
  },
  "claude-3-opus-20240229": {
    "model": "claude-3-opus-20240229",
    "win_much": 225,
    "win": 320,
    "tie": 240,
    "lose": 132,
    "lose_much": 33,
    "total": 1024,
    "avg_len": 2401.081052631579,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 68,
        "win": 38,
        "tie": 46,
        "lose": 23,
        "lose_much": 7
      },
      "Creative Tasks": {
        "win_much": 64,
        "win": 135,
        "tie": 89,
        "lose": 51,
        "lose_much": 9
      },
      "Information/Advice seeking": {
        "win_much": 69,
        "win": 140,
        "tie": 107,
        "lose": 57,
        "lose_much": 14
      },
      "Planning & Reasoning": {
        "win_much": 139,
        "win": 206,
        "tie": 167,
        "lose": 86,
        "lose_much": 23
      },
      "Math & Data Analysis": {
        "win_much": 81,
        "win": 68,
        "tie": 54,
        "lose": 30,
        "lose_much": 13
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.37637362637362637,
      "Creative Tasks": 0.27873563218390807,
      "Information/Advice seeking": 0.24935400516795866,
      "Planning & Reasoning": 0.2834138486312399,
      "Math & Data Analysis": 0.35365853658536583
    },
    "reward": 0.279296875,
    "task_macro_reward": 0.31679498953881513,
    "K": 500
  },
  "gpt-4-0125-preview": {
    "model": "gpt-4-0125-preview",
    "win_much": 318,
    "win": 119,
    "tie": 337,
    "lose": 129,
    "lose_much": 58,
    "total": 1024,
    "avg_len": 3200.6378772112384,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 104,
        "win": 51,
        "tie": 164,
        "lose": 48,
        "lose_much": 22
      },
      "Planning & Reasoning": {
        "win_much": 190,
        "win": 68,
        "tie": 256,
        "lose": 86,
        "lose_much": 28
      },
      "Coding & Debugging": {
        "win_much": 70,
        "win": 17,
        "tie": 51,
        "lose": 37,
        "lose_much": 13
      },
      "Math & Data Analysis": {
        "win_much": 89,
        "win": 17,
        "tie": 75,
        "lose": 42,
        "lose_much": 20
      },
      "Creative Tasks": {
        "win_much": 115,
        "win": 66,
        "tie": 127,
        "lose": 35,
        "lose_much": 13
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.21465295629820053,
      "Planning & Reasoning": 0.24363057324840764,
      "Coding & Debugging": 0.25,
      "Math & Data Analysis": 0.23251028806584362,
      "Creative Tasks": 0.3300561797752809
    },
    "reward": 0.2490234375,
    "task_macro_reward": 0.24748764457634612,
    "K": 500
  },
  "nemotron-4-340b-instruct": {
    "model": "nemotron-4-340b-instruct",
    "win_much": 243,
    "win": 257,
    "tie": 252,
    "lose": 151,
    "lose_much": 51,
    "total": 1024,
    "avg_len": 2561.143605870021,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 74,
        "win": 109,
        "tie": 125,
        "lose": 70,
        "lose_much": 10
      },
      "Planning & Reasoning": {
        "win_much": 141,
        "win": 173,
        "tie": 167,
        "lose": 107,
        "lose_much": 35
      },
      "Coding & Debugging": {
        "win_much": 68,
        "win": 54,
        "tie": 25,
        "lose": 29,
        "lose_much": 12
      },
      "Math & Data Analysis": {
        "win_much": 76,
        "win": 56,
        "tie": 47,
        "lose": 38,
        "lose_much": 21
      },
      "Creative Tasks": {
        "win_much": 83,
        "win": 98,
        "tie": 107,
        "lose": 50,
        "lose_much": 14
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.21520618556701032,
      "Planning & Reasoning": 0.2231139646869984,
      "Coding & Debugging": 0.36436170212765956,
      "Math & Data Analysis": 0.2689075630252101,
      "Creative Tasks": 0.26420454545454547
    },
    "reward": 0.2392578125,
    "task_macro_reward": 0.2730019070412764,
    "K": 500
  },
  "Meta-Llama-3-70B-Instruct": {
    "model": "Meta-Llama-3-70B-Instruct",
    "win_much": 264,
    "win": 198,
    "tie": 281,
    "lose": 154,
    "lose_much": 48,
    "total": 1023,
    "avg_len": 2773.202116402116,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 54,
        "win": 41,
        "tie": 43,
        "lose": 30,
        "lose_much": 11
      },
      "Creative Tasks": {
        "win_much": 97,
        "win": 74,
        "tie": 98,
        "lose": 65,
        "lose_much": 11
      },
      "Information/Advice seeking": {
        "win_much": 92,
        "win": 77,
        "tie": 141,
        "lose": 61,
        "lose_much": 17
      },
      "Planning & Reasoning": {
        "win_much": 171,
        "win": 126,
        "tie": 197,
        "lose": 92,
        "lose_much": 31
      },
      "Math & Data Analysis": {
        "win_much": 81,
        "win": 52,
        "tie": 61,
        "lose": 35,
        "lose_much": 14
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.2709497206703911,
      "Creative Tasks": 0.26231884057971017,
      "Information/Advice seeking": 0.21391752577319587,
      "Planning & Reasoning": 0.2544570502431118,
      "Math & Data Analysis": 0.31069958847736623
    },
    "reward": 0.23264907135874877,
    "task_macro_reward": 0.2650643403661046,
    "K": 500
  },
  "reka-core-20240501": {
    "model": "reka-core-20240501",
    "win_much": 240,
    "win": 214,
    "tie": 272,
    "lose": 129,
    "lose_much": 66,
    "total": 1024,
    "avg_len": 2528.546145494028,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 66,
        "win": 92,
        "tie": 134,
        "lose": 63,
        "lose_much": 17
      },
      "Planning & Reasoning": {
        "win_much": 153,
        "win": 146,
        "tie": 182,
        "lose": 83,
        "lose_much": 41
      },
      "Coding & Debugging": {
        "win_much": 49,
        "win": 53,
        "tie": 36,
        "lose": 24,
        "lose_much": 21
      },
      "Math & Data Analysis": {
        "win_much": 74,
        "win": 37,
        "tie": 64,
        "lose": 31,
        "lose_much": 23
      },
      "Creative Tasks": {
        "win_much": 93,
        "win": 83,
        "tie": 104,
        "lose": 43,
        "lose_much": 16
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.17069892473118278,
      "Planning & Reasoning": 0.2371900826446281,
      "Coding & Debugging": 0.23224043715846995,
      "Math & Data Analysis": 0.23580786026200873,
      "Creative Tasks": 0.2861356932153392
    },
    "reward": 0.21142578125,
    "task_macro_reward": 0.23025011582567115,
    "K": 500
  },
  "Llama-3-8B-Magpie-Align-v0.1": {
    "model": "Llama-3-8B-Magpie-Align-v0.1",
    "win_much": 265,
    "win": 187,
    "tie": 277,
    "lose": 125,
    "lose_much": 100,
    "total": 1024,
    "avg_len": 2900.16142557652,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 45,
        "win": 24,
        "tie": 47,
        "lose": 39,
        "lose_much": 29
      },
      "Creative Tasks": {
        "win_much": 123,
        "win": 70,
        "tie": 106,
        "lose": 33,
        "lose_much": 19
      },
      "Information/Advice seeking": {
        "win_much": 100,
        "win": 93,
        "tie": 127,
        "lose": 47,
        "lose_much": 22
      },
      "Planning & Reasoning": {
        "win_much": 143,
        "win": 139,
        "tie": 191,
        "lose": 88,
        "lose_much": 68
      },
      "Math & Data Analysis": {
        "win_much": 59,
        "win": 41,
        "tie": 70,
        "lose": 34,
        "lose_much": 42
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.04619565217391304,
      "Creative Tasks": 0.349002849002849,
      "Information/Advice seeking": 0.2596401028277635,
      "Planning & Reasoning": 0.15977742448330684,
      "Math & Data Analysis": 0.08333333333333333
    },
    "reward": 0.19140625,
    "task_macro_reward": 0.14948024858465372,
    "K": 500
  },
  "Llama-3-Instruct-8B-SimPO-ExPO": {
    "model": "Llama-3-Instruct-8B-SimPO-ExPO",
    "win_much": 215,
    "win": 276,
    "tie": 233,
    "lose": 141,
    "lose_much": 105,
    "total": 1024,
    "avg_len": 2382.2061855670104,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 40,
        "win": 49,
        "tie": 30,
        "lose": 34,
        "lose_much": 36
      },
      "Creative Tasks": {
        "win_much": 94,
        "win": 117,
        "tie": 94,
        "lose": 39,
        "lose_much": 15
      },
      "Information/Advice seeking": {
        "win_much": 83,
        "win": 116,
        "tie": 116,
        "lose": 61,
        "lose_much": 15
      },
      "Planning & Reasoning": {
        "win_much": 121,
        "win": 195,
        "tie": 154,
        "lose": 96,
        "lose_much": 70
      },
      "Math & Data Analysis": {
        "win_much": 42,
        "win": 57,
        "tie": 45,
        "lose": 48,
        "lose_much": 55
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.06084656084656084,
      "Creative Tasks": 0.3286908077994429,
      "Information/Advice seeking": 0.2442455242966752,
      "Planning & Reasoning": 0.1580188679245283,
      "Math & Data Analysis": -0.03441295546558704
    },
    "reward": 0.17333984375,
    "task_macro_reward": 0.12351544792010571,
    "K": 500
  },
  "gemini-1.5-flash": {
    "model": "gemini-1.5-flash",
    "win_much": 239,
    "win": 138,
    "tie": 284,
    "lose": 150,
    "lose_much": 56,
    "total": 1024,
    "avg_len": 2955.504036908881,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 74,
        "win": 59,
        "tie": 139,
        "lose": 65,
        "lose_much": 24
      },
      "Planning & Reasoning": {
        "win_much": 149,
        "win": 93,
        "tie": 204,
        "lose": 97,
        "lose_much": 34
      },
      "Coding & Debugging": {
        "win_much": 66,
        "win": 16,
        "tie": 43,
        "lose": 20,
        "lose_much": 11
      },
      "Math & Data Analysis": {
        "win_much": 76,
        "win": 30,
        "tie": 65,
        "lose": 42,
        "lose_much": 19
      },
      "Creative Tasks": {
        "win_much": 76,
        "win": 62,
        "tie": 108,
        "lose": 58,
        "lose_much": 11
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.13019390581717452,
      "Planning & Reasoning": 0.19584055459272098,
      "Coding & Debugging": 0.33974358974358976,
      "Math & Data Analysis": 0.21982758620689655,
      "Creative Tasks": 0.2126984126984127
    },
    "reward": 0.1728515625,
    "task_macro_reward": 0.2301689268082889,
    "K": 500
  },
  "deepseekv2-chat": {
    "model": "deepseekv2-chat",
    "win_much": 217,
    "win": 230,
    "tie": 257,
    "lose": 162,
    "lose_much": 83,
    "total": 1024,
    "avg_len": 2611.6164383561645,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 62,
        "win": 101,
        "tie": 123,
        "lose": 68,
        "lose_much": 33
      },
      "Planning & Reasoning": {
        "win_much": 131,
        "win": 156,
        "tie": 188,
        "lose": 99,
        "lose_much": 49
      },
      "Coding & Debugging": {
        "win_much": 43,
        "win": 40,
        "tie": 42,
        "lose": 37,
        "lose_much": 24
      },
      "Math & Data Analysis": {
        "win_much": 66,
        "win": 49,
        "tie": 59,
        "lose": 47,
        "lose_much": 22
      },
      "Creative Tasks": {
        "win_much": 83,
        "win": 98,
        "tie": 98,
        "lose": 49,
        "lose_much": 23
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.11757105943152454,
      "Planning & Reasoning": 0.17736757624398075,
      "Coding & Debugging": 0.11021505376344086,
      "Math & Data Analysis": 0.18518518518518517,
      "Creative Tasks": 0.24074074074074073
    },
    "reward": 0.1640625,
    "task_macro_reward": 0.1585709763449423,
    "K": 500
  },
  "claude-3-sonnet-20240229": {
    "model": "claude-3-sonnet-20240229",
    "win_much": 181,
    "win": 280,
    "tie": 240,
    "lose": 172,
    "lose_much": 74,
    "total": 1023,
    "avg_len": 2350.0834213305175,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 47,
        "win": 136,
        "tie": 106,
        "lose": 69,
        "lose_much": 26
      },
      "Planning & Reasoning": {
        "win_much": 123,
        "win": 175,
        "tie": 171,
        "lose": 113,
        "lose_much": 37
      },
      "Math & Data Analysis": {
        "win_much": 70,
        "win": 53,
        "tie": 55,
        "lose": 46,
        "lose_much": 19
      },
      "Creative Tasks": {
        "win_much": 50,
        "win": 103,
        "tie": 104,
        "lose": 69,
        "lose_much": 26
      },
      "Coding & Debugging": {
        "win_much": 55,
        "win": 34,
        "tie": 33,
        "lose": 38,
        "lose_much": 19
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.14192708333333334,
      "Planning & Reasoning": 0.1890145395799677,
      "Math & Data Analysis": 0.2242798353909465,
      "Creative Tasks": 0.11647727272727272,
      "Coding & Debugging": 0.18994413407821228
    },
    "reward": 0.1573802541544477,
    "task_macro_reward": 0.18161307922680167,
    "K": 500
  },
  "yi-large": {
    "model": "yi-large",
    "win_much": 252,
    "win": 139,
    "tie": 316,
    "lose": 152,
    "lose_much": 87,
    "total": 1024,
    "avg_len": 2909.2536997885836,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 81,
        "win": 57,
        "tie": 156,
        "lose": 57,
        "lose_much": 34
      },
      "Planning & Reasoning": {
        "win_much": 157,
        "win": 91,
        "tie": 227,
        "lose": 98,
        "lose_much": 50
      },
      "Coding & Debugging": {
        "win_much": 48,
        "win": 21,
        "tie": 53,
        "lose": 38,
        "lose_much": 21
      },
      "Math & Data Analysis": {
        "win_much": 78,
        "win": 22,
        "tie": 86,
        "lose": 31,
        "lose_much": 25
      },
      "Creative Tasks": {
        "win_much": 96,
        "win": 70,
        "tie": 102,
        "lose": 56,
        "lose_much": 24
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.12207792207792208,
      "Planning & Reasoning": 0.16613162118780098,
      "Coding & Debugging": 0.10220994475138122,
      "Math & Data Analysis": 0.20041322314049587,
      "Creative Tasks": 0.22701149425287356
    },
    "reward": 0.15478515625,
    "task_macro_reward": 0.1559798672103899,
    "K": 500
  },
  "deepseek-coder-v2": {
    "model": "deepseek-coder-v2",
    "win_much": 224,
    "win": 211,
    "tie": 260,
    "lose": 168,
    "lose_much": 88,
    "total": 1024,
    "avg_len": 2590.356466876972,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 65,
        "win": 89,
        "tie": 120,
        "lose": 78,
        "lose_much": 34
      },
      "Planning & Reasoning": {
        "win_much": 137,
        "win": 145,
        "tie": 176,
        "lose": 105,
        "lose_much": 60
      },
      "Coding & Debugging": {
        "win_much": 47,
        "win": 43,
        "tie": 39,
        "lose": 34,
        "lose_much": 22
      },
      "Math & Data Analysis": {
        "win_much": 68,
        "win": 52,
        "tie": 48,
        "lose": 46,
        "lose_much": 29
      },
      "Creative Tasks": {
        "win_much": 88,
        "win": 81,
        "tie": 108,
        "lose": 54,
        "lose_much": 21
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.09455958549222798,
      "Planning & Reasoning": 0.15569823434991975,
      "Coding & Debugging": 0.15945945945945947,
      "Math & Data Analysis": 0.1728395061728395,
      "Creative Tasks": 0.22869318181818182
    },
    "reward": 0.15380859375,
    "task_macro_reward": 0.15832692704480536,
    "K": 500
  },
  "Llama-3-Instruct-8B-SimPO": {
    "model": "Llama-3-Instruct-8B-SimPO",
    "win_much": 213,
    "win": 244,
    "tie": 257,
    "lose": 140,
    "lose_much": 113,
    "total": 1024,
    "avg_len": 2425.796277145812,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 34,
        "win": 44,
        "tie": 34,
        "lose": 39,
        "lose_much": 37
      },
      "Creative Tasks": {
        "win_much": 89,
        "win": 103,
        "tie": 104,
        "lose": 43,
        "lose_much": 20
      },
      "Information/Advice seeking": {
        "win_much": 84,
        "win": 102,
        "tie": 133,
        "lose": 51,
        "lose_much": 20
      },
      "Planning & Reasoning": {
        "win_much": 125,
        "win": 172,
        "tie": 172,
        "lose": 87,
        "lose_much": 77
      },
      "Math & Data Analysis": {
        "win_much": 44,
        "win": 59,
        "tie": 51,
        "lose": 40,
        "lose_much": 53
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.0026595744680851063,
      "Creative Tasks": 0.2757660167130919,
      "Information/Advice seeking": 0.22948717948717948,
      "Planning & Reasoning": 0.1429699842022117,
      "Math & Data Analysis": 0.0020242914979757085
    },
    "reward": 0.1484375,
    "task_macro_reward": 0.10261277823948727,
    "K": 500
  },
  "Yi-1.5-34B-Chat": {
    "model": "Yi-1.5-34B-Chat",
    "win_much": 273,
    "win": 91,
    "tie": 318,
    "lose": 159,
    "lose_much": 99,
    "total": 1024,
    "avg_len": 3269.627659574468,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 94,
        "win": 35,
        "tie": 155,
        "lose": 69,
        "lose_much": 32
      },
      "Planning & Reasoning": {
        "win_much": 167,
        "win": 69,
        "tie": 219,
        "lose": 107,
        "lose_much": 57
      },
      "Coding & Debugging": {
        "win_much": 50,
        "win": 15,
        "tie": 47,
        "lose": 34,
        "lose_much": 35
      },
      "Math & Data Analysis": {
        "win_much": 77,
        "win": 20,
        "tie": 70,
        "lose": 39,
        "lose_much": 32
      },
      "Creative Tasks": {
        "win_much": 111,
        "win": 39,
        "tie": 118,
        "lose": 56,
        "lose_much": 23
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.11688311688311688,
      "Planning & Reasoning": 0.1470113085621971,
      "Coding & Debugging": 0.03038674033149171,
      "Math & Data Analysis": 0.14915966386554622,
      "Creative Tasks": 0.22910662824207492
    },
    "reward": 0.13671875,
    "task_macro_reward": 0.12065744774021733,
    "K": 500
  },
  "Qwen1.5-72B-Chat": {
    "model": "Qwen1.5-72B-Chat",
    "win_much": 193,
    "win": 267,
    "tie": 226,
    "lose": 185,
    "lose_much": 101,
    "total": 1024,
    "avg_len": 2306.2088477366256,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 41,
        "win": 48,
        "tie": 32,
        "lose": 40,
        "lose_much": 30
      },
      "Creative Tasks": {
        "win_much": 78,
        "win": 112,
        "tie": 94,
        "lose": 52,
        "lose_much": 23
      },
      "Information/Advice seeking": {
        "win_much": 64,
        "win": 114,
        "tie": 101,
        "lose": 86,
        "lose_much": 27
      },
      "Planning & Reasoning": {
        "win_much": 118,
        "win": 175,
        "tie": 153,
        "lose": 123,
        "lose_much": 69
      },
      "Math & Data Analysis": {
        "win_much": 48,
        "win": 58,
        "tie": 50,
        "lose": 51,
        "lose_much": 40
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.07853403141361257,
      "Creative Tasks": 0.23676880222841226,
      "Information/Advice seeking": 0.13010204081632654,
      "Planning & Reasoning": 0.11755485893416928,
      "Math & Data Analysis": 0.0465587044534413
    },
    "reward": 0.1298828125,
    "task_macro_reward": 0.10686963139255151,
    "K": 500
  },
  "Qwen1.5-72B-Chat-greedy": {
    "model": "Qwen1.5-72B-Chat-greedy",
    "win_much": 179,
    "win": 271,
    "tie": 253,
    "lose": 167,
    "lose_much": 102,
    "total": 1024,
    "avg_len": 2296.3991769547324,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 36,
        "win": 50,
        "tie": 40,
        "lose": 30,
        "lose_much": 35
      },
      "Creative Tasks": {
        "win_much": 74,
        "win": 110,
        "tie": 108,
        "lose": 43,
        "lose_much": 24
      },
      "Information/Advice seeking": {
        "win_much": 55,
        "win": 113,
        "tie": 114,
        "lose": 82,
        "lose_much": 28
      },
      "Planning & Reasoning": {
        "win_much": 104,
        "win": 181,
        "tie": 173,
        "lose": 119,
        "lose_much": 60
      },
      "Math & Data Analysis": {
        "win_much": 36,
        "win": 62,
        "tie": 54,
        "lose": 55,
        "lose_much": 40
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.05759162303664921,
      "Creative Tasks": 0.23259052924791088,
      "Information/Advice seeking": 0.10841836734693877,
      "Planning & Reasoning": 0.11773940345368916,
      "Math & Data Analysis": -0.0020242914979757085
    },
    "reward": 0.1259765625,
    "task_macro_reward": 0.087315480368233,
    "K": 500
  },
  "Qwen2-72B-Instruct": {
    "model": "Qwen2-72B-Instruct",
    "win_much": 207,
    "win": 178,
    "tie": 261,
    "lose": 198,
    "lose_much": 115,
    "total": 1024,
    "avg_len": 2669.078206465068,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 40,
        "win": 34,
        "tie": 43,
        "lose": 37,
        "lose_much": 34
      },
      "Creative Tasks": {
        "win_much": 76,
        "win": 70,
        "tie": 95,
        "lose": 78,
        "lose_much": 36
      },
      "Information/Advice seeking": {
        "win_much": 64,
        "win": 76,
        "tie": 129,
        "lose": 83,
        "lose_much": 36
      },
      "Planning & Reasoning": {
        "win_much": 128,
        "win": 113,
        "tie": 192,
        "lose": 127,
        "lose_much": 68
      },
      "Math & Data Analysis": {
        "win_much": 64,
        "win": 42,
        "tie": 65,
        "lose": 42,
        "lose_much": 30
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": 0.023936170212765957,
      "Creative Tasks": 0.10140845070422536,
      "Information/Advice seeking": 0.06314432989690721,
      "Planning & Reasoning": 0.08439490445859872,
      "Math & Data Analysis": 0.13991769547325103
    },
    "reward": 0.080078125,
    "task_macro_reward": 0.07860926559731578,
    "K": 500
  },
  "SELM-Llama-3-8B-Instruct-iter-3": {
    "model": "SELM-Llama-3-8B-Instruct-iter-3",
    "win_much": 180,
    "win": 199,
    "tie": 273,
    "lose": 181,
    "lose_much": 118,
    "total": 1024,
    "avg_len": 2702.2344900105154,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 23,
        "win": 43,
        "tie": 34,
        "lose": 32,
        "lose_much": 44
      },
      "Creative Tasks": {
        "win_much": 81,
        "win": 86,
        "tie": 114,
        "lose": 63,
        "lose_much": 13
      },
      "Information/Advice seeking": {
        "win_much": 69,
        "win": 70,
        "tie": 141,
        "lose": 78,
        "lose_much": 30
      },
      "Planning & Reasoning": {
        "win_much": 109,
        "win": 132,
        "tie": 183,
        "lose": 123,
        "lose_much": 75
      },
      "Math & Data Analysis": {
        "win_much": 39,
        "win": 49,
        "tie": 47,
        "lose": 59,
        "lose_much": 50
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.08806818181818182,
      "Creative Tasks": 0.22268907563025211,
      "Information/Advice seeking": 0.09020618556701031,
      "Planning & Reasoning": 0.06189710610932476,
      "Math & Data Analysis": -0.06557377049180328
    },
    "reward": 0.0693359375,
    "task_macro_reward": 0.01699190776052825,
    "K": 500
  },
  "command-r-plus": {
    "model": "command-r-plus",
    "win_much": 174,
    "win": 148,
    "tie": 264,
    "lose": 184,
    "lose_much": 132,
    "total": 1024,
    "avg_len": 2618.987804878049,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 22,
        "win": 33,
        "tie": 44,
        "lose": 32,
        "lose_much": 47
      },
      "Creative Tasks": {
        "win_much": 78,
        "win": 54,
        "tie": 107,
        "lose": 57,
        "lose_much": 20
      },
      "Information/Advice seeking": {
        "win_much": 69,
        "win": 59,
        "tie": 130,
        "lose": 74,
        "lose_much": 35
      },
      "Planning & Reasoning": {
        "win_much": 103,
        "win": 101,
        "tie": 182,
        "lose": 125,
        "lose_much": 83
      },
      "Math & Data Analysis": {
        "win_much": 36,
        "win": 38,
        "tie": 54,
        "lose": 62,
        "lose_much": 52
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.13764044943820225,
      "Creative Tasks": 0.1787974683544304,
      "Information/Advice seeking": 0.07220708446866485,
      "Planning & Reasoning": 0.013468013468013467,
      "Math & Data Analysis": -0.11570247933884298
    },
    "reward": 0.0234375,
    "task_macro_reward": -0.026813468794287393,
    "K": 500
  },
  "Yi-1.5-9B-Chat-Test": {
    "model": "Yi-1.5-9B-Chat-Test",
    "win_much": 194,
    "win": 95,
    "tie": 309,
    "lose": 211,
    "lose_much": 130,
    "total": 1022,
    "avg_len": 3232.0660276890308,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 67,
        "win": 40,
        "tie": 137,
        "lose": 91,
        "lose_much": 53
      },
      "Planning & Reasoning": {
        "win_much": 132,
        "win": 67,
        "tie": 214,
        "lose": 131,
        "lose_much": 67
      },
      "Coding & Debugging": {
        "win_much": 36,
        "win": 17,
        "tie": 47,
        "lose": 48,
        "lose_much": 31
      },
      "Math & Data Analysis": {
        "win_much": 58,
        "win": 20,
        "tie": 71,
        "lose": 51,
        "lose_much": 34
      },
      "Creative Tasks": {
        "win_much": 74,
        "win": 47,
        "tie": 118,
        "lose": 76,
        "lose_much": 39
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.029639175257731958,
      "Planning & Reasoning": 0.054009819967266774,
      "Coding & Debugging": -0.05865921787709497,
      "Math & Data Analysis": 0.03632478632478633,
      "Creative Tasks": 0.05790960451977401
    },
    "reward": 0.005870841487279843,
    "task_macro_reward": 0.00783967573770297,
    "K": 500
  },
  "Yi-1.5-9B-Chat": {
    "model": "Yi-1.5-9B-Chat",
    "win_much": 187,
    "win": 93,
    "tie": 317,
    "lose": 208,
    "lose_much": 134,
    "total": 1022,
    "avg_len": 3232.0660276890308,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 64,
        "win": 38,
        "tie": 147,
        "lose": 86,
        "lose_much": 53
      },
      "Planning & Reasoning": {
        "win_much": 117,
        "win": 61,
        "tie": 233,
        "lose": 135,
        "lose_much": 65
      },
      "Coding & Debugging": {
        "win_much": 32,
        "win": 18,
        "tie": 51,
        "lose": 42,
        "lose_much": 36
      },
      "Math & Data Analysis": {
        "win_much": 61,
        "win": 21,
        "tie": 64,
        "lose": 52,
        "lose_much": 36
      },
      "Creative Tasks": {
        "win_much": 67,
        "win": 46,
        "tie": 124,
        "lose": 80,
        "lose_much": 37
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.03350515463917526,
      "Planning & Reasoning": 0.024549918166939442,
      "Coding & Debugging": -0.0893854748603352,
      "Math & Data Analysis": 0.0405982905982906,
      "Creative Tasks": 0.03672316384180791
    },
    "reward": -0.004403131115459882,
    "task_macro_reward": -0.009939671437248755,
    "K": 500
  },
  "glm-4-9b-chat": {
    "model": "glm-4-9b-chat",
    "win_much": 162,
    "win": 88,
    "tie": 323,
    "lose": 208,
    "lose_much": 126,
    "total": 1023,
    "avg_len": 3111.403528114664,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 32,
        "win": 14,
        "tie": 49,
        "lose": 52,
        "lose_much": 28
      },
      "Creative Tasks": {
        "win_much": 66,
        "win": 56,
        "tie": 118,
        "lose": 59,
        "lose_much": 34
      },
      "Information/Advice seeking": {
        "win_much": 53,
        "win": 24,
        "tie": 165,
        "lose": 94,
        "lose_much": 38
      },
      "Planning & Reasoning": {
        "win_much": 93,
        "win": 60,
        "tie": 229,
        "lose": 135,
        "lose_much": 77
      },
      "Math & Data Analysis": {
        "win_much": 41,
        "win": 18,
        "tie": 69,
        "lose": 52,
        "lose_much": 49
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.08571428571428572,
      "Creative Tasks": 0.0915915915915916,
      "Information/Advice seeking": -0.053475935828877004,
      "Planning & Reasoning": -0.0361952861952862,
      "Math & Data Analysis": -0.1091703056768559
    },
    "reward": -0.02346041055718475,
    "task_macro_reward": -0.05386703718730164,
    "K": 500
  },
  "reka-flash-20240226": {
    "model": "reka-flash-20240226",
    "win_much": 127,
    "win": 236,
    "tie": 245,
    "lose": 181,
    "lose_much": 180,
    "total": 1024,
    "avg_len": 2034.6078431372548,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 43,
        "win": 97,
        "tie": 106,
        "lose": 87,
        "lose_much": 58
      },
      "Planning & Reasoning": {
        "win_much": 68,
        "win": 159,
        "tie": 174,
        "lose": 111,
        "lose_much": 123
      },
      "Coding & Debugging": {
        "win_much": 22,
        "win": 45,
        "tie": 47,
        "lose": 22,
        "lose_much": 55
      },
      "Math & Data Analysis": {
        "win_much": 34,
        "win": 49,
        "tie": 61,
        "lose": 36,
        "lose_much": 66
      },
      "Creative Tasks": {
        "win_much": 50,
        "win": 96,
        "tie": 91,
        "lose": 77,
        "lose_much": 43
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.02557544757033248,
      "Planning & Reasoning": -0.048818897637795275,
      "Coding & Debugging": -0.112565445026178,
      "Math & Data Analysis": -0.10365853658536585,
      "Creative Tasks": 0.046218487394957986
    },
    "reward": -0.02490234375,
    "task_macro_reward": -0.06346553829381112,
    "K": 500
  },
  "mistral-large-2402": {
    "model": "mistral-large-2402",
    "win_much": 136,
    "win": 192,
    "tie": 247,
    "lose": 239,
    "lose_much": 139,
    "total": 1024,
    "avg_len": 2271.5561385099686,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 26,
        "win": 32,
        "tie": 45,
        "lose": 46,
        "lose_much": 39
      },
      "Creative Tasks": {
        "win_much": 53,
        "win": 92,
        "tie": 101,
        "lose": 81,
        "lose_much": 22
      },
      "Information/Advice seeking": {
        "win_much": 37,
        "win": 83,
        "tie": 99,
        "lose": 115,
        "lose_much": 55
      },
      "Planning & Reasoning": {
        "win_much": 81,
        "win": 118,
        "tie": 177,
        "lose": 148,
        "lose_much": 102
      },
      "Math & Data Analysis": {
        "win_much": 43,
        "win": 31,
        "tie": 56,
        "lose": 56,
        "lose_much": 57
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.10638297872340426,
      "Creative Tasks": 0.10458452722063037,
      "Information/Advice seeking": -0.08740359897172237,
      "Planning & Reasoning": -0.05750798722044728,
      "Math & Data Analysis": -0.10905349794238683
    },
    "reward": -0.02587890625,
    "task_macro_reward": -0.06887950336645848,
    "K": 500
  },
  "Starling-LM-7B-beta-ExPO": {
    "model": "Starling-LM-7B-beta-ExPO",
    "win_much": 142,
    "win": 165,
    "tie": 286,
    "lose": 225,
    "lose_much": 147,
    "total": 1024,
    "avg_len": 2681.9740932642485,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 46,
        "win": 70,
        "tie": 128,
        "lose": 109,
        "lose_much": 37
      },
      "Planning & Reasoning": {
        "win_much": 81,
        "win": 115,
        "tie": 192,
        "lose": 149,
        "lose_much": 94
      },
      "Coding & Debugging": {
        "win_much": 26,
        "win": 39,
        "tie": 30,
        "lose": 42,
        "lose_much": 54
      },
      "Math & Data Analysis": {
        "win_much": 32,
        "win": 36,
        "tie": 48,
        "lose": 70,
        "lose_much": 59
      },
      "Creative Tasks": {
        "win_much": 65,
        "win": 62,
        "tie": 137,
        "lose": 68,
        "lose_much": 25
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.026923076923076925,
      "Planning & Reasoning": -0.04754358161648178,
      "Coding & Debugging": -0.1544502617801047,
      "Math & Data Analysis": -0.17959183673469387,
      "Creative Tasks": 0.10364145658263306
    },
    "reward": -0.0341796875,
    "task_macro_reward": -0.08430646744871961,
    "K": 500
  },
  "SELM-Zephyr-7B-iter-3": {
    "model": "SELM-Zephyr-7B-iter-3",
    "win_much": 152,
    "win": 140,
    "tie": 264,
    "lose": 208,
    "lose_much": 186,
    "total": 1024,
    "avg_len": 2567.4494736842107,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 13,
        "win": 19,
        "tie": 36,
        "lose": 43,
        "lose_much": 70
      },
      "Creative Tasks": {
        "win_much": 77,
        "win": 67,
        "tie": 103,
        "lose": 73,
        "lose_much": 33
      },
      "Information/Advice seeking": {
        "win_much": 62,
        "win": 56,
        "tie": 130,
        "lose": 87,
        "lose_much": 47
      },
      "Planning & Reasoning": {
        "win_much": 87,
        "win": 91,
        "tie": 194,
        "lose": 132,
        "lose_much": 121
      },
      "Math & Data Analysis": {
        "win_much": 27,
        "win": 26,
        "tie": 54,
        "lose": 59,
        "lose_much": 76
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.3812154696132597,
      "Creative Tasks": 0.11614730878186968,
      "Information/Advice seeking": -0.0013089005235602095,
      "Planning & Reasoning": -0.0872,
      "Math & Data Analysis": -0.2706611570247934
    },
    "reward": -0.06640625,
    "task_macro_reward": -0.16822916106170596,
    "K": 500
  },
  "Starling-LM-7B-beta": {
    "model": "Starling-LM-7B-beta",
    "win_much": 130,
    "win": 169,
    "tie": 251,
    "lose": 237,
    "lose_much": 172,
    "total": 1024,
    "avg_len": 2562.4254431699687,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 48,
        "win": 62,
        "tie": 111,
        "lose": 119,
        "lose_much": 49
      },
      "Planning & Reasoning": {
        "win_much": 75,
        "win": 107,
        "tie": 167,
        "lose": 169,
        "lose_much": 111
      },
      "Coding & Debugging": {
        "win_much": 27,
        "win": 32,
        "tie": 29,
        "lose": 42,
        "lose_much": 58
      },
      "Math & Data Analysis": {
        "win_much": 25,
        "win": 40,
        "tie": 46,
        "lose": 67,
        "lose_much": 65
      },
      "Creative Tasks": {
        "win_much": 58,
        "win": 67,
        "tie": 116,
        "lose": 78,
        "lose_much": 37
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.07583547557840617,
      "Planning & Reasoning": -0.10651828298887123,
      "Coding & Debugging": -0.19148936170212766,
      "Math & Data Analysis": -0.22016460905349794,
      "Creative Tasks": 0.04353932584269663
    },
    "reward": -0.07421875,
    "task_macro_reward": -0.13216444393256901,
    "K": 500
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "model": "Mixtral-8x7B-Instruct-v0.1",
    "win_much": 118,
    "win": 145,
    "tie": 253,
    "lose": 249,
    "lose_much": 186,
    "total": 1024,
    "avg_len": 2357.1882229232388,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 25,
        "win": 26,
        "tie": 34,
        "lose": 35,
        "lose_much": 66
      },
      "Creative Tasks": {
        "win_much": 42,
        "win": 60,
        "tie": 120,
        "lose": 86,
        "lose_much": 42
      },
      "Information/Advice seeking": {
        "win_much": 36,
        "win": 62,
        "tie": 120,
        "lose": 118,
        "lose_much": 51
      },
      "Planning & Reasoning": {
        "win_much": 65,
        "win": 94,
        "tie": 162,
        "lose": 173,
        "lose_much": 128
      },
      "Math & Data Analysis": {
        "win_much": 37,
        "win": 28,
        "tie": 45,
        "lose": 60,
        "lose_much": 70
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.2446236559139785,
      "Creative Tasks": -0.037142857142857144,
      "Information/Advice seeking": -0.1111111111111111,
      "Planning & Reasoning": -0.1647909967845659,
      "Math & Data Analysis": -0.20416666666666666
    },
    "reward": -0.1171875,
    "task_macro_reward": -0.17217678830412822,
    "K": 500
  },
  "Meta-Llama-3-8B-Instruct": {
    "model": "Meta-Llama-3-8B-Instruct",
    "win_much": 127,
    "win": 125,
    "tie": 250,
    "lose": 260,
    "lose_much": 182,
    "total": 1024,
    "avg_len": 2631.0074152542375,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 22,
        "win": 15,
        "tie": 37,
        "lose": 50,
        "lose_much": 54
      },
      "Creative Tasks": {
        "win_much": 57,
        "win": 61,
        "tie": 95,
        "lose": 95,
        "lose_much": 43
      },
      "Information/Advice seeking": {
        "win_much": 47,
        "win": 41,
        "tie": 136,
        "lose": 101,
        "lose_much": 59
      },
      "Planning & Reasoning": {
        "win_much": 74,
        "win": 83,
        "tie": 172,
        "lose": 173,
        "lose_much": 119
      },
      "Math & Data Analysis": {
        "win_much": 33,
        "win": 21,
        "tie": 45,
        "lose": 77,
        "lose_much": 66
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.27808988764044945,
      "Creative Tasks": -0.008547008547008548,
      "Information/Advice seeking": -0.109375,
      "Planning & Reasoning": -0.14492753623188406,
      "Math & Data Analysis": -0.25206611570247933
    },
    "reward": -0.11962890625,
    "task_macro_reward": -0.18255634732976853,
    "K": 500
  },
  "dbrx-instruct@together": {
    "model": "dbrx-instruct@together",
    "win_much": 117,
    "win": 152,
    "tie": 229,
    "lose": 243,
    "lose_much": 216,
    "total": 1024,
    "avg_len": 2353.0052246603973,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 23,
        "win": 30,
        "tie": 49,
        "lose": 34,
        "lose_much": 54
      },
      "Creative Tasks": {
        "win_much": 47,
        "win": 63,
        "tie": 93,
        "lose": 82,
        "lose_much": 65
      },
      "Information/Advice seeking": {
        "win_much": 33,
        "win": 55,
        "tie": 92,
        "lose": 130,
        "lose_much": 77
      },
      "Planning & Reasoning": {
        "win_much": 66,
        "win": 97,
        "tie": 162,
        "lose": 171,
        "lose_much": 133
      },
      "Math & Data Analysis": {
        "win_much": 37,
        "win": 37,
        "tie": 52,
        "lose": 58,
        "lose_much": 59
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.1736842105263158,
      "Creative Tasks": -0.07857142857142857,
      "Information/Advice seeking": -0.21059431524547803,
      "Planning & Reasoning": -0.16534181240063592,
      "Math & Data Analysis": -0.1337448559670782
    },
    "reward": -0.14111328125,
    "task_macro_reward": -0.15889659691486122,
    "K": 500
  },
  "command-r": {
    "model": "command-r",
    "win_much": 115,
    "win": 107,
    "tie": 269,
    "lose": 237,
    "lose_much": 200,
    "total": 1024,
    "avg_len": 2449.2974137931033,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 20,
        "win": 15,
        "tie": 37,
        "lose": 40,
        "lose_much": 70
      },
      "Creative Tasks": {
        "win_much": 59,
        "win": 43,
        "tie": 101,
        "lose": 98,
        "lose_much": 29
      },
      "Information/Advice seeking": {
        "win_much": 48,
        "win": 44,
        "tie": 134,
        "lose": 106,
        "lose_much": 50
      },
      "Planning & Reasoning": {
        "win_much": 64,
        "win": 75,
        "tie": 183,
        "lose": 155,
        "lose_much": 133
      },
      "Math & Data Analysis": {
        "win_much": 21,
        "win": 22,
        "tie": 55,
        "lose": 52,
        "lose_much": 91
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.3434065934065934,
      "Creative Tasks": 0.007575757575757576,
      "Information/Advice seeking": -0.08638743455497382,
      "Planning & Reasoning": -0.17868852459016393,
      "Math & Data Analysis": -0.35269709543568467
    },
    "reward": -0.146484375,
    "task_macro_reward": -0.2244881452757859,
    "K": 500
  },
  "Hermes-2-Theta-Llama-3-8B": {
    "model": "Hermes-2-Theta-Llama-3-8B",
    "win_much": 102,
    "win": 144,
    "tie": 239,
    "lose": 252,
    "lose_much": 213,
    "total": 1023,
    "avg_len": 2450.127368421053,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 21,
        "win": 31,
        "tie": 28,
        "lose": 32,
        "lose_much": 68
      },
      "Creative Tasks": {
        "win_much": 34,
        "win": 60,
        "tie": 102,
        "lose": 102,
        "lose_much": 53
      },
      "Information/Advice seeking": {
        "win_much": 39,
        "win": 49,
        "tie": 114,
        "lose": 128,
        "lose_much": 57
      },
      "Planning & Reasoning": {
        "win_much": 59,
        "win": 86,
        "tie": 157,
        "lose": 176,
        "lose_much": 147
      },
      "Math & Data Analysis": {
        "win_much": 27,
        "win": 35,
        "tie": 48,
        "lose": 57,
        "lose_much": 76
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.2638888888888889,
      "Creative Tasks": -0.11396011396011396,
      "Information/Advice seeking": -0.14857881136950904,
      "Planning & Reasoning": -0.2128,
      "Math & Data Analysis": -0.24691358024691357
    },
    "reward": -0.16129032258064516,
    "task_macro_reward": -0.21288186460320283,
    "K": 500
  },
  "neo_7b_instruct_v0.1": {
    "model": "neo_7b_instruct_v0.1",
    "win_much": 132,
    "win": 45,
    "tie": 307,
    "lose": 260,
    "lose_much": 193,
    "total": 1024,
    "avg_len": 3440.821771611526,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 47,
        "win": 22,
        "tie": 140,
        "lose": 112,
        "lose_much": 61
      },
      "Planning & Reasoning": {
        "win_much": 77,
        "win": 29,
        "tie": 217,
        "lose": 182,
        "lose_much": 111
      },
      "Coding & Debugging": {
        "win_much": 16,
        "win": 6,
        "tie": 38,
        "lose": 46,
        "lose_much": 70
      },
      "Math & Data Analysis": {
        "win_much": 27,
        "win": 9,
        "tie": 60,
        "lose": 74,
        "lose_much": 69
      },
      "Creative Tasks": {
        "win_much": 68,
        "win": 25,
        "tie": 131,
        "lose": 89,
        "lose_much": 34
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.1544502617801047,
      "Planning & Reasoning": -0.1793831168831169,
      "Coding & Debugging": -0.42045454545454547,
      "Math & Data Analysis": -0.3117154811715481,
      "Creative Tasks": 0.005763688760806916
    },
    "reward": -0.16455078125,
    "task_macro_reward": -0.247256507589116,
    "K": 500
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "model": "Nous-Hermes-2-Mixtral-8x7B-DPO",
    "win_much": 113,
    "win": 130,
    "tie": 214,
    "lose": 251,
    "lose_much": 222,
    "total": 1023,
    "avg_len": 2423.65376344086,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 26,
        "win": 26,
        "tie": 34,
        "lose": 33,
        "lose_much": 59
      },
      "Creative Tasks": {
        "win_much": 36,
        "win": 54,
        "tie": 103,
        "lose": 90,
        "lose_much": 60
      },
      "Information/Advice seeking": {
        "win_much": 37,
        "win": 45,
        "tie": 98,
        "lose": 130,
        "lose_much": 68
      },
      "Planning & Reasoning": {
        "win_much": 69,
        "win": 75,
        "tie": 151,
        "lose": 168,
        "lose_much": 143
      },
      "Math & Data Analysis": {
        "win_much": 37,
        "win": 33,
        "tie": 41,
        "lose": 54,
        "lose_much": 74
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.2050561797752809,
      "Creative Tasks": -0.12244897959183673,
      "Information/Advice seeking": -0.19444444444444445,
      "Planning & Reasoning": -0.19884488448844884,
      "Math & Data Analysis": -0.19874476987447698
    },
    "reward": -0.1656891495601173,
    "task_macro_reward": -0.19172187859650333,
    "K": 500
  },
  "tulu-2-dpo-70b": {
    "model": "tulu-2-dpo-70b",
    "win_much": 103,
    "win": 138,
    "tie": 202,
    "lose": 266,
    "lose_much": 219,
    "total": 1024,
    "avg_len": 2393.4762931034484,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 40,
        "win": 57,
        "tie": 85,
        "lose": 127,
        "lose_much": 68
      },
      "Planning & Reasoning": {
        "win_much": 59,
        "win": 79,
        "tie": 142,
        "lose": 182,
        "lose_much": 152
      },
      "Coding & Debugging": {
        "win_much": 17,
        "win": 25,
        "tie": 23,
        "lose": 40,
        "lose_much": 72
      },
      "Math & Data Analysis": {
        "win_much": 26,
        "win": 20,
        "tie": 40,
        "lose": 66,
        "lose_much": 88
      },
      "Creative Tasks": {
        "win_much": 44,
        "win": 65,
        "tie": 87,
        "lose": 100,
        "lose_much": 41
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.16710875331564987,
      "Planning & Reasoning": -0.23534201954397393,
      "Coding & Debugging": -0.3531073446327684,
      "Math & Data Analysis": -0.3541666666666667,
      "Creative Tasks": -0.04302670623145401
    },
    "reward": -0.17578125,
    "task_macro_reward": -0.26033121894527556,
    "K": 500
  },
  "reka-edge": {
    "model": "reka-edge",
    "win_much": 92,
    "win": 155,
    "tie": 219,
    "lose": 216,
    "lose_much": 256,
    "total": 1024,
    "avg_len": 2306.7473347547975,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 34,
        "win": 57,
        "tie": 106,
        "lose": 92,
        "lose_much": 84
      },
      "Planning & Reasoning": {
        "win_much": 49,
        "win": 90,
        "tie": 144,
        "lose": 148,
        "lose_much": 181
      },
      "Coding & Debugging": {
        "win_much": 16,
        "win": 28,
        "tie": 35,
        "lose": 38,
        "lose_much": 72
      },
      "Math & Data Analysis": {
        "win_much": 18,
        "win": 28,
        "tie": 40,
        "lose": 56,
        "lose_much": 98
      },
      "Creative Tasks": {
        "win_much": 43,
        "win": 69,
        "tie": 93,
        "lose": 82,
        "lose_much": 59
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.18096514745308312,
      "Planning & Reasoning": -0.2630718954248366,
      "Coding & Debugging": -0.32275132275132273,
      "Math & Data Analysis": -0.39166666666666666,
      "Creative Tasks": -0.06502890173410404
    },
    "reward": -0.18994140625,
    "task_macro_reward": -0.2720387370197327,
    "K": 500
  },
  "neo_7b_instruct_v0.1-ExPO": {
    "model": "neo_7b_instruct_v0.1-ExPO",
    "win_much": 127,
    "win": 33,
    "tie": 294,
    "lose": 252,
    "lose_much": 212,
    "total": 1024,
    "avg_len": 3572.305010893246,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 43,
        "win": 11,
        "tie": 156,
        "lose": 103,
        "lose_much": 61
      },
      "Planning & Reasoning": {
        "win_much": 68,
        "win": 19,
        "tie": 206,
        "lose": 171,
        "lose_much": 140
      },
      "Math & Data Analysis": {
        "win_much": 25,
        "win": 3,
        "tie": 57,
        "lose": 75,
        "lose_much": 79
      },
      "Creative Tasks": {
        "win_much": 74,
        "win": 20,
        "tie": 123,
        "lose": 82,
        "lose_much": 42
      },
      "Coding & Debugging": {
        "win_much": 10,
        "win": 7,
        "tie": 21,
        "lose": 55,
        "lose_much": 76
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.1711229946524064,
      "Planning & Reasoning": -0.24503311258278146,
      "Math & Data Analysis": -0.37656903765690375,
      "Creative Tasks": 0.002932551319648094,
      "Coding & Debugging": -0.5325443786982249
    },
    "reward": -0.18994140625,
    "task_macro_reward": -0.31061407833424054,
    "K": 500
  },
  "Mistral-7B-Instruct-v0.2": {
    "model": "Mistral-7B-Instruct-v0.2",
    "win_much": 99,
    "win": 124,
    "tie": 210,
    "lose": 287,
    "lose_much": 219,
    "total": 1024,
    "avg_len": 2478.094781682641,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 18,
        "win": 23,
        "tie": 24,
        "lose": 45,
        "lose_much": 72
      },
      "Creative Tasks": {
        "win_much": 45,
        "win": 66,
        "tie": 94,
        "lose": 100,
        "lose_much": 42
      },
      "Information/Advice seeking": {
        "win_much": 42,
        "win": 43,
        "tie": 101,
        "lose": 136,
        "lose_much": 61
      },
      "Planning & Reasoning": {
        "win_much": 51,
        "win": 67,
        "tie": 149,
        "lose": 205,
        "lose_much": 146
      },
      "Math & Data Analysis": {
        "win_much": 20,
        "win": 13,
        "tie": 40,
        "lose": 76,
        "lose_much": 87
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.35714285714285715,
      "Creative Tasks": -0.040345821325648415,
      "Information/Advice seeking": -0.17101827676240208,
      "Planning & Reasoning": -0.26537216828478966,
      "Math & Data Analysis": -0.4173728813559322
    },
    "reward": -0.19677734375,
    "task_macro_reward": -0.28293753292107715,
    "K": 500
  },
  "Qwen1.5-7B-Chat@together": {
    "model": "Qwen1.5-7B-Chat@together",
    "win_much": 92,
    "win": 151,
    "tie": 211,
    "lose": 254,
    "lose_much": 249,
    "total": 1022,
    "avg_len": 2364.1264367816093,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 20,
        "win": 23,
        "tie": 26,
        "lose": 44,
        "lose_much": 71
      },
      "Creative Tasks": {
        "win_much": 46,
        "win": 76,
        "tie": 93,
        "lose": 93,
        "lose_much": 48
      },
      "Information/Advice seeking": {
        "win_much": 31,
        "win": 59,
        "tie": 105,
        "lose": 115,
        "lose_much": 80
      },
      "Planning & Reasoning": {
        "win_much": 50,
        "win": 97,
        "tie": 146,
        "lose": 174,
        "lose_much": 159
      },
      "Math & Data Analysis": {
        "win_much": 20,
        "win": 23,
        "tie": 53,
        "lose": 58,
        "lose_much": 88
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.3342391304347826,
      "Creative Tasks": -0.02949438202247191,
      "Information/Advice seeking": -0.19743589743589743,
      "Planning & Reasoning": -0.2356230031948882,
      "Math & Data Analysis": -0.35330578512396693
    },
    "reward": -0.20401174168297456,
    "task_macro_reward": -0.2586226455261504,
    "K": 500
  },
  "gpt-3.5-turbo-0125": {
    "model": "gpt-3.5-turbo-0125",
    "win_much": 80,
    "win": 178,
    "tie": 229,
    "lose": 196,
    "lose_much": 286,
    "total": 1024,
    "avg_len": 1747.4912280701753,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 21,
        "win": 33,
        "tie": 41,
        "lose": 17,
        "lose_much": 79
      },
      "Creative Tasks": {
        "win_much": 34,
        "win": 78,
        "tie": 92,
        "lose": 81,
        "lose_much": 72
      },
      "Information/Advice seeking": {
        "win_much": 21,
        "win": 69,
        "tie": 95,
        "lose": 100,
        "lose_much": 107
      },
      "Planning & Reasoning": {
        "win_much": 45,
        "win": 107,
        "tie": 163,
        "lose": 138,
        "lose_much": 184
      },
      "Math & Data Analysis": {
        "win_much": 22,
        "win": 39,
        "tie": 49,
        "lose": 41,
        "lose_much": 96
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.2617801047120419,
      "Creative Tasks": -0.11064425770308123,
      "Information/Advice seeking": -0.25892857142857145,
      "Planning & Reasoning": -0.2425431711145997,
      "Math & Data Analysis": -0.30364372469635625
    },
    "reward": -0.2099609375,
    "task_macro_reward": -0.24917186882160577,
    "K": 500
  },
  "Yi-1.5-6B-Chat": {
    "model": "Yi-1.5-6B-Chat",
    "win_much": 111,
    "win": 67,
    "tie": 237,
    "lose": 229,
    "lose_much": 248,
    "total": 1023,
    "avg_len": 2959.330717488789,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 39,
        "win": 24,
        "tie": 113,
        "lose": 101,
        "lose_much": 81
      },
      "Planning & Reasoning": {
        "win_much": 69,
        "win": 38,
        "tie": 170,
        "lose": 154,
        "lose_much": 157
      },
      "Coding & Debugging": {
        "win_much": 14,
        "win": 16,
        "tie": 29,
        "lose": 43,
        "lose_much": 77
      },
      "Math & Data Analysis": {
        "win_much": 39,
        "win": 14,
        "tie": 51,
        "lose": 53,
        "lose_much": 77
      },
      "Creative Tasks": {
        "win_much": 40,
        "win": 26,
        "tie": 100,
        "lose": 88,
        "lose_much": 66
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.22486033519553073,
      "Planning & Reasoning": -0.24829931972789115,
      "Coding & Debugging": -0.4273743016759777,
      "Math & Data Analysis": -0.24572649572649571,
      "Creative Tasks": -0.178125
    },
    "reward": -0.2130987292277615,
    "task_macro_reward": -0.28379500502694316,
    "K": 500
  },
  "Phi-3-medium-128k-instruct": {
    "model": "Phi-3-medium-128k-instruct",
    "win_much": 87,
    "win": 117,
    "tie": 217,
    "lose": 259,
    "lose_much": 256,
    "total": 1024,
    "avg_len": 2262.357905982906,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 19,
        "win": 19,
        "tie": 40,
        "lose": 42,
        "lose_much": 66
      },
      "Creative Tasks": {
        "win_much": 36,
        "win": 42,
        "tie": 91,
        "lose": 95,
        "lose_much": 75
      },
      "Information/Advice seeking": {
        "win_much": 22,
        "win": 43,
        "tie": 90,
        "lose": 122,
        "lose_much": 101
      },
      "Planning & Reasoning": {
        "win_much": 48,
        "win": 75,
        "tie": 152,
        "lose": 180,
        "lose_much": 165
      },
      "Math & Data Analysis": {
        "win_much": 30,
        "win": 36,
        "tie": 50,
        "lose": 54,
        "lose_much": 68
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.31451612903225806,
      "Creative Tasks": -0.19321533923303835,
      "Information/Advice seeking": -0.3134920634920635,
      "Planning & Reasoning": -0.27338709677419354,
      "Math & Data Analysis": -0.19747899159663865
    },
    "reward": -0.234375,
    "task_macro_reward": -0.2661231033874253,
    "K": 500
  },
  "Llama-2-70b-chat-hf": {
    "model": "Llama-2-70b-chat-hf",
    "win_much": 96,
    "win": 85,
    "tie": 218,
    "lose": 280,
    "lose_much": 254,
    "total": 1023,
    "avg_len": 2761.801714898178,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 42,
        "win": 42,
        "tie": 108,
        "lose": 135,
        "lose_much": 58
      },
      "Planning & Reasoning": {
        "win_much": 52,
        "win": 57,
        "tie": 145,
        "lose": 180,
        "lose_much": 178
      },
      "Coding & Debugging": {
        "win_much": 9,
        "win": 11,
        "tie": 28,
        "lose": 40,
        "lose_much": 92
      },
      "Math & Data Analysis": {
        "win_much": 20,
        "win": 10,
        "tie": 34,
        "lose": 65,
        "lose_much": 112
      },
      "Creative Tasks": {
        "win_much": 47,
        "win": 32,
        "tie": 94,
        "lose": 111,
        "lose_much": 53
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.16233766233766234,
      "Planning & Reasoning": -0.30637254901960786,
      "Coding & Debugging": -0.5416666666666666,
      "Math & Data Analysis": -0.495850622406639,
      "Creative Tasks": -0.13501483679525222
    },
    "reward": -0.24975562072336266,
    "task_macro_reward": -0.36740216992954844,
    "K": 500
  },
  "Magpie-Pro-SFT-v0.1": {
    "model": "Magpie-Pro-SFT-v0.1",
    "win_much": 78,
    "win": 85,
    "tie": 216,
    "lose": 254,
    "lose_much": 267,
    "total": 1023,
    "avg_len": 2699.12,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 14,
        "win": 12,
        "tie": 24,
        "lose": 41,
        "lose_much": 74
      },
      "Creative Tasks": {
        "win_much": 35,
        "win": 31,
        "tie": 87,
        "lose": 98,
        "lose_much": 84
      },
      "Information/Advice seeking": {
        "win_much": 26,
        "win": 30,
        "tie": 119,
        "lose": 116,
        "lose_much": 82
      },
      "Planning & Reasoning": {
        "win_much": 49,
        "win": 58,
        "tie": 147,
        "lose": 162,
        "lose_much": 171
      },
      "Math & Data Analysis": {
        "win_much": 18,
        "win": 32,
        "tie": 33,
        "lose": 57,
        "lose_much": 84
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.45151515151515154,
      "Creative Tasks": -0.2462686567164179,
      "Information/Advice seeking": -0.26541554959785524,
      "Planning & Reasoning": -0.29642248722316866,
      "Math & Data Analysis": -0.35044642857142855
    },
    "reward": -0.2673509286412512,
    "task_macro_reward": -0.3384346247475641,
    "K": 500
  },
  "Phi-3-mini-128k-instruct": {
    "model": "Phi-3-mini-128k-instruct",
    "win_much": 79,
    "win": 125,
    "tie": 202,
    "lose": 216,
    "lose_much": 326,
    "total": 1023,
    "avg_len": 2140.9535864978902,
    "task_categorized_results": {
      "Coding & Debugging": {
        "win_much": 20,
        "win": 29,
        "tie": 31,
        "lose": 36,
        "lose_much": 71
      },
      "Creative Tasks": {
        "win_much": 33,
        "win": 49,
        "tie": 90,
        "lose": 82,
        "lose_much": 94
      },
      "Information/Advice seeking": {
        "win_much": 14,
        "win": 50,
        "tie": 85,
        "lose": 90,
        "lose_much": 142
      },
      "Planning & Reasoning": {
        "win_much": 46,
        "win": 78,
        "tie": 140,
        "lose": 152,
        "lose_much": 207
      },
      "Math & Data Analysis": {
        "win_much": 26,
        "win": 27,
        "tie": 48,
        "lose": 51,
        "lose_much": 91
      }
    },
    "task_categorized_rewards": {
      "Coding & Debugging": -0.2914438502673797,
      "Creative Tasks": -0.22270114942528735,
      "Information/Advice seeking": -0.3884514435695538,
      "Planning & Reasoning": -0.31781701444622795,
      "Math & Data Analysis": -0.3168724279835391
    },
    "reward": -0.2859237536656892,
    "task_macro_reward": -0.3118184560558648,
    "K": 500
  },
  "Llama-2-7b-chat-hf": {
    "model": "Llama-2-7b-chat-hf",
    "win_much": 41,
    "win": 63,
    "tie": 177,
    "lose": 290,
    "lose_much": 357,
    "total": 1023,
    "avg_len": 2628.8588362068967,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 19,
        "win": 19,
        "tie": 93,
        "lose": 142,
        "lose_much": 108
      },
      "Planning & Reasoning": {
        "win_much": 20,
        "win": 36,
        "tie": 109,
        "lose": 195,
        "lose_much": 241
      },
      "Coding & Debugging": {
        "win_much": 4,
        "win": 10,
        "tie": 8,
        "lose": 39,
        "lose_much": 115
      },
      "Math & Data Analysis": {
        "win_much": 6,
        "win": 11,
        "tie": 29,
        "lose": 57,
        "lose_much": 132
      },
      "Creative Tasks": {
        "win_much": 22,
        "win": 37,
        "tie": 85,
        "lose": 118,
        "lose_much": 82
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.39501312335958005,
      "Planning & Reasoning": -0.5,
      "Coding & Debugging": -0.7130681818181818,
      "Math & Data Analysis": -0.6340425531914894,
      "Creative Tasks": -0.2921511627906977
    },
    "reward": -0.4198435972629521,
    "task_macro_reward": -0.5458343588166844,
    "K": 500
  },
  "gemma-7b-it": {
    "model": "gemma-7b-it",
    "win_much": 30,
    "win": 106,
    "tie": 181,
    "lose": 172,
    "lose_much": 482,
    "total": 1024,
    "avg_len": 1670.7322348094747,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 6,
        "win": 51,
        "tie": 72,
        "lose": 73,
        "lose_much": 190
      },
      "Planning & Reasoning": {
        "win_much": 16,
        "win": 67,
        "tie": 130,
        "lose": 105,
        "lose_much": 319
      },
      "Coding & Debugging": {
        "win_much": 6,
        "win": 15,
        "tie": 35,
        "lose": 19,
        "lose_much": 115
      },
      "Math & Data Analysis": {
        "win_much": 6,
        "win": 17,
        "tie": 45,
        "lose": 34,
        "lose_much": 145
      },
      "Creative Tasks": {
        "win_much": 19,
        "win": 49,
        "tie": 81,
        "lose": 73,
        "lose_much": 136
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.49744897959183676,
      "Planning & Reasoning": -0.5054945054945055,
      "Coding & Debugging": -0.5842105263157895,
      "Math & Data Analysis": -0.597165991902834,
      "Creative Tasks": -0.36033519553072624
    },
    "reward": -0.4736328125,
    "task_macro_reward": -0.5289582345526197,
    "K": 500
  },
  "gemma-2b-it": {
    "model": "gemma-2b-it",
    "win_much": 14,
    "win": 44,
    "tie": 141,
    "lose": 139,
    "lose_much": 633,
    "total": 1024,
    "avg_len": 1520.9011328527292,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 2,
        "win": 17,
        "tie": 56,
        "lose": 51,
        "lose_much": 266
      },
      "Planning & Reasoning": {
        "win_much": 8,
        "win": 21,
        "tie": 100,
        "lose": 83,
        "lose_much": 425
      },
      "Coding & Debugging": {
        "win_much": 0,
        "win": 8,
        "tie": 23,
        "lose": 20,
        "lose_much": 139
      },
      "Math & Data Analysis": {
        "win_much": 3,
        "win": 11,
        "tie": 38,
        "lose": 20,
        "lose_much": 175
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 21,
        "tie": 62,
        "lose": 73,
        "lose_much": 191
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": -0.7168367346938775,
      "Planning & Reasoning": -0.7032967032967034,
      "Coding & Debugging": -0.7631578947368421,
      "Math & Data Analysis": -0.7145748987854251,
      "Creative Tasks": -0.5754189944134078
    },
    "reward": -0.65087890625,
    "task_macro_reward": -0.7101010935904145,
    "K": 500
  }
}