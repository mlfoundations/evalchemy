LlaMa-3-8B-SambaNova:
  prompt_template: "LlaMa-3-8B-SambaNova/prompt.txt"
  fn_completions: "sambanova_completions" # use "huggingface_local_completions" if you want local serving
  completions_kwargs:
    model_name: "llama3-8b"
    #is_fast_tokenizer: False # needed for local
  completion_parser_kwargs:
    outputs_to_match:
      1: '(?:^|\n) ?Output \(a\)'
      2: '(?:^|\n) ?Output \(b\)'
  batch_size: 1
